{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import anndata\n",
    "from scipy.spatial import distance\n",
    "import scanpy as sc\n",
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import functools\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from tqdm.notebook import tqdm\n",
    "import scanpy as sc\n",
    "from collections import Counter\n",
    "import matplotlib.cm as cm\n",
    "from anndata import AnnData\n",
    "import itertools as it\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from scipy.sparse import vstack\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('-pretrain_batch', '--pretrain_batch',\n",
    "                        type=int,\n",
    "                        help='Batch size for pretraining. Default: no batch',\n",
    "                        default=None)\n",
    "    \n",
    "    parser.add_argument('-pretrain','--pretrain',\n",
    "                        type = bool,\n",
    "                        default = True,\n",
    "                        help='Pretrain model with autoencoder; otherwise load existing')\n",
    "    \n",
    "    parser.add_argument('-nepoch', '--epochs',\n",
    "                        type=int,\n",
    "                        help='number of epochs to train for',\n",
    "                        default=30)\n",
    "\n",
    "    parser.add_argument('-nepoch_pretrain', '--epochs_pretrain',\n",
    "                        type=int,\n",
    "                        help='number of epochs to pretrain for',\n",
    "                        default=25)\n",
    "\n",
    "    parser.add_argument('-source_file','--model_file',\n",
    "                        type = str,\n",
    "                        default = 'trained_models/source.pt',\n",
    "                        help='location for storing source model and data')\n",
    "\n",
    "    parser.add_argument('-lr', '--learning_rate',\n",
    "                        type=float,\n",
    "                        help='learning rate for the model, default=0.001',\n",
    "                        default=0.001)\n",
    "\n",
    "    parser.add_argument('-lrS', '--lr_scheduler_step',\n",
    "                        type=int,\n",
    "                        help='StepLR learning rate scheduler step, default=20',\n",
    "                        default=20) \n",
    "\n",
    "    parser.add_argument('-lrG', '--lr_scheduler_gamma',\n",
    "                        type=float,\n",
    "                        help='StepLR learning rate scheduler gamma, default=0.5',\n",
    "                        default=0.5)\n",
    "  \n",
    "    parser.add_argument('-seed', '--manual_seed',\n",
    "                        type=int,\n",
    "                        help='input for the manual seeds initializations',\n",
    "                        default=3)\n",
    "    \n",
    "    parser.add_argument('--cuda',\n",
    "                        action='store_true',\n",
    "                        help='enables cuda')\n",
    "    \n",
    "    return parser\n",
    "\n",
    "\n",
    "'''\n",
    "Class representing dataset for an single-cell experiment.\n",
    "'''\n",
    "\n",
    "IMG_CACHE = {}\n",
    "\n",
    "\n",
    "class ExperimentDataset(data.Dataset):\n",
    "    \n",
    "    \n",
    "    def __init__(self, x, cells, genes, metadata, y=[]):\n",
    "        '''\n",
    "        x: numpy array of gene expressions of cells (rows are cells)\n",
    "        cells: cell IDs in the order of appearance\n",
    "        genes: gene IDs in the order of appearance\n",
    "        metadata: experiment identifier\n",
    "        y: numeric labels of cells (empty list if unknown)\n",
    "        '''\n",
    "        super(ExperimentDataset, self).__init__()\n",
    "        \n",
    "        self.nitems = x.shape[0]\n",
    "        if len(y)>0:\n",
    "            print(\"== Dataset: Found %d items \" % x.shape[0])\n",
    "            print(\"== Dataset: Found %d classes\" % len(np.unique(y)))\n",
    "                \n",
    "        if type(x)==torch.Tensor:\n",
    "            self.x = x\n",
    "        else:\n",
    "            shape = x.shape[1]\n",
    "            self.x = [torch.from_numpy(inst).view(shape).float() for inst in x]\n",
    "        if len(y)==0:\n",
    "            y = np.zeros(len(self.x), dtype=np.int64)\n",
    "        \n",
    "        self.y = tuple(y.tolist())\n",
    "        self.xIDs = cells\n",
    "        self.yIDs = genes\n",
    "        self.metadata = metadata\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx].squeeze(), self.y[idx], self.xIDs[idx]\n",
    "    #, self.yIDs[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nitems\n",
    "    \n",
    "    def get_dim(self):\n",
    "        return self.x[0].shape[0]\n",
    "\n",
    "class EpochSampler(object):\n",
    "    '''\n",
    "    EpochSampler: yield permuted indexes at each epoch.\n",
    "   \n",
    "    __len__ returns the number of episodes per epoch (same as 'self.iterations').\n",
    "    '''\n",
    "\n",
    "    def __init__(self, indices):\n",
    "        '''\n",
    "        Initialize the EpochSampler object\n",
    "        Args:\n",
    "        - labels: an iterable containing all the labels for the current dataset\n",
    "        samples indexes will be infered from this iterable.\n",
    "        - iterations: number of epochs\n",
    "        '''\n",
    "        super(EpochSampler, self).__init__()\n",
    "        \n",
    "        self.indices = indices\n",
    "        \n",
    "\n",
    "    def __iter__(self):\n",
    "        '''\n",
    "        yield a batch of indexes\n",
    "        '''\n",
    "        \n",
    "    \n",
    "        while(True):\n",
    "            shuffled_idx = self.indices[torch.randperm(len(self.indices))]\n",
    "            \n",
    "            yield shuffled_idx\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        returns the number of iterations (episodes) per epoch\n",
    "        '''\n",
    "        return self.iterations\n",
    "    \n",
    "import torch\n",
    "from torch.utils.data import DataLoader ##prefetch by batch\n",
    "#from model.epoch_sampler import EpochSampler\n",
    "\n",
    "\n",
    "def init_labeled_loader(data, val_split = 0.8):\n",
    "    \"\"\"Initialize loaders for train and validation sets. \n",
    "    Class labels are used only\n",
    "    for stratified sampling between train and validation set.\n",
    "    \n",
    "    Validation_split = % keeps in training set \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    target = torch.tensor(list(data.y))\n",
    "    uniq = torch.unique(target, sorted=True)\n",
    "    \n",
    "    class_idxs = list(map(lambda c: target.eq(c).nonzero(), uniq))\n",
    "    class_idxs = [idx[torch.randperm(len(idx))] for idx in class_idxs]\n",
    "    \n",
    "    train_idx = torch.cat([idx[:int(val_split*len(idx))] for idx in class_idxs])\n",
    "    val_idx = torch.cat([idx[int(val_split*len(idx)):] for idx in class_idxs])\n",
    "    \n",
    "    train_loader = DataLoader(data, \n",
    "                              batch_sampler=EpochSampler(train_idx),\n",
    "                              pin_memory=True)\n",
    "    \n",
    "    val_loader = DataLoader(data, \n",
    "                            batch_sampler=EpochSampler(val_idx),\n",
    "                            pin_memory=True)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def init_loader(datasets, val_split = 0.8):\n",
    "    \n",
    "    train_loader_all = []\n",
    "    val_loader_all = []\n",
    "    \n",
    "    for data in datasets:\n",
    "        \n",
    "        curr_load_tr, curr_load_val = init_labeled_loader(data, val_split)\n",
    "        train_loader_all.append(curr_load_tr)\n",
    "        val_loader_all.append(curr_load_val)\n",
    "    \n",
    "    if val_split==1:\n",
    "        val_loader_all = None\n",
    "        \n",
    "    return train_loader_all, val_loader_all\n",
    "\n",
    "\n",
    "def init_data_loaders(labeled_data, unlabeled_data, \n",
    "                      pretrain_data, pretrain_batch, val_split):\n",
    "    \n",
    "    \"\"\"Initialize loaders for pretraing, \n",
    "    training (labeled and unlabeled datasets) and validation. \"\"\"\n",
    "    \n",
    "    train_loader, val_loader = init_loader(labeled_data, val_split)\n",
    "    \n",
    "    if not pretrain_data:\n",
    "        pretrain_data = unlabeled_data\n",
    "    \n",
    "    pretrain_loader = torch.utils.data.DataLoader(dataset=pretrain_data,\n",
    "                                                  shuffle=True,\n",
    "                                                  batch_size=pretrain_batch if pretrain_batch!=None else len(unlabeled_data.x))        \n",
    "    test_loader = DataLoader(unlabeled_data, \n",
    "                            batch_sampler=EpochSampler(torch.randperm(len(unlabeled_data.x))),\n",
    "                            pin_memory=True) \n",
    "    \n",
    "    #test_loader,_ = init_loader([unlabeled_data], 1.0) # to reproduce results in the paper\n",
    "    #test_loader = test_loader[0]\n",
    "    return train_loader, test_loader, pretrain_loader, val_loader\n",
    "           \n",
    "           \n",
    "def euclidean_dist(x, y):\n",
    "    '''\n",
    "    Compute euclidean distance between two tensors\n",
    "    '''\n",
    "    # x: N x D\n",
    "    # y: M x D\n",
    "    n = x.size(0)\n",
    "    m = y.size(0)\n",
    "    d = x.size(1)\n",
    "    if d != y.size(1):\n",
    "        raise Exception\n",
    "\n",
    "    x = x.unsqueeze(1).expand(n, m, d)\n",
    "    y = y.unsqueeze(0).expand(n, m, d)\n",
    "\n",
    "    return torch.pow(x - y, 2).sum(2)\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "from scipy.optimize import linear_sum_assignment \n",
    "\n",
    "def compute_scores(y_true, y_pred, scoring={'accuracy','precision','recall','nmi',\n",
    "                                                'adj_rand','f1_score','adj_mi'}):\n",
    "    y_true = y_true.cpu().numpy()\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    \n",
    "    scores = {}\n",
    "    y_true, y_pred = hungarian_match(y_true, y_pred)\n",
    "    set_scores(scores, y_true, y_pred, scoring)\n",
    "        \n",
    "    return scores\n",
    "\n",
    "\n",
    "def set_scores(scores, y_true, y_pred, scoring):\n",
    "    labels=list(set(y_true))\n",
    "    \n",
    "    for metric in scoring:\n",
    "        if metric=='accuracy':\n",
    "            scores[metric] = metrics.accuracy_score(y_true, y_pred)\n",
    "        elif metric=='precision':\n",
    "            scores[metric] = metrics.precision_score(y_true, y_pred, labels, average='macro')\n",
    "        elif metric=='recall':\n",
    "            scores[metric] = metrics.recall_score(y_true, y_pred, labels, average='macro')\n",
    "        elif metric=='f1_score':\n",
    "            scores[metric] = metrics.f1_score(y_true, y_pred, labels, average='macro')\n",
    "        elif metric=='nmi':\n",
    "            scores[metric] = metrics.normalized_mutual_info_score(y_true, y_pred)\n",
    "        elif metric=='adj_mi':\n",
    "            scores[metric] = metrics.adjusted_mutual_info_score(y_true, y_pred)\n",
    "        elif metric=='adj_rand':\n",
    "            scores[metric] = metrics.adjusted_rand_score(y_true, y_pred)\n",
    "                \n",
    "                \n",
    "def hungarian_match(y_true, y_pred):\n",
    "    \"\"\"Matches predicted labels to original using hungarian algorithm.\"\"\"\n",
    "    \n",
    "    y_true = adjust_range(y_true)\n",
    "    y_pred = adjust_range(y_pred)\n",
    "    \n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    # Confusion matrix.\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    ind = linear_sum_assignment(-w)\n",
    "    ind = np.asarray(ind)\n",
    "    ind = np.transpose(ind)\n",
    "    d = {i:j for i, j in ind}\n",
    "    y_pred = np.array([d[v] for v in y_pred])\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "def adjust_range(y):\n",
    "    \"\"\"Assures that the range of indices if from 0 to n-1.\"\"\"\n",
    "    y = np.array(y, dtype=np.int64)\n",
    "    val_set = set(y)\n",
    "    mapping = {val:i for  i,val in enumerate(val_set)}\n",
    "    y = np.array([mapping[val] for val in y], dtype=np.int64)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AE_MARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "#from sklearn.cluster import k_means_\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#from model.utils import euclidean_dist\n",
    "\n",
    "class EpochSampler(object):\n",
    "    '''\n",
    "    EpochSampler: yield permuted indexes at each epoch.\n",
    "   \n",
    "    __len__ returns the number of episodes per epoch (same as 'self.iterations').\n",
    "    '''\n",
    "\n",
    "    def __init__(self, indices):\n",
    "        '''\n",
    "        Initialize the EpochSampler object\n",
    "        Args:\n",
    "        - labels: an iterable containing all the labels for the current dataset\n",
    "        samples indexes will be infered from this iterable.\n",
    "        - iterations: number of epochs\n",
    "        '''\n",
    "        super(EpochSampler, self).__init__()\n",
    "        self.indices = indices\n",
    "        \n",
    "\n",
    "    def __iter__(self):\n",
    "        '''\n",
    "        yield a batch of indexes\n",
    "        '''\n",
    "        \n",
    "        while(True):\n",
    "            shuffled_idx = self.indices[torch.randperm(len(self.indices))]\n",
    "            \n",
    "            yield shuffled_idx\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        returns the number of iterations (episodes) per epoch\n",
    "        '''\n",
    "        return self.iterations\n",
    "\n",
    "def init_labeled_loader(data, val_split = 0.8):\n",
    "    \"\"\"Initialize loaders for train and validation sets. \n",
    "    Class labels are used only\n",
    "    for stratified sampling between train and validation set.\"\"\"\n",
    "    \n",
    "    target = torch.tensor(list(data.y))\n",
    "    uniq = torch.unique(target, sorted=True)\n",
    "    \n",
    "    class_idxs = list(map(lambda c: target.eq(c).nonzero(), uniq))\n",
    "    class_idxs = [idx[torch.randperm(len(idx))] for idx in class_idxs]\n",
    "    \n",
    "    train_idx = torch.cat([idx[:int(val_split*len(idx))] for idx in class_idxs])\n",
    "    val_idx = torch.cat([idx[int(val_split*len(idx)):] for idx in class_idxs])\n",
    "    \n",
    "    train_loader = DataLoader(data, \n",
    "                              batch_sampler=EpochSampler(train_idx),\n",
    "                              pin_memory=True)\n",
    "    \n",
    "    val_loader = DataLoader(data, \n",
    "                            batch_sampler=EpochSampler(val_idx),\n",
    "                            pin_memory=True)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def init_loader(datasets, val_split = 0.8):\n",
    "    \n",
    "    train_loader_all = []\n",
    "    val_loader_all = []\n",
    "    \n",
    "    for data in datasets:\n",
    "        \n",
    "        curr_load_tr, curr_load_val = init_labeled_loader(data, val_split)\n",
    "        train_loader_all.append(curr_load_tr)\n",
    "        val_loader_all.append(curr_load_val)\n",
    "    \n",
    "    if val_split==1:\n",
    "        val_loader_all = None\n",
    "        \n",
    "    return train_loader_all, val_loader_all\n",
    "\n",
    "\n",
    "def init_data_loaders(labeled_data, unlabeled_data, \n",
    "                      pretrain_data, pretrain_batch, val_split):\n",
    "    \n",
    "    \"\"\"Initialize loaders for pretraing, \n",
    "    training (labeled and unlabeled datasets) and validation. \"\"\"\n",
    "    \n",
    "    train_loader, val_loader = init_loader(labeled_data, val_split)\n",
    "    \n",
    "    if not pretrain_data:\n",
    "        pretrain_data = unlabeled_data\n",
    "    \n",
    "    pretrain_loader = torch.utils.data.DataLoader(dataset=pretrain_data, shuffle=True,\n",
    "                                                  batch_size=pretrain_batch if pretrain_batch!=None else len(unlabeled_data.x))        \n",
    "    test_loader = DataLoader(unlabeled_data, \n",
    "                            batch_sampler=EpochSampler(torch.randperm(len(unlabeled_data.x))),\n",
    "                            pin_memory=True) \n",
    "    \n",
    "    #test_loader,_ = init_loader([unlabeled_data], 1.0) # to reproduce results in the paper\n",
    "    #test_loader = test_loader[0]\n",
    "    return train_loader, test_loader, pretrain_loader, val_loader\n",
    "           \n",
    "           \n",
    "def euclidean_dist(x, y):\n",
    "    '''\n",
    "    Compute euclidean distance between two tensors\n",
    "    '''\n",
    "    # x: N x D\n",
    "    # y: M x D\n",
    "    n = x.size(0)\n",
    "    m = y.size(0)\n",
    "    d = x.size(1)\n",
    "    if d != y.size(1):\n",
    "        raise Exception\n",
    "\n",
    "    x = x.unsqueeze(1).expand(n, m, d)\n",
    "    y = y.unsqueeze(0).expand(n, m, d)\n",
    "\n",
    "    return torch.pow(x - y, 2).sum(2)\n",
    "\n",
    "def loss_task(encoded, prototypes, target, criterion='dist'):\n",
    "    \"\"\"Calculate loss.\n",
    "    criterion: NNLoss - assign to closest prototype and calculate NNLoss\n",
    "    dist - loss is distance to prototype that example needs to be assigned to\n",
    "                and -distance to prototypes from other class\n",
    "    \"\"\"\n",
    "    \n",
    "    uniq = torch.unique(target, sorted=True)\n",
    "    \n",
    "    ###index of samples for each class of labels\n",
    "    class_idxs = list(map(lambda c: target.eq(c).nonzero(), uniq))\n",
    "    \n",
    "    # prepare targets so they start from 0,1\n",
    "    for idx,v in enumerate(uniq):\n",
    "        target[target==v]=idx\n",
    "    \n",
    "    dists = euclidean_dist(encoded, prototypes)\n",
    "    \n",
    "    if criterion=='NNLoss':\n",
    "       \n",
    "        loss = torch.nn.NLLLoss()\n",
    "        log_p_y = F.log_softmax(-dists, dim=1)\n",
    "        \n",
    "        loss_val = loss(log_p_y, target)\n",
    "        _, y_hat = log_p_y.max(1)\n",
    "        \n",
    "    \n",
    "    elif criterion=='dist':\n",
    "        \n",
    "        loss_val = torch.stack([dists[idx_example, idx_proto].mean(0) for idx_proto,idx_example in enumerate(class_idxs)]).mean()\n",
    "        #loss_val1 = loss_val1/len(embeddings) \n",
    "        y_hat = torch.max(-dists,1)[1]\n",
    "        \n",
    "    acc_val = y_hat.eq(target.squeeze()).float().mean()    \n",
    "        \n",
    "    return loss_val, acc_val\n",
    "\n",
    "def loss_test_nn(encoded, prototypes):\n",
    "    dists = euclidean_dist(encoded, prototypes)\n",
    "    min_dist = torch.min(dists, 1)\n",
    "    \n",
    "    y_hat = min_dist[1]\n",
    "    args_uniq = torch.unique(y_hat, sorted=True)\n",
    "    args_count = torch.stack([(y_hat==x_u).sum() for x_u in args_uniq])\n",
    "    print(args_count)\n",
    "    \n",
    "    loss = torch.nn.NLLLoss()\n",
    "    log_p_y = F.log_softmax(-dists, dim=1)\n",
    "    print(log_p_y.shape)\n",
    "        \n",
    "    loss_val = loss(log_p_y, y_hat)\n",
    "    _, y_hat = log_p_y.max(1)\n",
    "    \n",
    "    return loss_val, args_count\n",
    "\n",
    "###Intra cluster distance\n",
    "def loss_test_basic(encoded, prototypes):\n",
    "    dists = euclidean_dist(encoded, prototypes)\n",
    "    min_dist = torch.min(dists, 1)\n",
    "    \n",
    "    y_hat = min_dist[1]\n",
    "    args_uniq = torch.unique(y_hat, sorted=True)\n",
    "    args_count = torch.stack([(y_hat==x_u).sum() for x_u in args_uniq])\n",
    "    #print(args_count)\n",
    "    \n",
    "    min_dist = min_dist[0] # get_distances\n",
    "    \n",
    "    #thr = torch.stack([torch.sort(min_dist[y_hat==idx_class])[0][int(len(min_dist[y_hat==idx_class])*0.9)] for idx_class in args_uniq])\n",
    "    #loss_val = torch.stack([min_dist[y_hat==idx_class][min_dist[y_hat==idx_class]>=thr[idx_class]].mean(0) for idx_class in args_uniq]).mean()\n",
    "    \n",
    "    loss_val = torch.stack([min_dist[y_hat==idx_class].mean(0) for idx_class in args_uniq]).mean()\n",
    "    \n",
    "    #loss_val,_ = loss_task(encoded, prototypes, y_hat, criterion='dist') # same\n",
    "    \n",
    "    return loss_val, args_count\n",
    "\n",
    "def loss_test(encoded, prototypes, tau):\n",
    "    #prototypes = torch.stack(prototypes).squeeze() \n",
    "    loss_val_test, args_count = loss_test_basic(encoded, prototypes)\n",
    "    \n",
    "    \n",
    "    ###inter cluster distance \n",
    "    if tau>0:\n",
    "        dists = euclidean_dist(prototypes, prototypes)\n",
    "        nproto = prototypes.shape[0]\n",
    "        loss_val2 = - torch.sum(dists)/(nproto*nproto-nproto)\n",
    "        \n",
    "        loss_val_test += tau*loss_val2\n",
    "        \n",
    "    return loss_val_test, args_count\n",
    "\n",
    "def reconstruction_loss(decoded, x):\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    loss_rcn = loss_func(decoded, x)\n",
    "    #print('Reconstruction {}'.format(loss_rcn))\n",
    "    \n",
    "    return loss_rcn\n",
    "\n",
    "#############################################\n",
    "#############################################\n",
    "def compute_landmarks_tr(embeddings, target, prev_landmarks=None, tau=0.2):\n",
    "    \n",
    "    \"\"\"Computing landmarks of each class in the labeled meta-dataset. \n",
    "    \n",
    "    Landmark is a closed form solution of \n",
    "    minimizing distance to the mean and maximizing distance to other landmarks. \n",
    "    \n",
    "    If tau=0, landmarks are just mean of data points.\n",
    "    \n",
    "    embeddings: embeddings of the labeled dataset\n",
    "    target: labels in the labeled dataset\n",
    "    prev_landmarks: landmarks from previous iteration\n",
    "    tau: regularizer for inter- and intra-cluster distance\n",
    "    \"\"\"\n",
    "    \n",
    "    uniq = torch.unique(target, sorted=True)\n",
    "    class_idxs = list(map(lambda c: target.eq(c).nonzero(), uniq))\n",
    "    \n",
    "    landmarks_mean = torch.stack([embeddings[idx_class].mean(0) for idx_class in class_idxs]).squeeze()\n",
    "    \n",
    "    if prev_landmarks is None or tau==0:\n",
    "        return landmarks_mean\n",
    "    \n",
    "    suma = prev_landmarks.sum(0)\n",
    "    nlndmk = prev_landmarks.shape[0]\n",
    "    lndmk_dist_part = (tau/(nlndmk-1))*torch.stack([suma-p for p in prev_landmarks])\n",
    "    landmarks = 1/(1-tau)*(landmarks_mean-lndmk_dist_part)\n",
    "    \n",
    "    return landmarks\n",
    "\n",
    "#############################################\n",
    "#############################################\n",
    "\n",
    "\n",
    "def init_landmarks(n_clusters, tr_load, test_load, model, device, mode='kmeans', pretrain=True):\n",
    "    \"\"\"Initialization of landmarks of the labeled and unlabeled meta-dataset.\n",
    "    nclusters: number of expected clusters in the unlabeled meta-dataset\n",
    "    tr_load: data loader for labeled meta-dataset\n",
    "    test_load: data loader for unlabeled meta-dataset\n",
    "    \"\"\"\n",
    "    lndmk_tr = [torch.zeros(size=(len(np.unique(dl.dataset.y)), model.z_dim), \n",
    "                            requires_grad=True, device=device) for dl in tr_load]\n",
    "    \n",
    "    lndmk_test = [torch.zeros(size=(1, model.z_dim), \n",
    "                              requires_grad=True, device=device) \n",
    "                       for _ in range(n_clusters)]\n",
    "    \n",
    "    kmeans_init_tr = [init_step(dl.dataset, model, device, pretrained=pretrain, mode=mode) \n",
    "                      for dl in tr_load]\n",
    "    \n",
    "    kmeans_init_test = init_step(test_load.dataset, model, device, \n",
    "                                 pretrained=pretrain, mode=mode, \n",
    "                                 n_clusters=n_clusters)\n",
    "    \n",
    "    ##No gradient calculation\n",
    "    with torch.no_grad():\n",
    "        [lndmk.copy_(kmeans_init_tr[idx])  for idx,lndmk in enumerate(lndmk_tr)]\n",
    "        [lndmk_test[i].copy_(kmeans_init_test[i,:]) for i in range(kmeans_init_test.shape[0])]\n",
    "        \n",
    "    return lndmk_tr, lndmk_test\n",
    "\n",
    "\n",
    "def init_step(dataset, model, device, pretrained, mode='kmeans',n_clusters=None):\n",
    "    \"\"\"Initialization of landmarks with k-means or k-means++ given dataset.\"\"\"\n",
    "    \n",
    "    if n_clusters==None:\n",
    "        n_clusters = len(np.unique(dataset.y))\n",
    "    nexamples = len(dataset.x)\n",
    "        \n",
    "    X =  torch.stack([dataset.x[i] for i in range(nexamples)])\n",
    "    \n",
    "    if mode=='kmeans++':\n",
    "        if not pretrained: # find centroids in original space\n",
    "            landmarks = k_means_._init_centroids(X.cpu().numpy(), n_clusters, 'k-means++')\n",
    "            landmarks = torch.tensor(landmarks, device=device)\n",
    "            landmarks = landmarks.to(device)\n",
    "            lndmk_encoded,_ = model(landmarks)\n",
    "            \n",
    "        else:\n",
    "            X = X.to(device)\n",
    "            encoded,_ = model(X)\n",
    "            landmarks = k_means_._init_centroids(encoded.data.cpu().numpy(), n_clusters, 'k-means++')\n",
    "            lndmk_encoded = torch.tensor(landmarks, device=device)\n",
    "    \n",
    "    elif mode=='kmeans': # run kmeans clustering\n",
    "        if not pretrained: \n",
    "            kmeans = KMeans(n_clusters, random_state=0).fit(X.cpu().numpy())\n",
    "            landmarks = torch.tensor(kmeans.cluster_centers_, device=device)\n",
    "            landmarks = landmarks.to(device)\n",
    "            ##Feed forward net on landmarks  (k means cluster)\n",
    "            ##landmarks are k means cluster centers = coordinates of cluster center\n",
    "            lndmk_encoded,_ = model(landmarks)\n",
    "        \n",
    "        ##cluster on lower embedding space\n",
    "        else:\n",
    "            X = X.to(device)\n",
    "            encoded,_ = model(X)\n",
    "            kmeans = KMeans(n_clusters, random_state=0).fit(encoded.data.cpu().numpy())\n",
    "            lndmk_encoded = torch.tensor(kmeans.cluster_centers_, device=device)\n",
    "    \n",
    "    return lndmk_encoded\n",
    "\n",
    "\n",
    "def full_block(in_features, out_features, p_drop):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_features, out_features, bias=True),\n",
    "            nn.LayerNorm(out_features),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=p_drop),\n",
    "        )\n",
    "\n",
    "class FullNet(nn.Module):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(self, x_dim, hid_dim=64, z_dim=64, p_drop=0.2):\n",
    "        super(FullNet, self).__init__()\n",
    "        \n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            full_block(x_dim, hid_dim, p_drop),\n",
    "            full_block(hid_dim, z_dim, p_drop),\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            full_block(z_dim, hid_dim, p_drop),\n",
    "            full_block(hid_dim, x_dim, p_drop),\n",
    "        )\n",
    "      \n",
    "    def forward(self, x):\n",
    "        \n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        \n",
    "        return encoded, decoded\n",
    "\n",
    "\n",
    "class AE_MARS:\n",
    "    def __init__(self, n_clusters, params, \n",
    "                 labeled_data, unlabeled_data, \n",
    "                 pretrain_data=None, \n",
    "                 val_split=1.0, hid_dim_1=1000, hid_dim_2=100, p_drop=0.0, tau=0.2):\n",
    "        \"\"\"Initialization of MARS.\n",
    "        n_clusters: number of clusters in the unlabeled meta-dataset\n",
    "        params: parameters of the MARS model\n",
    "        labeled_data: list of labeled datasets. Each dataset needs to be instance of CellDataset.\n",
    "        unlabeled_data: unlabeled dataset. Instance of CellDataset.\n",
    "        pretrain_data: dataset for pretraining MARS. Instance of CellDataset. If not specified, unlabeled_data\n",
    "                        will be used.\n",
    "        val_split: percentage of data to use for train/val split (default: 1, meaning no validation set)\n",
    "        hid_dim_1: dimension in the first layer of the network (default: 1000)\n",
    "        hid_dim_2: dimension in the second layer of the network (default: 100)\n",
    "        p_drop: dropout probability (default: 0)\n",
    "        tau: regularizer for inter-cluster distance\n",
    "        \"\"\"\n",
    "        train_load, test_load, pretrain_load, val_load = init_data_loaders(labeled_data, unlabeled_data, \n",
    "                                                                           pretrain_data, params.pretrain_batch, \n",
    "                                                                           val_split)\n",
    "        self.train_loader = train_load\n",
    "        self.test_loader = test_load\n",
    "        self.pretrain_loader = pretrain_load\n",
    "        self.val_loader = val_load\n",
    "        \n",
    "        ##data file type (string name)\n",
    "        self.labeled_metadata = [data.metadata for data in labeled_data]\n",
    "        self.unlabeled_metadata = unlabeled_data.metadata\n",
    "        \n",
    "        self.genes = unlabeled_data.yIDs\n",
    "        \n",
    "        ##number of genes \n",
    "        x_dim = self.test_loader.dataset.get_dim()\n",
    "        \n",
    "        ##Feed forward neural net\n",
    "        self.init_model(x_dim, hid_dim_1, hid_dim_2, p_drop, params.device)\n",
    "        \n",
    "        self.n_clusters = n_clusters\n",
    "        \n",
    "        self.device = params.device\n",
    "        self.epochs = params.epochs\n",
    "        self.epochs_pretrain = params.epochs_pretrain\n",
    "        self.pretrain_flag = params.pretrain\n",
    "        self.model_file = params.model_file\n",
    "        self.lr = params.learning_rate\n",
    "        self.lr_gamma = params.lr_scheduler_gamma\n",
    "        self.step_size = params.lr_scheduler_step\n",
    "        \n",
    "        self.tau = tau\n",
    "        \n",
    "    ###################################################################\n",
    "    ###################################################################    \n",
    "    ###### With the fine tuned hyper parameter     \n",
    "    ###### Change the implementation of AE to VAE     \n",
    "    def init_model(self, x_dim, hid_dim, z_dim, p_drop, device):\n",
    "        \"\"\"\n",
    "        Initialize the model.\n",
    "        \"\"\"\n",
    "        self.model = FullNet(x_dim, hid_dim, z_dim, p_drop).to(device)\n",
    "    \n",
    "    ###################################################################\n",
    "    ###################################################################\n",
    "    \n",
    "    def init_optim(self, param1, param2, learning_rate):\n",
    "        \"\"\"Initializing optimizers.\"\"\"\n",
    "        \n",
    "        optim = torch.optim.Adam(params=param1, lr=learning_rate)\n",
    "        optim_landmk_test = torch.optim.Adam(params=param2, lr=learning_rate)\n",
    "        \n",
    "        return optim, optim_landmk_test\n",
    "    \n",
    "    ###################################################################\n",
    "    ###################################################################\n",
    "    ########Will need to alter this part for VAE (KL + reconstruction loss + regularized)\n",
    "    def pretrain(self, optim):\n",
    "        \"\"\"\n",
    "        Pretraining model with autoencoder.\n",
    "        optim: optimizer\n",
    "        \"\"\"\n",
    "        print('Pretraining..')\n",
    "        \n",
    "        for e in range(self.epochs_pretrain):\n",
    "            \n",
    "            start = time.time()\n",
    "            epoch_loss = 0\n",
    "            for _, batch in enumerate(self.pretrain_loader):\n",
    "                x,_,_ = batch\n",
    "                x = x.to(self.device)\n",
    "                _, decoded = self.model(x)\n",
    "                loss = reconstruction_loss(decoded, x) \n",
    "                optim.zero_grad()              \n",
    "                loss.backward()                    \n",
    "                optim.step() \n",
    "                \n",
    "                epoch_loss += loss \n",
    "                \n",
    "            print(f\"Pretraining Epoch {e}, Loss: {epoch_loss}\")\n",
    "            print(\"Time: \", time.time()-start)\n",
    "        \n",
    "        print(\"Pretraining done\")\n",
    "            \n",
    "    ###################################################################\n",
    "    ###################################################################\n",
    "    \n",
    "    \n",
    "    def train(self, evaluation_mode=True, save_all_embeddings=True):\n",
    "        \"\"\"Train model.\n",
    "        evaluation_mode: if True, validates model on the unlabeled dataset. \n",
    "        In the evaluation mode, ground truth labels of the unlabeled dataset must be \n",
    "        provided to validate model\n",
    "        \n",
    "        save_all_embeddings: if True, MARS embeddings for annotated and unannotated \n",
    "        experiments will be saved in an anndata object,\n",
    "        otherwise only unnanotated will be saved. \n",
    "        If naming is called after, all embeddings need to be saved\n",
    "        \n",
    "        return: adata: anndata object containing labeled and unlabeled meta-dataset \n",
    "        with MARS embeddings and estimated labels on the unlabeled dataset\n",
    "                landmk_all: landmarks of the labeled and unlabeled meta-dataset in the \n",
    "                order given for training. Landmarks on the unlabeled\n",
    "                            dataset are provided last\n",
    "                metrics: clustering metrics if evaluation_mode is True\n",
    "                \n",
    "        \"\"\"\n",
    "        tr_iter = [iter(dl) for dl in self.train_loader]\n",
    "        \n",
    "        if self.val_loader is not None:\n",
    "            val_iter = [iter(dl) for dl in self.val_loader]\n",
    "        \n",
    "        ####Pre train step \n",
    "        optim_pretrain = torch.optim.Adam(params=list(self.model.parameters()), lr=self.lr)\n",
    "        if self.pretrain_flag:\n",
    "            self.pretrain(optim_pretrain)\n",
    "        else:\n",
    "            self.model.load_state_dict(torch.load(self.MODEL_FILE))    \n",
    "        ####\n",
    "        \n",
    "        test_iter = iter(self.test_loader)\n",
    "        \n",
    "        \n",
    "        ##initialize training (annotated landmark) and testing (unannotated landmark)\n",
    "        landmk_tr, landmk_test = init_landmarks(self.n_clusters, \n",
    "                                                self.train_loader, \n",
    "                                                self.test_loader, \n",
    "                                                self.model, self.device)\n",
    "        \n",
    "        optim, optim_landmk_test = self.init_optim(list(self.model.encoder.parameters()), \n",
    "                                                   landmk_test, self.lr)\n",
    "        \n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optim,\n",
    "                                                       gamma=self.lr_gamma,\n",
    "                                                       step_size=self.step_size)\n",
    "        \n",
    "        latent_tracker = []\n",
    "        training_history = {'Loss':[],'Accuracy':[], 'Loss_tracker':[]}\n",
    "        \n",
    "        best_acc = 0\n",
    "        for epoch in range(1, self.epochs+1):\n",
    "            \n",
    "            ##Set model for training\n",
    "            self.model.train()\n",
    "            start = time.time()\n",
    "            ##This is equivalent to train step \n",
    "            loss_tr, acc_tr, landmk_tr, landmk_test, latent_history, loss_tracker = self.do_epoch(tr_iter, \n",
    "                                                                                    test_iter,\n",
    "                                                                                    optim, \n",
    "                                                                                    optim_landmk_test,\n",
    "                                                                                    landmk_tr, \n",
    "                                                                                    landmk_test)\n",
    "            ##Loss training includes 2- embedding and landmark distance \n",
    "            print(f'Epoch {epoch} Loss training: {loss_tr}, Accuracy training: {acc_tr}')\n",
    "            \n",
    "            ###Track model training \n",
    "            training_history['Loss'].append(loss_tr)\n",
    "            training_history['Accuracy'].append(acc_tr)\n",
    "            training_history['Loss_tracker'].append(loss_tracker)\n",
    "            \n",
    "            latent_tracker.append(latent_history)\n",
    "            \n",
    "            print('Time: ', time.time()-start)\n",
    "            \n",
    "            if epoch==self.epochs: \n",
    "                print('\\n=== Epoch: {} ==='.format(epoch))\n",
    "                print('Train acc: {}'.format(acc_tr))\n",
    "            if self.val_loader is None:\n",
    "                continue\n",
    "            self.model.eval()\n",
    "            ##Stop training \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                loss_val,acc_val = self.do_val_epoch(val_iter, landmk_tr)\n",
    "                if acc_val > best_acc:\n",
    "                    print('Saving model...')\n",
    "                    best_acc = acc_val\n",
    "                    best_state = self.model.state_dict()\n",
    "                    #torch.save(model.state_dict(), self.model_file)\n",
    "                postfix = ' (Best)' if acc_val >= best_acc else ' (Best: {})'.format(best_acc)\n",
    "                print('Val loss: {}, acc: {}{}'.format(loss_val, acc_val, postfix))\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "        if self.val_loader is None:\n",
    "            best_state = self.model.state_dict() # best is last\n",
    "        \n",
    "        landmk_all = landmk_tr+[torch.stack(landmk_test).squeeze()]\n",
    "        \n",
    "        ##Test time (assign labels to unlabaled data)\n",
    "        adata_test, eval_results = self.assign_labels(landmk_all[-1], evaluation_mode)\n",
    "        \n",
    "        adata = self.save_result(tr_iter, adata_test, save_all_embeddings)\n",
    "        \n",
    "        if evaluation_mode:\n",
    "            return adata, landmk_all, eval_results, training_history, latent_tracker\n",
    "        \n",
    "        return adata, landmk_all, training_history, latent_tracker\n",
    "    \n",
    "    def save_result(self, tr_iter, adata_test, save_all_embeddings):\n",
    "        \"\"\"Saving embeddings from labeled and unlabeled dataset, ground truth labels and \n",
    "        predictions to joint anndata object.\"\"\"\n",
    "        adata_all = []\n",
    "\n",
    "        if save_all_embeddings:\n",
    "            for task in range(len(tr_iter)): # saving embeddings from labeled dataset\n",
    "                task = int(task)\n",
    "                x, y, cells = next(tr_iter[task])\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                encoded,_ = self.model(x)\n",
    "                adata_all.append(self.pack_anndata(x, cells, encoded, gtruth=y))\n",
    "            \n",
    "        adata_all.append(adata_test)    \n",
    "        \n",
    "        if save_all_embeddings:\n",
    "            adata = adata_all[0].concatenate(adata_all[1:], \n",
    "                                             batch_key='experiment',\n",
    "                                             batch_categories=self.labeled_metadata+[self.unlabeled_metadata])\n",
    "        else:\n",
    "            adata = adata_all[0]\n",
    "\n",
    "            \n",
    "        adata.obsm['MARS_embedding'] = np.concatenate([a.uns['MARS_embedding'] for a in adata_all])\n",
    "        #adata.write('result_adata.h5ad')\n",
    "        \n",
    "        return adata\n",
    "    \n",
    "    def assign_labels(self, landmk_test, evaluation_mode):\n",
    "        \"\"\"Assigning cluster labels to the unlabeled meta-dataset.\n",
    "        test_iter: iterator over unlabeled dataset\n",
    "        landmk_test: landmarks in the unlabeled dataset\n",
    "        evaluation mode: computes clustering metrics if True\n",
    "        \"\"\"\n",
    "        #test_iter = iter(self.test_loader)\n",
    "            \n",
    "        torch.no_grad()\n",
    "        self.model.eval() # eval mode\n",
    "        \n",
    "        test_iter = iter(self.test_loader)\n",
    "        \n",
    "        x_test, y_true, cells = next(test_iter) # cells are needed because dataset is in random order\n",
    "        x_test = x_test.to(self.device)\n",
    "        \n",
    "        encoded_test,_ = self.model(x_test)\n",
    "        \n",
    "        ###Embedding space eucledian distance \n",
    "        dists = euclidean_dist(encoded_test, landmk_test)\n",
    "        \n",
    "        ###Prediction based on the minimal distance to learned landmark\n",
    "        y_pred = torch.min(dists, 1)[1]\n",
    "        \n",
    "        adata = self.pack_anndata(x_test, cells, encoded_test, y_true, y_pred)\n",
    "        \n",
    "        eval_results = None\n",
    "        if evaluation_mode:\n",
    "            eval_results = compute_scores(y_true, y_pred)\n",
    "            \n",
    "        return adata, eval_results\n",
    "    \n",
    "    \n",
    "    def pack_anndata(self, x_input, cells, embedding, gtruth=[], estimated=[]):\n",
    "        \"\"\"Pack results in anndata object.\n",
    "        x_input: gene expressions in the input space\n",
    "        cells: cell identifiers\n",
    "        embedding: resulting embedding of x_test using MARS\n",
    "        landmk: MARS estimated landmarks\n",
    "        gtruth: ground truth labels if available (default: empty list)\n",
    "        estimated: MARS estimated clusters if available (default: empty list)\n",
    "        \"\"\"\n",
    "        adata = anndata.AnnData(x_input.data.cpu().numpy())\n",
    "        adata.obs_names = cells\n",
    "        adata.var_names = self.genes\n",
    "        if len(estimated)!=0:\n",
    "            adata.obs['MARS_labels'] = pd.Categorical(values=estimated.cpu().numpy())\n",
    "        if len(gtruth)!=0:\n",
    "            adata.obs['truth_labels'] = pd.Categorical(values=gtruth.cpu().numpy())\n",
    "        adata.uns['MARS_embedding'] = embedding.data.cpu().numpy()\n",
    "        \n",
    "        return adata\n",
    "    \n",
    "    \n",
    "    def do_epoch(self, tr_iter, test_iter, optim, optim_landmk_test, landmk_tr, landmk_test):\n",
    "        \"\"\"\n",
    "        One training epoch.\n",
    "        tr_iter: iterator over labeled meta-data\n",
    "        test_iter: iterator over unlabeled meta-data\n",
    "        \n",
    "        optim: optimizer for embedding\n",
    "        optim_landmk_test: optimizer for test landmarks\n",
    "        \n",
    "        landmk_tr: landmarks of labeled meta-data from previous epoch\n",
    "        landmk_test: landmarks of unlabeled meta-data from previous epoch\n",
    "        \"\"\"\n",
    "        \n",
    "        latent_history = {'Train_latent':[], 'Train_label':[],\n",
    "                          'Test_latent':[], 'Test_label':[], \n",
    "                          'Train_landmark':[], 'Test_landmark':[]}\n",
    "        \n",
    "        loss_tracker = {'Test_anno':0,\n",
    "                        'Train_latent_loss':0, 'Train_reconstr_loss': 0, \n",
    "                        'Test_latent_loss':0, 'Test_reconstr_loss': 0}\n",
    "        \n",
    "        \n",
    "        self.set_requires_grad(False)\n",
    "        \n",
    "        for landmk in landmk_test:\n",
    "            landmk.requires_grad=False\n",
    "        \n",
    "        optim_landmk_test.zero_grad()\n",
    "        \n",
    "        # update centroids    \n",
    "        task_idx = torch.randperm(len(tr_iter)) ##shuffle per epoch\n",
    "        \n",
    "        ###Annotated landmark \n",
    "        for task in task_idx:\n",
    "            \n",
    "            task = int(task)\n",
    "            x, y, _ = next(tr_iter[task])\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            encoded,_ = self.model(x)\n",
    "            \n",
    "            curr_landmk_tr = compute_landmarks_tr(encoded, y, landmk_tr[task], tau=self.tau)\n",
    "            landmk_tr[task] = curr_landmk_tr.data # save landmarks\n",
    "            \n",
    "            latent_history['Train_landmark'].append(encoded)\n",
    "            latent_history['Train_label'].append(y)\n",
    "            \n",
    "        ###Unannotated landmark\n",
    "        for landmk in landmk_test:\n",
    "            landmk.requires_grad=True\n",
    "            \n",
    "        x,y_test,_ = next(test_iter)\n",
    "        x = x.to(self.device)\n",
    "        encoded,_ = self.model(x)\n",
    "        loss, args_count = loss_test(encoded, \n",
    "                                     torch.stack(landmk_test).squeeze(), \n",
    "                                     self.tau)\n",
    "        loss.backward()\n",
    "        optim_landmk_test.step()\n",
    "        \n",
    "        latent_history['Test_landmark'].append(encoded)\n",
    "        latent_history['Test_label'].append(y_test)\n",
    "        loss_tracker['Test_anno'] = loss\n",
    "        \n",
    "        # update embedding\n",
    "        self.set_requires_grad(True)\n",
    "        for landmk in landmk_test:\n",
    "            landmk.requires_grad=False\n",
    "            \n",
    "        optim.zero_grad()\n",
    "        total_accuracy = 0\n",
    "        total_loss = 0\n",
    "        ntasks = 0\n",
    "        mean_accuracy = 0\n",
    "        \n",
    "        ###Annotated set\n",
    "        task_idx = torch.randperm(len(tr_iter))\n",
    "        for task in task_idx:\n",
    "            task = int(task)\n",
    "            x, y, _ = next(tr_iter[task])\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            encoded, decoded = self.model(x)\n",
    "            ###Eucledian distance between embedding and landmark\n",
    "            loss, acc = loss_task(encoded, landmk_tr[task], y, criterion='dist')\n",
    "            total_loss += loss\n",
    "            total_accuracy += acc.item()\n",
    "            ntasks += 1\n",
    "            latent_history['Train_latent'].append(encoded)\n",
    "            \n",
    "            loss_tracker['Train_latent_loss'] = loss\n",
    "            loss_tracker['Train_reconstr_loss'] = reconstruction_loss(decoded, x)\n",
    "            \n",
    "        if ntasks>0:\n",
    "            mean_accuracy = total_accuracy / ntasks\n",
    "        \n",
    "        ##Un-Annotated\n",
    "        # test part\n",
    "        x,y,_ = next(test_iter)\n",
    "        x = x.to(self.device)\n",
    "        encoded, decoded = self.model(x)\n",
    "        loss,_ = loss_test(encoded, torch.stack(landmk_test).squeeze(), self.tau)\n",
    "        latent_history['Test_latent'].append(encoded)\n",
    "        \n",
    "        loss_tracker['Test_latent_loss'] = loss\n",
    "        loss_tracker['Test_reconstr_loss'] = reconstruction_loss(decoded, x)\n",
    "        \n",
    "        total_loss += loss\n",
    "        ntasks += 1\n",
    "    \n",
    "        mean_loss = total_loss / ntasks\n",
    "        \n",
    "        mean_loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        return mean_loss, mean_accuracy, landmk_tr, landmk_test, latent_history, loss_tracker\n",
    "    \n",
    "    def do_val_epoch(self, val_iter, prev_landmk):\n",
    "        \"\"\"One epoch of validation.\n",
    "        val_iter: iterator over validation set\n",
    "        prev_landmk: landmarks from previous epoch\n",
    "        \"\"\"\n",
    "        ntasks = len(val_iter)\n",
    "        task_idx = torch.randperm(ntasks)\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        \n",
    "        for task in task_idx:\n",
    "            x, y, _ = next(val_iter[task])\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            encoded = self.model(x)\n",
    "            loss, acc = loss_task(encoded, prev_landmk[task], y, criterion='dist')\n",
    "            total_loss += loss\n",
    "            total_accuracy += acc.item()\n",
    "        mean_accuracy = total_accuracy / ntasks\n",
    "        mean_loss = total_loss / ntasks\n",
    "        \n",
    "        return mean_loss, mean_accuracy\n",
    "    \n",
    "    \n",
    "    def set_requires_grad(self, requires_grad):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "    \n",
    "    def name_cell_types(self, adata, landmk_all, cell_name_mappings, \n",
    "                        top_match=5, umap_reduce_dim=True, ndim=10):\n",
    "        \"\"\"For each test cluster, estimate sigma and mean. \n",
    "        Fit Gaussian distribution with that mean and sigma\n",
    "        and calculate the probability of each of the train landmarks \n",
    "        to be the neighbor to the mean data point.\n",
    "        Normalization is performed with regards to all other landmarks in train.\"\"\"\n",
    "        \n",
    "        experiments = list(OrderedDict.fromkeys(list(adata.obs['experiment'])))\n",
    "        \n",
    "        encoded_tr = []\n",
    "        landmk_tr = []\n",
    "        landmk_tr_labels = []\n",
    "        for idx, exp in enumerate(experiments[:-1]):\n",
    "            tiss = adata[adata.obs['experiment'] == exp,:]\n",
    "            \n",
    "            if exp==self.unlabeled_metadata: \n",
    "                raise ValueError(\"Error: Unlabeled dataset needs to be last one in the input anndata object.\")\n",
    "                \n",
    "            encoded_tr.append(tiss.obsm['MARS_embedding'])\n",
    "            landmk_tr.append(landmk_all[idx])\n",
    "            landmk_tr_labels.append(np.unique(tiss.obs['truth_labels']))\n",
    "            \n",
    "        tiss = adata[adata.obs['experiment'] == self.unlabeled_metadata,:]\n",
    "        ypred_test = tiss.obs['MARS_labels']\n",
    "        uniq_ytest = np.unique(ypred_test)\n",
    "        encoded_test = tiss.obsm['MARS_embedding']\n",
    "        \n",
    "        landmk_tr_labels = np.concatenate(landmk_tr_labels)\n",
    "        encoded_tr = np.concatenate(encoded_tr)\n",
    "        landmk_tr = np.concatenate([p.cpu() for p in landmk_tr])\n",
    "        if  umap_reduce_dim:\n",
    "            encoded_extend = np.concatenate((encoded_tr, encoded_test, landmk_tr))\n",
    "            adata = anndata.AnnData(encoded_extend)\n",
    "            sc.pp.neighbors(adata, n_neighbors=15, use_rep='X')\n",
    "            sc.tl.umap(adata, n_components=ndim)\n",
    "            encoded_extend = adata.obsm['X_umap']\n",
    "            n1 = len(encoded_tr)\n",
    "            n2 = n1 + len(encoded_test)\n",
    "            encoded_tr = encoded_extend[:n1,:]\n",
    "            encoded_test = encoded_extend[n1:n2,:]\n",
    "            landmk_tr = encoded_extend[n2:,:]\n",
    "        \n",
    "        interp_names = defaultdict(list)\n",
    "        for ytest in uniq_ytest:\n",
    "            print('\\nCluster label: {}'.format(str(ytest)))\n",
    "            idx = np.where(ypred_test==ytest)\n",
    "            subset_encoded = encoded_test[idx[0],:]\n",
    "            mean = np.expand_dims(np.mean(subset_encoded, axis=0),0)\n",
    "            \n",
    "            sigma  = self.estimate_sigma(subset_encoded)\n",
    "            \n",
    "            prob = np.exp(-np.power(distance.cdist(mean, landmk_tr, metric='euclidean'),2)/(2*sigma*sigma))\n",
    "            prob = np.squeeze(prob, 0)\n",
    "            normalizat = np.sum(prob)\n",
    "            if normalizat==0:\n",
    "                print('Unassigned')\n",
    "                interp_names[ytest].append(\"unassigned\")\n",
    "                continue\n",
    "            \n",
    "            prob = np.divide(prob, normalizat)\n",
    "            \n",
    "            uniq_tr = np.unique(landmk_tr_labels)\n",
    "            prob_unique = []\n",
    "            for cell_type in uniq_tr: # sum probabilities of same landmarks\n",
    "                prob_unique.append(np.sum(prob[np.where(landmk_tr_labels==cell_type)]))\n",
    "            \n",
    "            sorted = np.argsort(prob_unique, axis=0)\n",
    "            best = uniq_tr[sorted[-top_match:]]\n",
    "            sortedv = np.sort(prob_unique, axis=0)\n",
    "            sortedv = sortedv[-top_match:]\n",
    "            for idx, b in enumerate(best):\n",
    "                interp_names[ytest].append((cell_name_mappings[b], sortedv[idx]))\n",
    "                print('{}: {}'.format(cell_name_mappings[b], sortedv[idx]))\n",
    "                \n",
    "        return interp_names\n",
    "    \n",
    "    \n",
    "    def estimate_sigma(self, dataset):\n",
    "        nex = dataset.shape[0]\n",
    "        dst = []\n",
    "        for i in range(nex):\n",
    "            for j in range(i+1, nex):\n",
    "                dst.append(distance.euclidean(dataset[i,:],dataset[j,:]))\n",
    "        return np.std(dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MARS_loss_tracker (training_history, save):\n",
    "    \n",
    "    loss = pd.DataFrame.from_dict(training_history['Loss_tracker'])\n",
    "    epochs = loss.shape[0]\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2, figsize=(11, 6.5)) \n",
    "    \n",
    "    ax[0].plot(np.arange(epochs), loss['Test_anno'].values, label = \"Unannotated landmark loss\")\n",
    "    ax[0].plot(np.arange(epochs), loss['Train_latent_loss'].values, label = \"Train latent loss\")\n",
    "    ax[0].plot(np.arange(epochs), loss['Test_latent_loss'].values, label = \"Test latent loss\")\n",
    "    ax[0].set_title(\"MARS original loss\")\n",
    "    ax[0].set_xlabel(\"Epochs\")\n",
    "    ax[0].set_ylabel(\"Loss\")\n",
    "    ax[0].legend()\n",
    "    \n",
    "\n",
    "    ax[1].plot(np.arange(epochs), loss['Train_reconstr_loss'].values, label = \"Train reconstruction loss\")\n",
    "    ax[1].plot(np.arange(epochs), loss['Test_reconstr_loss'].values, label = \"Test reconstruction loss\")\n",
    "    ax[1].set_title(\"Reconstruction loss\")\n",
    "    ax[1].set_xlabel(\"Epochs\")\n",
    "    ax[1].legend()\n",
    "    \n",
    "    \n",
    "    fig.suptitle(\"MARS loss across epochs\",fontsize=16,y=1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if save == True:\n",
    "        fig.savefig(\"MARS_loss_tracker.png\")\n",
    "\n",
    "\n",
    "def MARS_history(history_train, save):\n",
    "    \n",
    "    ###Loss function trajectory\n",
    "    loss = []\n",
    "    for l in history_train['Loss']:\n",
    "        loss.append(l.detach().numpy())\n",
    "    \n",
    "    accuracy = history_train['Accuracy']\n",
    "    \n",
    "    ##Line plot visuzlie loss per epoch \n",
    "    fig, ax = plt.subplots(1,2, figsize=(11, 6.5)) \n",
    "    ax[0].scatter(np.arange(len(loss)),loss)\n",
    "    ax[0].plot(np.arange(len(loss)),loss)\n",
    "    ax[0].set_xlabel(\"Epochs\", fontsize=12)\n",
    "    ax[0].set_ylabel(\"Training loss\", fontsize=12)\n",
    "    ax[0].set_title(\"Training loss\", fontsize=14)\n",
    "    \n",
    "    ax[1].scatter(np.arange(len(accuracy)),accuracy)\n",
    "    ax[1].plot(np.arange(len(accuracy)),accuracy)\n",
    "    ax[1].set_xlabel(\"Epochs\", fontsize=12)\n",
    "    ax[1].set_ylabel(\"Training accuracy\", fontsize=12)\n",
    "    ax[1].set_title(\"Training accuracy\", fontsize=14)\n",
    "    \n",
    "    fig.suptitle(\"Visualization of MARS metrics across epochs\",fontsize=16,y=1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()    \n",
    "    \n",
    "    if save == True:\n",
    "        fig.savefig(\"MARS_history.png\")\n",
    "\n",
    "    \n",
    "def MARS_latent_pca(latent_tracker , epoch_num):\n",
    "    ###Latent space \n",
    "    \n",
    "    train_latent = latent_tracker[epoch_num]['Train_latent'][0].detach().numpy()\n",
    "    train_label = latent_tracker[epoch_num]['Train_label'][0].detach().numpy()\n",
    "    \n",
    "    test_latent = latent_tracker[epoch_num]['Test_latent'][0].detach().numpy()\n",
    "    test_label = latent_tracker[epoch_num]['Test_label'][0].detach().numpy()\n",
    "    \n",
    "    ###Validation PCA visualization \n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(train_latent)\n",
    "    pca_df=pd.DataFrame(pca.transform(train_latent))\n",
    "    pca_df.columns=['PC1','PC2']\n",
    "    pca_df=pca_df.copy()\n",
    "    pca_df['dbscan']=train_label\n",
    "    \n",
    "    plt.subplots(1, figsize=(10,8))\n",
    "    sns.scatterplot(data=pca_df, x=\"PC1\", y=\"PC2\", hue=\"dbscan\", palette=\"deep\")\n",
    "    plt.title(\"Training latent space\")\n",
    "    plt.show()  \n",
    "    \n",
    "    ###Validation PCA visualization \n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(test_latent)\n",
    "    pca_df=pd.DataFrame(pca.transform(test_latent))\n",
    "    pca_df.columns=['PC1','PC2']\n",
    "    pca_df=pca_df.copy()\n",
    "    pca_df['dbscan']=test_label\n",
    "    \n",
    "    plt.subplots(1, figsize=(10,8))\n",
    "    sns.scatterplot(data=pca_df, x=\"PC1\", y=\"PC2\", hue=\"dbscan\", palette=\"deep\")\n",
    "    plt.title(\"Testing latent space\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def MARS_latent_umap(adata,save, plot_gene_list = ['MARS_labels','experiment',\n",
    "                                              'CST3','CCL5','FCGR3A','NKG7','MS4A1','CD79A','CD8A']  ):\n",
    "    \n",
    "    sc.pp.neighbors(adata, n_neighbors=30, use_rep='MARS_embedding')\n",
    "    sc.tl.umap(adata)\n",
    "    sc.pl.umap(adata, color=plot_gene_list, ncols=2 )\n",
    "\n",
    "    sub_data = adata[adata.obs['experiment']=='Unannotated',]\n",
    "    if save == True:\n",
    "        sc.pl.umap(sub_data, color=plot_gene_list, ncols=2, save = '.png' )\n",
    "    else: \n",
    "        sc.pl.umap(sub_data, color=plot_gene_list, ncols=2 )\n",
    "        \n",
    "        \n",
    "def cell_type_assign(adata, save):\n",
    "\n",
    "    num2name={1:'Malignant',2:'Endothelial',3:'T cell', 4:'Macrophage',\n",
    "              5:'B cell',6:'CAF',7:'Dendritic',8:'Plasma B', 9:'NK'}\n",
    "\n",
    "    adata.obs['label_name']=adata.obs['truth_labels'].map(num2name)  \n",
    "    untable = adata.obs.loc[adata.obs['experiment']=='Unannotated',:]\n",
    "    \n",
    "    groundtruth_sum = pd.crosstab(untable['truth_labels'], untable['MARS_labels']).sum(axis=1)\n",
    "    plot = pd.crosstab(untable['truth_labels'], untable['MARS_labels']).div(groundtruth_sum/100, axis=0).plot.bar(figsize=(17,5), width=1)\n",
    "    plot.set_xticklabels(['Malignant','Endothelial', 'T cell', 'Macrophage', 'B cell',  'CAF', 'Dendritic', 'Plasma B', 'NK' ])\n",
    "    plot.legend(title=\"MARS labels\")\n",
    "    plot.set_xlabel(\"Ground truth labels\", fontsize=18)\n",
    "    plot.set_title(\"Distribution of MARS labels and Ground Truths\", fontsize=24)\n",
    "    plot.set_ylabel(\"% Truth labels in MARS labels\", fontsize=18)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    if save==True:\n",
    "        plot.get_figure().savefig(\"cell_type_groundtruth.png\")\n",
    "    \n",
    "    groundtruth_sum = pd.crosstab(untable['MARS_labels'], untable['label_name']).sum(axis=1)\n",
    "    plot = pd.crosstab(untable['MARS_labels'], untable['label_name']).div(groundtruth_sum/100, axis=0).plot.bar(figsize=(17,5), width=1)\n",
    "    plot.legend(title=\"Ground truth\")\n",
    "    plot.set_xlabel(\"MARS labels\", fontsize=18)\n",
    "    plot.set_ylabel(\"% MARS labels in Truth labels\", fontsize=18)\n",
    "    plt.show()\n",
    "    \n",
    "    if save==True:\n",
    "        plot.get_figure().savefig(\"cell_type_MARS.png\")\n",
    "\n",
    "        \n",
    "def MARS_silhouette(adata, save):\n",
    "    \n",
    "    \n",
    "    anno_data = adata[adata.obs['experiment'] == \"Annotated\", :]\n",
    "    unanno_data = adata[adata.obs['experiment'] == \"Unannotated\", :]\n",
    "    \n",
    "    anno_obs = anno_data.obs\n",
    "    unanno_obs = unanno_data.obs\n",
    "    \n",
    "    ###Silhouette\n",
    "    train_latent = anno_data.obsm['MARS_embedding']\n",
    "    train_sil=silhouette_samples(train_latent, anno_obs['truth_labels'].values)\n",
    "    \n",
    "    val_latent = unanno_data.obsm['MARS_embedding']\n",
    "    val_sil=silhouette_samples(val_latent, unanno_obs['truth_labels'].values)\n",
    "    \n",
    "    n_clusters=len(Counter(adata.obs['truth_labels'].values).keys())\n",
    "    \n",
    "    ###Plot silhouette score for train and test \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 10)\n",
    "        \n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_ylim([0, len(train_latent) + (n_clusters + 1) * 10])\n",
    "    \n",
    "    ax2.set_xlim([-0.1, 1])\n",
    "    ax2.set_ylim([0, len(val_latent) + (n_clusters + 1) * 10])\n",
    "         \n",
    "    y_lower = 10\n",
    "    val_y_lower = 10\n",
    "    \n",
    "    \n",
    "    name2num={'Malignant':1, 'Endothelial':2, 'T cell':3, \n",
    "          'Macrophage':4, 'B cell':5,'CAF':6, \n",
    "          'Dendritic':7 ,'Plasma B':8, 'NK':9}\n",
    "\n",
    "    cluster_list = list(name2num.keys())\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        \n",
    "        \n",
    "        #cluster_label = list(Counter(adata.obs['truth_labels']).keys())[i]\n",
    "        cluster_label = cluster_list[i]\n",
    "        \n",
    "        #############################################################\n",
    "        ###############Training \n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        train_ith_cluster_silhouette_values = train_sil[anno_obs['label_name'] == cluster_label]\n",
    "        train_ith_cluster_silhouette_values.sort()\n",
    "        \n",
    "        size_cluster_i = train_ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        #color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, train_ith_cluster_silhouette_values) \n",
    "        #edgecolor=color, facecolor=color\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(0.8, y_lower + 0.5 * size_cluster_i, str(cluster_label))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "        \n",
    "        #############################################################\n",
    "        ###############Validation \n",
    "        val_ith_cluster_silhouette_values = val_sil[unanno_obs['label_name'] == cluster_label]\n",
    "        val_ith_cluster_silhouette_values.sort()\n",
    "        \n",
    "        val_size_cluster_i = val_ith_cluster_silhouette_values.shape[0]\n",
    "        val_y_upper = val_y_lower + val_size_cluster_i\n",
    "\n",
    "        #color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax2.fill_betweenx(np.arange(val_y_lower, val_y_upper),\n",
    "                          0, val_ith_cluster_silhouette_values)\n",
    "        # edgecolor=color, facecolor=color\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax2.text(0.8, val_y_lower + 0.5 * val_size_cluster_i, str(cluster_label))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        val_y_lower = val_y_upper + 10  # 10 for the 0 samples\n",
    "     \n",
    "    ax1.set_title(\"Training silhouette plot for known labels\")\n",
    "    ax1.set_xlabel(\"Silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "    \n",
    "    ax2.set_title(\"Validation silhouette plot for known labels\")\n",
    "    ax2.set_xlabel(\"Silhouette coefficient values\")\n",
    "    ax2.set_ylabel(\"Cluster label\")\n",
    "    \n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    train_avg=silhouette_score(train_latent, anno_obs['truth_labels'].values)\n",
    "    val_avg=silhouette_score(val_latent, unanno_obs['truth_labels'].values)\n",
    "    \n",
    "    ax1.axvline(x=train_avg, color=\"red\", linestyle=\"--\")\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    \n",
    "    ax2.axvline(x=val_avg, color=\"red\", linestyle=\"--\")\n",
    "    ax2.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax2.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    \n",
    "    plt.suptitle(\"Silhouette scores for MARS latent space\", fontsize=14, fontweight='bold')\n",
    "    plt.show()  \n",
    "    \n",
    "    if save ==True:\n",
    "        fig.savefig(\"MARS_silhouette.png\")\n",
    "        \n",
    "    return train_sil, val_sil, train_avg, val_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and process scATAC-seq data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatac=sc.read_h5ad(\"scAtac_labeled.h5ad\")\n",
    "\n",
    "sample2response={'620':'PD', \n",
    "                 '776':'PD',\n",
    "                 '856_A':'PD',\n",
    "                 '856_B':'PD',\n",
    "                 '1224_post_Ft':'PD', \n",
    "                 '1224_post_Cln':'PD',\n",
    "                 '509_2':'R',\n",
    "                 '509_post':'R',\n",
    "                 '1227':'R',\n",
    "                 '1009':'R',\n",
    "                 '1130':'R'}\n",
    "\n",
    "scatac.obs['response']=scatac.obs['orig.ident'].map(sample2response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatac.obs['dbscan']=np.zeros(scatac.X.shape[0])\n",
    "\n",
    "for ind, name in enumerate(Counter(scatac.obs['predicted.id'])):\n",
    "    if name == \"malignant\":\n",
    "        scatac.obs.loc[scatac.obs['predicted.id']==name,'dbscan']=51\n",
    "    if name == \"Endothelial\":\n",
    "        scatac.obs.loc[scatac.obs['predicted.id']==name,'dbscan']=52\n",
    "    if name == \"T\":\n",
    "        scatac.obs.loc[scatac.obs['predicted.id']==name,'dbscan']=53\n",
    "    if name == \"Macrophage\":\n",
    "        scatac.obs.loc[scatac.obs['predicted.id']==name,'dbscan']=54\n",
    "    if name == \"B\":\n",
    "        scatac.obs.loc[scatac.obs['predicted.id']==name,'dbscan']=55\n",
    "    if name == \"CAF\":\n",
    "        scatac.obs.loc[scatac.obs['predicted.id']==name,'dbscan']=56\n",
    "    if name == \"Dendritic\":\n",
    "        scatac.obs.loc[scatac.obs['predicted.id']==name,'dbscan']=57\n",
    "    if name == \"Plasma B\":\n",
    "        scatac.obs.loc[scatac.obs['predicted.id']==name,'dbscan']=58\n",
    "    if name == \"NK\":\n",
    "        scatac.obs.loc[scatac.obs['predicted.id']==name,'dbscan']=59\n",
    "\n",
    "scatac.obs['dbscan'] = scatac.obs['dbscan'].astype('int32')\n",
    "\n",
    "num2name={51:'Malignant',52:'Endothelial',53:'T cell', 54:'Macrophage',\n",
    "          55:'B cell',56:'CAF',57:'Dendritic',58:'Plasma B', 59:'NK'}\n",
    "\n",
    "scatac.obs['annotated_celltype']=scatac.obs['dbscan'].map(num2name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig.ident</th>\n",
       "      <th>nCount_ATAC</th>\n",
       "      <th>nFeature_ATAC</th>\n",
       "      <th>total</th>\n",
       "      <th>duplicate</th>\n",
       "      <th>chimeric</th>\n",
       "      <th>unmapped</th>\n",
       "      <th>lowmapq</th>\n",
       "      <th>mitochondrial</th>\n",
       "      <th>passed_filters</th>\n",
       "      <th>...</th>\n",
       "      <th>prediction.score.Macrophage</th>\n",
       "      <th>prediction.score.B</th>\n",
       "      <th>prediction.score.CAF</th>\n",
       "      <th>prediction.score.Dendritic</th>\n",
       "      <th>prediction.score.Plasma.B</th>\n",
       "      <th>prediction.score.NK</th>\n",
       "      <th>prediction.score.max</th>\n",
       "      <th>response</th>\n",
       "      <th>dbscan</th>\n",
       "      <th>annotated_celltype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pt620_AAACGAAGTCGACTGC-1</th>\n",
       "      <td>620</td>\n",
       "      <td>1269.0</td>\n",
       "      <td>1155</td>\n",
       "      <td>63791</td>\n",
       "      <td>1678</td>\n",
       "      <td>262</td>\n",
       "      <td>56754</td>\n",
       "      <td>2750</td>\n",
       "      <td>3</td>\n",
       "      <td>2344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017169</td>\n",
       "      <td>0.975328</td>\n",
       "      <td>PD</td>\n",
       "      <td>53</td>\n",
       "      <td>T cell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt620_AAACTCGTCGCTCTAC-1</th>\n",
       "      <td>620</td>\n",
       "      <td>1186.0</td>\n",
       "      <td>1072</td>\n",
       "      <td>55879</td>\n",
       "      <td>1949</td>\n",
       "      <td>268</td>\n",
       "      <td>49294</td>\n",
       "      <td>2289</td>\n",
       "      <td>8</td>\n",
       "      <td>2071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.961465</td>\n",
       "      <td>PD</td>\n",
       "      <td>53</td>\n",
       "      <td>T cell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt620_AAATGCCCAACTACTG-1</th>\n",
       "      <td>620</td>\n",
       "      <td>4069.0</td>\n",
       "      <td>3479</td>\n",
       "      <td>209588</td>\n",
       "      <td>3038</td>\n",
       "      <td>860</td>\n",
       "      <td>188302</td>\n",
       "      <td>8281</td>\n",
       "      <td>24</td>\n",
       "      <td>9083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.751651</td>\n",
       "      <td>PD</td>\n",
       "      <td>51</td>\n",
       "      <td>Malignant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt620_AAATGCCCACCTGGTG-1</th>\n",
       "      <td>620</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>919</td>\n",
       "      <td>39886</td>\n",
       "      <td>1135</td>\n",
       "      <td>190</td>\n",
       "      <td>35186</td>\n",
       "      <td>1484</td>\n",
       "      <td>1</td>\n",
       "      <td>1890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009672</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006184</td>\n",
       "      <td>0.941880</td>\n",
       "      <td>PD</td>\n",
       "      <td>53</td>\n",
       "      <td>T cell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt620_AACAGTCGTTGGCTAT-1</th>\n",
       "      <td>620</td>\n",
       "      <td>1149.0</td>\n",
       "      <td>1089</td>\n",
       "      <td>48830</td>\n",
       "      <td>824</td>\n",
       "      <td>275</td>\n",
       "      <td>41939</td>\n",
       "      <td>2916</td>\n",
       "      <td>3</td>\n",
       "      <td>2873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013815</td>\n",
       "      <td>0.953158</td>\n",
       "      <td>PD</td>\n",
       "      <td>53</td>\n",
       "      <td>T cell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt509_2_TTTGTGTCATCATCGA-1</th>\n",
       "      <td>509_2</td>\n",
       "      <td>1908.0</td>\n",
       "      <td>1799</td>\n",
       "      <td>7461</td>\n",
       "      <td>4035</td>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>525</td>\n",
       "      <td>0</td>\n",
       "      <td>2861</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060133</td>\n",
       "      <td>0.768885</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.768885</td>\n",
       "      <td>R</td>\n",
       "      <td>55</td>\n",
       "      <td>B cell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt509_2_TTTGTGTGTAACGGCA-1</th>\n",
       "      <td>509_2</td>\n",
       "      <td>5457.0</td>\n",
       "      <td>4326</td>\n",
       "      <td>22576</td>\n",
       "      <td>11667</td>\n",
       "      <td>13</td>\n",
       "      <td>146</td>\n",
       "      <td>1531</td>\n",
       "      <td>2</td>\n",
       "      <td>9217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.998298</td>\n",
       "      <td>R</td>\n",
       "      <td>53</td>\n",
       "      <td>T cell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt509_2_TTTGTGTGTAGCAGGT-1</th>\n",
       "      <td>509_2</td>\n",
       "      <td>7525.0</td>\n",
       "      <td>5830</td>\n",
       "      <td>33581</td>\n",
       "      <td>21251</td>\n",
       "      <td>18</td>\n",
       "      <td>217</td>\n",
       "      <td>1664</td>\n",
       "      <td>59</td>\n",
       "      <td>10372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>R</td>\n",
       "      <td>53</td>\n",
       "      <td>T cell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt509_2_TTTGTGTGTTAGGAAT-1</th>\n",
       "      <td>509_2</td>\n",
       "      <td>3874.0</td>\n",
       "      <td>3400</td>\n",
       "      <td>12213</td>\n",
       "      <td>6014</td>\n",
       "      <td>6</td>\n",
       "      <td>79</td>\n",
       "      <td>786</td>\n",
       "      <td>0</td>\n",
       "      <td>5328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923745</td>\n",
       "      <td>R</td>\n",
       "      <td>55</td>\n",
       "      <td>B cell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt509_2_TTTGTGTTCTCGGCGA-1</th>\n",
       "      <td>509_2</td>\n",
       "      <td>2954.0</td>\n",
       "      <td>2681</td>\n",
       "      <td>15966</td>\n",
       "      <td>9660</td>\n",
       "      <td>5</td>\n",
       "      <td>113</td>\n",
       "      <td>1106</td>\n",
       "      <td>29</td>\n",
       "      <td>5053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006218</td>\n",
       "      <td>0.993782</td>\n",
       "      <td>R</td>\n",
       "      <td>53</td>\n",
       "      <td>T cell</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43956 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           orig.ident  nCount_ATAC  nFeature_ATAC   total  \\\n",
       "pt620_AAACGAAGTCGACTGC-1          620       1269.0           1155   63791   \n",
       "pt620_AAACTCGTCGCTCTAC-1          620       1186.0           1072   55879   \n",
       "pt620_AAATGCCCAACTACTG-1          620       4069.0           3479  209588   \n",
       "pt620_AAATGCCCACCTGGTG-1          620       1002.0            919   39886   \n",
       "pt620_AACAGTCGTTGGCTAT-1          620       1149.0           1089   48830   \n",
       "...                               ...          ...            ...     ...   \n",
       "pt509_2_TTTGTGTCATCATCGA-1      509_2       1908.0           1799    7461   \n",
       "pt509_2_TTTGTGTGTAACGGCA-1      509_2       5457.0           4326   22576   \n",
       "pt509_2_TTTGTGTGTAGCAGGT-1      509_2       7525.0           5830   33581   \n",
       "pt509_2_TTTGTGTGTTAGGAAT-1      509_2       3874.0           3400   12213   \n",
       "pt509_2_TTTGTGTTCTCGGCGA-1      509_2       2954.0           2681   15966   \n",
       "\n",
       "                            duplicate  chimeric  unmapped  lowmapq  \\\n",
       "pt620_AAACGAAGTCGACTGC-1         1678       262     56754     2750   \n",
       "pt620_AAACTCGTCGCTCTAC-1         1949       268     49294     2289   \n",
       "pt620_AAATGCCCAACTACTG-1         3038       860    188302     8281   \n",
       "pt620_AAATGCCCACCTGGTG-1         1135       190     35186     1484   \n",
       "pt620_AACAGTCGTTGGCTAT-1          824       275     41939     2916   \n",
       "...                               ...       ...       ...      ...   \n",
       "pt509_2_TTTGTGTCATCATCGA-1       4035         3        37      525   \n",
       "pt509_2_TTTGTGTGTAACGGCA-1      11667        13       146     1531   \n",
       "pt509_2_TTTGTGTGTAGCAGGT-1      21251        18       217     1664   \n",
       "pt509_2_TTTGTGTGTTAGGAAT-1       6014         6        79      786   \n",
       "pt509_2_TTTGTGTTCTCGGCGA-1       9660         5       113     1106   \n",
       "\n",
       "                            mitochondrial  passed_filters  ...  \\\n",
       "pt620_AAACGAAGTCGACTGC-1                3            2344  ...   \n",
       "pt620_AAACTCGTCGCTCTAC-1                8            2071  ...   \n",
       "pt620_AAATGCCCAACTACTG-1               24            9083  ...   \n",
       "pt620_AAATGCCCACCTGGTG-1                1            1890  ...   \n",
       "pt620_AACAGTCGTTGGCTAT-1                3            2873  ...   \n",
       "...                                   ...             ...  ...   \n",
       "pt509_2_TTTGTGTCATCATCGA-1              0            2861  ...   \n",
       "pt509_2_TTTGTGTGTAACGGCA-1              2            9217  ...   \n",
       "pt509_2_TTTGTGTGTAGCAGGT-1             59           10372  ...   \n",
       "pt509_2_TTTGTGTGTTAGGAAT-1              0            5328  ...   \n",
       "pt509_2_TTTGTGTTCTCGGCGA-1             29            5053  ...   \n",
       "\n",
       "                           prediction.score.Macrophage  prediction.score.B  \\\n",
       "pt620_AAACGAAGTCGACTGC-1                      0.000000            0.000000   \n",
       "pt620_AAACTCGTCGCTCTAC-1                      0.010110            0.000000   \n",
       "pt620_AAATGCCCAACTACTG-1                      0.000000            0.000000   \n",
       "pt620_AAATGCCCACCTGGTG-1                      0.009672            0.001498   \n",
       "pt620_AACAGTCGTTGGCTAT-1                      0.010078            0.000000   \n",
       "...                                                ...                 ...   \n",
       "pt509_2_TTTGTGTCATCATCGA-1                    0.060133            0.768885   \n",
       "pt509_2_TTTGTGTGTAACGGCA-1                    0.000000            0.001702   \n",
       "pt509_2_TTTGTGTGTAGCAGGT-1                    0.000000            0.000000   \n",
       "pt509_2_TTTGTGTGTTAGGAAT-1                    0.000000            0.923745   \n",
       "pt509_2_TTTGTGTTCTCGGCGA-1                    0.000000            0.000000   \n",
       "\n",
       "                            prediction.score.CAF  prediction.score.Dendritic  \\\n",
       "pt620_AAACGAAGTCGACTGC-1                     0.0                    0.000000   \n",
       "pt620_AAACTCGTCGCTCTAC-1                     0.0                    0.000000   \n",
       "pt620_AAATGCCCAACTACTG-1                     0.0                    0.000000   \n",
       "pt620_AAATGCCCACCTGGTG-1                     0.0                    0.000000   \n",
       "pt620_AACAGTCGTTGGCTAT-1                     0.0                    0.000000   \n",
       "...                                          ...                         ...   \n",
       "pt509_2_TTTGTGTCATCATCGA-1                   0.0                    0.023013   \n",
       "pt509_2_TTTGTGTGTAACGGCA-1                   0.0                    0.000000   \n",
       "pt509_2_TTTGTGTGTAGCAGGT-1                   0.0                    0.000000   \n",
       "pt509_2_TTTGTGTGTTAGGAAT-1                   0.0                    0.076255   \n",
       "pt509_2_TTTGTGTTCTCGGCGA-1                   0.0                    0.000000   \n",
       "\n",
       "                            prediction.score.Plasma.B  prediction.score.NK  \\\n",
       "pt620_AAACGAAGTCGACTGC-1                          0.0             0.017169   \n",
       "pt620_AAACTCGTCGCTCTAC-1                          0.0             0.000000   \n",
       "pt620_AAATGCCCAACTACTG-1                          0.0             0.000000   \n",
       "pt620_AAATGCCCACCTGGTG-1                          0.0             0.006184   \n",
       "pt620_AACAGTCGTTGGCTAT-1                          0.0             0.013815   \n",
       "...                                               ...                  ...   \n",
       "pt509_2_TTTGTGTCATCATCGA-1                        0.0             0.001110   \n",
       "pt509_2_TTTGTGTGTAACGGCA-1                        0.0             0.000000   \n",
       "pt509_2_TTTGTGTGTAGCAGGT-1                        0.0             0.000000   \n",
       "pt509_2_TTTGTGTGTTAGGAAT-1                        0.0             0.000000   \n",
       "pt509_2_TTTGTGTTCTCGGCGA-1                        0.0             0.006218   \n",
       "\n",
       "                            prediction.score.max  response  dbscan  \\\n",
       "pt620_AAACGAAGTCGACTGC-1                0.975328        PD      53   \n",
       "pt620_AAACTCGTCGCTCTAC-1                0.961465        PD      53   \n",
       "pt620_AAATGCCCAACTACTG-1                0.751651        PD      51   \n",
       "pt620_AAATGCCCACCTGGTG-1                0.941880        PD      53   \n",
       "pt620_AACAGTCGTTGGCTAT-1                0.953158        PD      53   \n",
       "...                                          ...       ...     ...   \n",
       "pt509_2_TTTGTGTCATCATCGA-1              0.768885         R      55   \n",
       "pt509_2_TTTGTGTGTAACGGCA-1              0.998298         R      53   \n",
       "pt509_2_TTTGTGTGTAGCAGGT-1              1.000000         R      53   \n",
       "pt509_2_TTTGTGTGTTAGGAAT-1              0.923745         R      55   \n",
       "pt509_2_TTTGTGTTCTCGGCGA-1              0.993782         R      53   \n",
       "\n",
       "                            annotated_celltype  \n",
       "pt620_AAACGAAGTCGACTGC-1                T cell  \n",
       "pt620_AAACTCGTCGCTCTAC-1                T cell  \n",
       "pt620_AAATGCCCAACTACTG-1             Malignant  \n",
       "pt620_AAATGCCCACCTGGTG-1                T cell  \n",
       "pt620_AACAGTCGTTGGCTAT-1                T cell  \n",
       "...                                        ...  \n",
       "pt509_2_TTTGTGTCATCATCGA-1              B cell  \n",
       "pt509_2_TTTGTGTGTAACGGCA-1              T cell  \n",
       "pt509_2_TTTGTGTGTAGCAGGT-1              T cell  \n",
       "pt509_2_TTTGTGTGTTAGGAAT-1              B cell  \n",
       "pt509_2_TTTGTGTTCTCGGCGA-1              T cell  \n",
       "\n",
       "[43956 rows x 44 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scatac.obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use melanoma scRNA-seq & annotated colon scRNA-seq data to predict unannotated melanoma scATAC labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scRNA-data from melanoma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mit_data = sc.read_h5ad(\"scRNA_mit.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2response={'620':'PD', '776_on':'PD','856_A':'PD','856_B':'PD',\n",
    "                 '1224_post-FT':'PD', '1224_post-Cln':'PD',\n",
    "                 '509_2':'R',\n",
    "                 '509_post':'R',\n",
    "                 '1227_pre':'R',\n",
    "                 '1009_on':'R',\n",
    "                 '1130':'R'}\n",
    "\n",
    "mit_data.obs['response']=mit_data.obs['orig.ident'].map(sample2response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mit_data.obs.loc[mit_data.obs['leiden'] == '7' , 'dbscan'] = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num2name={1:'Malignant',2:'Endothelial',3:'T cell', 4:'Macrophage',5:'Malignant',\n",
    "          6:'B cell', 7: 'Malignant', 8:'CAF',9:'Dendritic',10:'Plasma B', 11:'NK'}\n",
    "\n",
    "mit_data.obs['annotated_celltype']=mit_data.obs['dbscan'].map(num2name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "name2num={'Malignant':1, 'Endothelial':2, 'T cell':3, \n",
    "          'Macrophage':4, 'B cell':5,'CAF':6, \n",
    "          'Dendritic':7 ,'Plasma B':8, 'NK':9}\n",
    "\n",
    "mit_data.obs['new_dbscan']=mit_data.obs['annotated_celltype'].map(name2num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig.ident</th>\n",
       "      <th>nCount_RNA</th>\n",
       "      <th>nFeature_RNA</th>\n",
       "      <th>percent.mt</th>\n",
       "      <th>nCount_SCT</th>\n",
       "      <th>nFeature_SCT</th>\n",
       "      <th>SCT_snn_res.0.4</th>\n",
       "      <th>seurat_clusters</th>\n",
       "      <th>cell.type</th>\n",
       "      <th>hc_euclidean_5</th>\n",
       "      <th>hc_euclidean_10</th>\n",
       "      <th>hc_euclidean_15</th>\n",
       "      <th>hc_corelation_5</th>\n",
       "      <th>hc_corelation_10</th>\n",
       "      <th>hc_corelation_15</th>\n",
       "      <th>dbscan</th>\n",
       "      <th>response</th>\n",
       "      <th>leiden</th>\n",
       "      <th>annotated_celltype</th>\n",
       "      <th>new_dbscan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PT620_AAACGCTGTGCATCTA-1</th>\n",
       "      <td>620</td>\n",
       "      <td>603.729939</td>\n",
       "      <td>451</td>\n",
       "      <td>3.808450</td>\n",
       "      <td>3084.0</td>\n",
       "      <td>775</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PD</td>\n",
       "      <td>13</td>\n",
       "      <td>Malignant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PT620_AAAGAACAGGTACCTT-1</th>\n",
       "      <td>620</td>\n",
       "      <td>1621.040015</td>\n",
       "      <td>962</td>\n",
       "      <td>8.686491</td>\n",
       "      <td>3639.0</td>\n",
       "      <td>991</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PD</td>\n",
       "      <td>13</td>\n",
       "      <td>Malignant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PT620_AATGGCTGTAAGATAC-1</th>\n",
       "      <td>620</td>\n",
       "      <td>1921.037679</td>\n",
       "      <td>1159</td>\n",
       "      <td>9.822692</td>\n",
       "      <td>3850.0</td>\n",
       "      <td>1187</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PD</td>\n",
       "      <td>13</td>\n",
       "      <td>Malignant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PT620_AATTTCCAGGCCCAAA-1</th>\n",
       "      <td>620</td>\n",
       "      <td>3453.070497</td>\n",
       "      <td>1784</td>\n",
       "      <td>9.505623</td>\n",
       "      <td>3833.0</td>\n",
       "      <td>1784</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PD</td>\n",
       "      <td>13</td>\n",
       "      <td>Malignant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PT620_ACCACAACAGGACAGT-1</th>\n",
       "      <td>620</td>\n",
       "      <td>2145.965853</td>\n",
       "      <td>1255</td>\n",
       "      <td>9.435712</td>\n",
       "      <td>3555.0</td>\n",
       "      <td>1269</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PD</td>\n",
       "      <td>13</td>\n",
       "      <td>Malignant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PT1009_TTGGGATGTTTACCAG-1</th>\n",
       "      <td>1009_on</td>\n",
       "      <td>8558.603451</td>\n",
       "      <td>2433</td>\n",
       "      <td>6.825769</td>\n",
       "      <td>5008.0</td>\n",
       "      <td>2190</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>R</td>\n",
       "      <td>20</td>\n",
       "      <td>B cell</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PT1009_TTGTTCACACTGCGAC-1</th>\n",
       "      <td>1009_on</td>\n",
       "      <td>16172.649320</td>\n",
       "      <td>4107</td>\n",
       "      <td>3.297175</td>\n",
       "      <td>4710.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>R</td>\n",
       "      <td>20</td>\n",
       "      <td>B cell</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PT1009_TTTAGTCAGCGTTACT-1</th>\n",
       "      <td>1009_on</td>\n",
       "      <td>5249.795876</td>\n",
       "      <td>1915</td>\n",
       "      <td>3.016424</td>\n",
       "      <td>4672.0</td>\n",
       "      <td>1914</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>R</td>\n",
       "      <td>20</td>\n",
       "      <td>B cell</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PT1009_TTTCATGCAGCTTCGG-1</th>\n",
       "      <td>1009_on</td>\n",
       "      <td>6544.909225</td>\n",
       "      <td>2390</td>\n",
       "      <td>5.161245</td>\n",
       "      <td>4967.0</td>\n",
       "      <td>2387</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>R</td>\n",
       "      <td>20</td>\n",
       "      <td>B cell</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PT1009_TTTGACTGTCCTGTTC-1</th>\n",
       "      <td>1009_on</td>\n",
       "      <td>6534.094667</td>\n",
       "      <td>2091</td>\n",
       "      <td>3.898612</td>\n",
       "      <td>4870.0</td>\n",
       "      <td>2087</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>R</td>\n",
       "      <td>20</td>\n",
       "      <td>B cell</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46089 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          orig.ident    nCount_RNA  nFeature_RNA  percent.mt  \\\n",
       "PT620_AAACGCTGTGCATCTA-1         620    603.729939           451    3.808450   \n",
       "PT620_AAAGAACAGGTACCTT-1         620   1621.040015           962    8.686491   \n",
       "PT620_AATGGCTGTAAGATAC-1         620   1921.037679          1159    9.822692   \n",
       "PT620_AATTTCCAGGCCCAAA-1         620   3453.070497          1784    9.505623   \n",
       "PT620_ACCACAACAGGACAGT-1         620   2145.965853          1255    9.435712   \n",
       "...                              ...           ...           ...         ...   \n",
       "PT1009_TTGGGATGTTTACCAG-1    1009_on   8558.603451          2433    6.825769   \n",
       "PT1009_TTGTTCACACTGCGAC-1    1009_on  16172.649320          4107    3.297175   \n",
       "PT1009_TTTAGTCAGCGTTACT-1    1009_on   5249.795876          1915    3.016424   \n",
       "PT1009_TTTCATGCAGCTTCGG-1    1009_on   6544.909225          2390    5.161245   \n",
       "PT1009_TTTGACTGTCCTGTTC-1    1009_on   6534.094667          2091    3.898612   \n",
       "\n",
       "                           nCount_SCT  nFeature_SCT  SCT_snn_res.0.4  \\\n",
       "PT620_AAACGCTGTGCATCTA-1       3084.0           775               14   \n",
       "PT620_AAAGAACAGGTACCTT-1       3639.0           991               14   \n",
       "PT620_AATGGCTGTAAGATAC-1       3850.0          1187               14   \n",
       "PT620_AATTTCCAGGCCCAAA-1       3833.0          1784               14   \n",
       "PT620_ACCACAACAGGACAGT-1       3555.0          1269               14   \n",
       "...                               ...           ...              ...   \n",
       "PT1009_TTGGGATGTTTACCAG-1      5008.0          2190                5   \n",
       "PT1009_TTGTTCACACTGCGAC-1      4710.0          2000               10   \n",
       "PT1009_TTTAGTCAGCGTTACT-1      4672.0          1914                5   \n",
       "PT1009_TTTCATGCAGCTTCGG-1      4967.0          2387                5   \n",
       "PT1009_TTTGACTGTCCTGTTC-1      4870.0          2087                5   \n",
       "\n",
       "                           seurat_clusters  cell.type  hc_euclidean_5  \\\n",
       "PT620_AAACGCTGTGCATCTA-1                14          1               1   \n",
       "PT620_AAAGAACAGGTACCTT-1                14          1               1   \n",
       "PT620_AATGGCTGTAAGATAC-1                14          1               1   \n",
       "PT620_AATTTCCAGGCCCAAA-1                14          1               1   \n",
       "PT620_ACCACAACAGGACAGT-1                14          1               1   \n",
       "...                                    ...        ...             ...   \n",
       "PT1009_TTGGGATGTTTACCAG-1                5          0               2   \n",
       "PT1009_TTGTTCACACTGCGAC-1               10          0               2   \n",
       "PT1009_TTTAGTCAGCGTTACT-1                5          0               2   \n",
       "PT1009_TTTCATGCAGCTTCGG-1                5          0               2   \n",
       "PT1009_TTTGACTGTCCTGTTC-1                5          0               2   \n",
       "\n",
       "                           hc_euclidean_10  hc_euclidean_15  hc_corelation_5  \\\n",
       "PT620_AAACGCTGTGCATCTA-1                 1                1                1   \n",
       "PT620_AAAGAACAGGTACCTT-1                 1                1                1   \n",
       "PT620_AATGGCTGTAAGATAC-1                 1                1                1   \n",
       "PT620_AATTTCCAGGCCCAAA-1                 1                1                1   \n",
       "PT620_ACCACAACAGGACAGT-1                 1                1                1   \n",
       "...                                    ...              ...              ...   \n",
       "PT1009_TTGGGATGTTTACCAG-1                5                6                3   \n",
       "PT1009_TTGTTCACACTGCGAC-1                5                6                3   \n",
       "PT1009_TTTAGTCAGCGTTACT-1                5                6                3   \n",
       "PT1009_TTTCATGCAGCTTCGG-1                5                6                3   \n",
       "PT1009_TTTGACTGTCCTGTTC-1                5                6                3   \n",
       "\n",
       "                           hc_corelation_10  hc_corelation_15  dbscan  \\\n",
       "PT620_AAACGCTGTGCATCTA-1                  1                 1       1   \n",
       "PT620_AAAGAACAGGTACCTT-1                  1                 1       1   \n",
       "PT620_AATGGCTGTAAGATAC-1                  1                 1       1   \n",
       "PT620_AATTTCCAGGCCCAAA-1                  1                 1       1   \n",
       "PT620_ACCACAACAGGACAGT-1                  1                 1       1   \n",
       "...                                     ...               ...     ...   \n",
       "PT1009_TTGGGATGTTTACCAG-1                 7                 8       6   \n",
       "PT1009_TTGTTCACACTGCGAC-1                 7                 8       6   \n",
       "PT1009_TTTAGTCAGCGTTACT-1                 7                 8       6   \n",
       "PT1009_TTTCATGCAGCTTCGG-1                 7                 8       6   \n",
       "PT1009_TTTGACTGTCCTGTTC-1                 7                 8       6   \n",
       "\n",
       "                          response leiden annotated_celltype  new_dbscan  \n",
       "PT620_AAACGCTGTGCATCTA-1        PD     13          Malignant           1  \n",
       "PT620_AAAGAACAGGTACCTT-1        PD     13          Malignant           1  \n",
       "PT620_AATGGCTGTAAGATAC-1        PD     13          Malignant           1  \n",
       "PT620_AATTTCCAGGCCCAAA-1        PD     13          Malignant           1  \n",
       "PT620_ACCACAACAGGACAGT-1        PD     13          Malignant           1  \n",
       "...                            ...    ...                ...         ...  \n",
       "PT1009_TTGGGATGTTTACCAG-1        R     20             B cell           5  \n",
       "PT1009_TTGTTCACACTGCGAC-1        R     20             B cell           5  \n",
       "PT1009_TTTAGTCAGCGTTACT-1        R     20             B cell           5  \n",
       "PT1009_TTTCATGCAGCTTCGG-1        R     20             B cell           5  \n",
       "PT1009_TTTGACTGTCCTGTTC-1        R     20             B cell           5  \n",
       "\n",
       "[46089 rows x 20 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mit_data.obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and process annotated colon scRNA-seq data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "colon = sc.read_h5ad(\"maarten.h5ad\")\n",
    "\n",
    "COLON_TYPES = {'CD8+ T cell': 101,'CD4+ T cell': 102,'NKs': 103,'B cell': 104,'Mast': 105,\n",
    "               'Monocyte': 106,'Fibroblasts': 107,'Endothelial cells': 108,'Glia': 109}\n",
    "\n",
    "colon.obs['annotated_celltype']=colon.obs['cell_type']\n",
    "colon.obs['new_dbscan']=colon.obs['annotated_celltype'].map(COLON_TYPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nGene</th>\n",
       "      <th>nUMI</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Location</th>\n",
       "      <th>Sample</th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Health</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>nCount_RNA</th>\n",
       "      <th>nFeature_RNA</th>\n",
       "      <th>annotated_celltype</th>\n",
       "      <th>new_dbscan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N10.EpiA.AAACGCACACAGCT</th>\n",
       "      <td>474</td>\n",
       "      <td>1144.0</td>\n",
       "      <td>N10</td>\n",
       "      <td>Epi</td>\n",
       "      <td>N10.EpiA</td>\n",
       "      <td>CD8+ IELs</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>CD8+ T cell</td>\n",
       "      <td>1144.0</td>\n",
       "      <td>474</td>\n",
       "      <td>CD8+ T cell</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N10.EpiA.AAACGCTGGTTCGA</th>\n",
       "      <td>697</td>\n",
       "      <td>1615.0</td>\n",
       "      <td>N10</td>\n",
       "      <td>Epi</td>\n",
       "      <td>N10.EpiA</td>\n",
       "      <td>CD8+ IELs</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>CD8+ T cell</td>\n",
       "      <td>1615.0</td>\n",
       "      <td>697</td>\n",
       "      <td>CD8+ T cell</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N10.EpiA.AAAGTTTGCCTAAG</th>\n",
       "      <td>652</td>\n",
       "      <td>1488.0</td>\n",
       "      <td>N10</td>\n",
       "      <td>Epi</td>\n",
       "      <td>N10.EpiA</td>\n",
       "      <td>CD8+ IELs</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>CD8+ T cell</td>\n",
       "      <td>1488.0</td>\n",
       "      <td>652</td>\n",
       "      <td>CD8+ T cell</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N10.EpiA.AAATCTGAGGTCTA</th>\n",
       "      <td>408</td>\n",
       "      <td>781.0</td>\n",
       "      <td>N10</td>\n",
       "      <td>Epi</td>\n",
       "      <td>N10.EpiA</td>\n",
       "      <td>CD8+ LP</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>CD8+ T cell</td>\n",
       "      <td>781.0</td>\n",
       "      <td>408</td>\n",
       "      <td>CD8+ T cell</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N10.EpiA.AACCTACTAGTTCG</th>\n",
       "      <td>568</td>\n",
       "      <td>1208.0</td>\n",
       "      <td>N10</td>\n",
       "      <td>Epi</td>\n",
       "      <td>N10.EpiA</td>\n",
       "      <td>CD8+ IELs</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>CD8+ T cell</td>\n",
       "      <td>1208.0</td>\n",
       "      <td>568</td>\n",
       "      <td>CD8+ T cell</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N9.LPB.TGCTTAACGTTCGA</th>\n",
       "      <td>2495</td>\n",
       "      <td>8440.0</td>\n",
       "      <td>N9</td>\n",
       "      <td>LP</td>\n",
       "      <td>N9.LPB</td>\n",
       "      <td>Post-capillary Venules</td>\n",
       "      <td>Inflamed</td>\n",
       "      <td>Endothelial cells</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2147483648</td>\n",
       "      <td>Endothelial cells</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N9.LPB.TGGACTGACCTGAA</th>\n",
       "      <td>1319</td>\n",
       "      <td>3413.0</td>\n",
       "      <td>N9</td>\n",
       "      <td>LP</td>\n",
       "      <td>N9.LPB</td>\n",
       "      <td>Post-capillary Venules</td>\n",
       "      <td>Inflamed</td>\n",
       "      <td>Endothelial cells</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2147483648</td>\n",
       "      <td>Endothelial cells</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N9.LPB.TGGGTATGTCCTTA</th>\n",
       "      <td>1520</td>\n",
       "      <td>3975.0</td>\n",
       "      <td>N9</td>\n",
       "      <td>LP</td>\n",
       "      <td>N9.LPB</td>\n",
       "      <td>WNT2B+ Fos-lo 1</td>\n",
       "      <td>Inflamed</td>\n",
       "      <td>Fibroblasts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2147483648</td>\n",
       "      <td>Fibroblasts</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N9.LPB.TGTACTTGCCGAAT</th>\n",
       "      <td>754</td>\n",
       "      <td>1603.0</td>\n",
       "      <td>N9</td>\n",
       "      <td>LP</td>\n",
       "      <td>N9.LPB</td>\n",
       "      <td>WNT2B+ Fos-lo 2</td>\n",
       "      <td>Inflamed</td>\n",
       "      <td>Fibroblasts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2147483648</td>\n",
       "      <td>Fibroblasts</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N9.LPB.TTGCATTGGTCACA</th>\n",
       "      <td>546</td>\n",
       "      <td>1212.0</td>\n",
       "      <td>N9</td>\n",
       "      <td>LP</td>\n",
       "      <td>N9.LPB</td>\n",
       "      <td>Post-capillary Venules</td>\n",
       "      <td>Inflamed</td>\n",
       "      <td>Endothelial cells</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2147483648</td>\n",
       "      <td>Endothelial cells</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65540 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         nGene    nUMI Subject Location    Sample  \\\n",
       "N10.EpiA.AAACGCACACAGCT    474  1144.0     N10      Epi  N10.EpiA   \n",
       "N10.EpiA.AAACGCTGGTTCGA    697  1615.0     N10      Epi  N10.EpiA   \n",
       "N10.EpiA.AAAGTTTGCCTAAG    652  1488.0     N10      Epi  N10.EpiA   \n",
       "N10.EpiA.AAATCTGAGGTCTA    408   781.0     N10      Epi  N10.EpiA   \n",
       "N10.EpiA.AACCTACTAGTTCG    568  1208.0     N10      Epi  N10.EpiA   \n",
       "...                        ...     ...     ...      ...       ...   \n",
       "N9.LPB.TGCTTAACGTTCGA     2495  8440.0      N9       LP    N9.LPB   \n",
       "N9.LPB.TGGACTGACCTGAA     1319  3413.0      N9       LP    N9.LPB   \n",
       "N9.LPB.TGGGTATGTCCTTA     1520  3975.0      N9       LP    N9.LPB   \n",
       "N9.LPB.TGTACTTGCCGAAT      754  1603.0      N9       LP    N9.LPB   \n",
       "N9.LPB.TTGCATTGGTCACA      546  1212.0      N9       LP    N9.LPB   \n",
       "\n",
       "                                        Cluster    Health          cell_type  \\\n",
       "N10.EpiA.AAACGCACACAGCT               CD8+ IELs   Healthy        CD8+ T cell   \n",
       "N10.EpiA.AAACGCTGGTTCGA               CD8+ IELs   Healthy        CD8+ T cell   \n",
       "N10.EpiA.AAAGTTTGCCTAAG               CD8+ IELs   Healthy        CD8+ T cell   \n",
       "N10.EpiA.AAATCTGAGGTCTA                 CD8+ LP   Healthy        CD8+ T cell   \n",
       "N10.EpiA.AACCTACTAGTTCG               CD8+ IELs   Healthy        CD8+ T cell   \n",
       "...                                         ...       ...                ...   \n",
       "N9.LPB.TGCTTAACGTTCGA    Post-capillary Venules  Inflamed  Endothelial cells   \n",
       "N9.LPB.TGGACTGACCTGAA    Post-capillary Venules  Inflamed  Endothelial cells   \n",
       "N9.LPB.TGGGTATGTCCTTA           WNT2B+ Fos-lo 1  Inflamed        Fibroblasts   \n",
       "N9.LPB.TGTACTTGCCGAAT           WNT2B+ Fos-lo 2  Inflamed        Fibroblasts   \n",
       "N9.LPB.TTGCATTGGTCACA    Post-capillary Venules  Inflamed  Endothelial cells   \n",
       "\n",
       "                         nCount_RNA  nFeature_RNA annotated_celltype  \\\n",
       "N10.EpiA.AAACGCACACAGCT      1144.0           474        CD8+ T cell   \n",
       "N10.EpiA.AAACGCTGGTTCGA      1615.0           697        CD8+ T cell   \n",
       "N10.EpiA.AAAGTTTGCCTAAG      1488.0           652        CD8+ T cell   \n",
       "N10.EpiA.AAATCTGAGGTCTA       781.0           408        CD8+ T cell   \n",
       "N10.EpiA.AACCTACTAGTTCG      1208.0           568        CD8+ T cell   \n",
       "...                             ...           ...                ...   \n",
       "N9.LPB.TGCTTAACGTTCGA           NaN   -2147483648  Endothelial cells   \n",
       "N9.LPB.TGGACTGACCTGAA           NaN   -2147483648  Endothelial cells   \n",
       "N9.LPB.TGGGTATGTCCTTA           NaN   -2147483648        Fibroblasts   \n",
       "N9.LPB.TGTACTTGCCGAAT           NaN   -2147483648        Fibroblasts   \n",
       "N9.LPB.TTGCATTGGTCACA           NaN   -2147483648  Endothelial cells   \n",
       "\n",
       "                         new_dbscan  \n",
       "N10.EpiA.AAACGCACACAGCT         101  \n",
       "N10.EpiA.AAACGCTGGTTCGA         101  \n",
       "N10.EpiA.AAAGTTTGCCTAAG         101  \n",
       "N10.EpiA.AAATCTGAGGTCTA         101  \n",
       "N10.EpiA.AACCTACTAGTTCG         101  \n",
       "...                             ...  \n",
       "N9.LPB.TGCTTAACGTTCGA           108  \n",
       "N9.LPB.TGGACTGACCTGAA           108  \n",
       "N9.LPB.TGGGTATGTCCTTA           107  \n",
       "N9.LPB.TGTACTTGCCGAAT           107  \n",
       "N9.LPB.TTGCATTGGTCACA           108  \n",
       "\n",
       "[65540 rows x 12 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colon.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy, x_ind, y_ind = np.intersect1d(scatac.var_names,  mit_data.var_names, return_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_intersection = np.intersect1d(xy, colon.var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices for all three datasets\n",
    "_, _, mit_ind = np.intersect1d(gene_intersection,  mit_data.var_names, return_indices=True)\n",
    "_, _, atac_ind = np.intersect1d(gene_intersection,  scatac.var_names, return_indices=True)\n",
    "_, _, colon_ind = np.intersect1d(gene_intersection,  colon.var_names, return_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_annotated = np.array(colon.obs['cell_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the intersection of ATAC-seq and RNA-seq melanoma genes, as well as colon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_y = mit_data.obs[['new_dbscan','response','annotated_celltype']]\n",
    "rna_y.columns=['dbscan','response','annotated_celltype']\n",
    "rna_y = rna_y.copy()\n",
    "rna_y['omic'] = 'RNA'\n",
    "\n",
    "colon_y = colon.obs[['new_dbscan','annotated_celltype']]\n",
    "colon_y.columns=['dbscan','annotated_celltype']\n",
    "colon_y = colon_y.copy()\n",
    "colon_y['omic'] = 'RNA'\n",
    "\n",
    "atac_y = scatac.obs[['dbscan','response','annotated_celltype']]\n",
    "atac_y = atac_y.copy()\n",
    "atac_y['omic'] = 'ATAC'\n",
    "\n",
    "common_y = rna_y.append(atac_y)\n",
    "common_y = common_y.copy()\n",
    "common_y['annotation'] = common_y['annotated_celltype']+\"_\"+common_y['omic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_rna = mit_data.X[:,mit_ind]\n",
    "raw_colon = colon.X[:,colon_ind].toarray()\n",
    "raw_atac = scatac.X[:,atac_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_matrix = np.concatenate(np.concatenate((raw_rna, raw_atac), axis=0), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((46089, 261), (65540, 261), (43956, 261))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_rna.shape, raw_colon.shape, raw_atac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Dataset: Found 46089 items \n",
      "== Dataset: Found 9 classes\n",
      "== Dataset: Found 65540 items \n",
      "== Dataset: Found 9 classes\n",
      "== Dataset: Found 43956 items \n",
      "== Dataset: Found 9 classes\n"
     ]
    }
   ],
   "source": [
    "annotated_scrna = ExperimentDataset(raw_rna, \n",
    "                              mit_data.obs.index, \n",
    "                              gene_intersection, \n",
    "                              'Annotated', rna_y['dbscan'])\n",
    "\n",
    "annotated_colon = ExperimentDataset(raw_colon, \n",
    "                              colon.obs.index, \n",
    "                              gene_intersection, \n",
    "                              'Annotated', colon_y['dbscan'])\n",
    "\n",
    "unannotated = ExperimentDataset(raw_atac, \n",
    "                                scatac.obs.index, \n",
    "                                gene_intersection, \n",
    "                                'Unannotated', atac_y['dbscan'])\n",
    "\n",
    "pretrain = ExperimentDataset(raw_atac, \n",
    "                             scatac.obs.index, \n",
    "                             gene_intersection,\n",
    "                             'Unannotated')\n",
    "\n",
    "n_clusters= len(np.unique(unannotated.y))\n",
    "\n",
    "params, unknown= get_parser().parse_known_args()\n",
    "if torch.cuda.is_available() and not params.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "device = 'cuda:0' if torch.cuda.is_available() and params.cuda else 'cpu'\n",
    "\n",
    "params.device = device\n",
    "params.epochs_pretrain = 30\n",
    "params.pretrain_batch = 128\n",
    "params.epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_merge_mars = AE_MARS(n_clusters, params, [annotated_colon], unannotated, pretrain, \n",
    "                ##These are roll over parametesr form original model \n",
    "                hid_dim_1=1000, hid_dim_2=100, p_drop=0.0, tau=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining..\n",
      "Pretraining Epoch 0, Loss: 204.66497802734375\n",
      "Time:  4.610968828201294\n",
      "Pretraining Epoch 1, Loss: 159.64651489257812\n",
      "Time:  4.978879928588867\n",
      "Pretraining Epoch 2, Loss: 137.91664123535156\n",
      "Time:  5.898734092712402\n",
      "Pretraining Epoch 3, Loss: 123.95004272460938\n",
      "Time:  5.316322088241577\n",
      "Pretraining Epoch 4, Loss: 116.322509765625\n",
      "Time:  4.807079076766968\n",
      "Pretraining Epoch 5, Loss: 111.19530487060547\n",
      "Time:  5.216607093811035\n",
      "Pretraining Epoch 6, Loss: 107.22210693359375\n",
      "Time:  4.863080024719238\n",
      "Pretraining Epoch 7, Loss: 104.0864486694336\n",
      "Time:  4.734585285186768\n",
      "Pretraining Epoch 8, Loss: 101.49949645996094\n",
      "Time:  4.8365318775177\n",
      "Pretraining Epoch 9, Loss: 99.1728286743164\n",
      "Time:  4.553593158721924\n",
      "Pretraining Epoch 10, Loss: 97.08411407470703\n",
      "Time:  4.810671329498291\n",
      "Pretraining Epoch 11, Loss: 95.23031616210938\n",
      "Time:  5.737428903579712\n",
      "Pretraining Epoch 12, Loss: 93.51380920410156\n",
      "Time:  4.733135223388672\n",
      "Pretraining Epoch 13, Loss: 91.75343322753906\n",
      "Time:  4.606585741043091\n",
      "Pretraining Epoch 14, Loss: 90.00100708007812\n",
      "Time:  5.311151742935181\n",
      "Pretraining Epoch 15, Loss: 88.55315399169922\n",
      "Time:  4.67720890045166\n",
      "Pretraining Epoch 16, Loss: 87.18964385986328\n",
      "Time:  4.465389013290405\n",
      "Pretraining Epoch 17, Loss: 85.89237976074219\n",
      "Time:  4.551522970199585\n",
      "Pretraining Epoch 18, Loss: 84.64143371582031\n",
      "Time:  4.597562789916992\n",
      "Pretraining Epoch 19, Loss: 83.44229888916016\n",
      "Time:  4.492374658584595\n",
      "Pretraining Epoch 20, Loss: 82.2442626953125\n",
      "Time:  4.444756031036377\n",
      "Pretraining Epoch 21, Loss: 81.11238861083984\n",
      "Time:  4.576909780502319\n",
      "Pretraining Epoch 22, Loss: 79.94463348388672\n",
      "Time:  4.425085067749023\n",
      "Pretraining Epoch 23, Loss: 78.99385070800781\n",
      "Time:  4.8109331130981445\n",
      "Pretraining Epoch 24, Loss: 78.13915252685547\n",
      "Time:  4.50744104385376\n",
      "Pretraining Epoch 25, Loss: 77.32579803466797\n",
      "Time:  4.456125736236572\n",
      "Pretraining Epoch 26, Loss: 76.53959655761719\n",
      "Time:  4.438321113586426\n",
      "Pretraining Epoch 27, Loss: 75.82821655273438\n",
      "Time:  4.627766132354736\n",
      "Pretraining Epoch 28, Loss: 75.0829086303711\n",
      "Time:  4.4294350147247314\n",
      "Pretraining Epoch 29, Loss: 74.46673583984375\n",
      "Time:  4.89640998840332\n",
      "Pretraining done\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FullNet' object has no attribute 'latent_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-f85eb5b4c8fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mae_merge_adata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mae_merge_landmarks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mae_merge_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mae_merge_training_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mae_merge_latent_tracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mae_merge_mars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_all_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-2a61a5814f9f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, evaluation_mode, save_all_embeddings)\u001b[0m\n\u001b[1;32m    508\u001b[0m                                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m                                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m                                                 self.model, self.device)\n\u001b[0m\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         optim, optim_landmk_test = self.init_optim(list(self.model.encoder.parameters()), \n",
      "\u001b[0;32m<ipython-input-31-629d07ca64c2>\u001b[0m in \u001b[0;36minit_landmarks\u001b[0;34m(n_clusters, tr_load, test_load, model, device, mode, pretrain)\u001b[0m\n\u001b[1;32m    261\u001b[0m     lndmk_tr = [torch.zeros(size=(len(np.unique(dl.dataset.y)), \n\u001b[1;32m    262\u001b[0m                                   model.latent_dim), \n\u001b[0;32m--> 263\u001b[0;31m                             requires_grad=True, device=device) for dl in tr_load]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     lndmk_test = [torch.zeros(size=(1, model.latent_dim), \n",
      "\u001b[0;32m<ipython-input-31-629d07ca64c2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m     lndmk_tr = [torch.zeros(size=(len(np.unique(dl.dataset.y)), \n\u001b[1;32m    262\u001b[0m                                   model.latent_dim), \n\u001b[0;32m--> 263\u001b[0;31m                             requires_grad=True, device=device) for dl in tr_load]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     lndmk_test = [torch.zeros(size=(1, model.latent_dim), \n",
      "\u001b[0;32m~/opt/anaconda3/envs/mars/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    583\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 585\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FullNet' object has no attribute 'latent_dim'"
     ]
    }
   ],
   "source": [
    "ae_merge_adata, ae_merge_landmarks, ae_merge_scores, ae_merge_training_history, ae_merge_latent_tracker = ae_merge_mars.train(evaluation_mode=True, save_all_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Unannoatted landmark loss is the same as test latent loss(distance between embedding and landmarks)\n",
    "MARS_loss_tracker(ae_merge_training_history, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_history(ae_merge_training_history, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_latent_pca(ae_merge_latent_tracker , 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_latent_umap(ae_merge_adata, plot_gene_list = ['MARS_labels','experiment'], save =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell_type_assign(adata, save):\n",
    "\n",
    "    num2name={1:'Malignant',2:'Endothelial',3:'T cell', 4:'Macrophage',\n",
    "              5:'B cell',6:'CAF',7:'Dendritic',8:'Plasma B', 9:'NK'}\n",
    "\n",
    "    adata.obs['label_name']=adata.obs['truth_labels'].map(num2name)  \n",
    "    untable = adata.obs.loc[adata.obs['experiment']=='Unannotated',:]\n",
    "    \n",
    "    groundtruth_sum = pd.crosstab(untable['truth_labels'], untable['MARS_labels']).sum(axis=1)\n",
    "    plot = pd.crosstab(untable['truth_labels'], untable['MARS_labels']).div(groundtruth_sum/100, axis=0).plot.bar(figsize=(17,5), width=1)\n",
    "    plot.set_xticklabels(['Malignant','Endothelial', 'T cell', 'Macrophage', 'B cell',  'CAF', 'Dendritic', 'Plasma B', 'NK' ])\n",
    "    plot.legend(title=\"MARS labels\")\n",
    "    plot.set_xlabel(\"Ground truth labels\", fontsize=18)\n",
    "    plot.set_title(\"Distribution of MARS labels and Ground Truths\", fontsize=24)\n",
    "    plot.set_ylabel(\"% Truth labels in MARS labels\", fontsize=18)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    if save==True:\n",
    "        plot.get_figure().savefig(\"cell_type_groundtruth.png\")\n",
    "    \n",
    "    groundtruth_sum = pd.crosstab(untable['MARS_labels'], untable['label_name']).sum(axis=1)\n",
    "    plot = pd.crosstab(untable['MARS_labels'], untable['label_name']).div(groundtruth_sum/100, axis=0).plot.bar(figsize=(17,5), width=1)\n",
    "    plot.legend(title=\"Ground truth\")\n",
    "    plot.set_xlabel(\"MARS labels\", fontsize=18)\n",
    "    plot.set_ylabel(\"% MARS labels in Truth labels\", fontsize=18)\n",
    "    plt.show()\n",
    "    \n",
    "    if save==True:\n",
    "        plot.get_figure().savefig(\"cell_type_MARS.png\")\n",
    "\n",
    "        \n",
    "def MARS_silhouette(adata, save):\n",
    "    \n",
    "    \n",
    "    anno_data = adata[adata.obs['experiment'] == \"Annotated\", :]\n",
    "    unanno_data = adata[adata.obs['experiment'] == \"Unannotated\", :]\n",
    "    \n",
    "    anno_obs = anno_data.obs\n",
    "    unanno_obs = unanno_data.obs\n",
    "    \n",
    "    ###Silhouette\n",
    "    train_latent = anno_data.obsm['MARS_embedding']\n",
    "    train_sil=silhouette_samples(train_latent, anno_obs['truth_labels'].values)\n",
    "    \n",
    "    val_latent = unanno_data.obsm['MARS_embedding']\n",
    "    val_sil=silhouette_samples(val_latent, unanno_obs['truth_labels'].values)\n",
    "    \n",
    "    n_clusters=len(Counter(adata.obs['truth_labels'].values).keys())\n",
    "    \n",
    "    ###Plot silhouette score for train and test \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 10)\n",
    "        \n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_ylim([0, len(train_latent) + (n_clusters + 1) * 10])\n",
    "    \n",
    "    ax2.set_xlim([-0.1, 1])\n",
    "    ax2.set_ylim([0, len(val_latent) + (n_clusters + 1) * 10])\n",
    "         \n",
    "    y_lower = 10\n",
    "    val_y_lower = 10\n",
    "    \n",
    "    \n",
    "    name2num={'Malignant':1, 'Endothelial':2, 'T cell':3, \n",
    "          'Macrophage':4, 'B cell':5,'CAF':6, \n",
    "          'Dendritic':7 ,'Plasma B':8, 'NK':9}\n",
    "\n",
    "    cluster_list = list(name2num.keys())\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        \n",
    "        \n",
    "        #cluster_label = list(Counter(adata.obs['truth_labels']).keys())[i]\n",
    "        cluster_label = cluster_list[i]\n",
    "        \n",
    "        #############################################################\n",
    "        ###############Training \n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        train_ith_cluster_silhouette_values = train_sil[anno_obs['label_name'] == cluster_label]\n",
    "        train_ith_cluster_silhouette_values.sort()\n",
    "        \n",
    "        size_cluster_i = train_ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        #color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, train_ith_cluster_silhouette_values) \n",
    "        #edgecolor=color, facecolor=color\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(0.8, y_lower + 0.5 * size_cluster_i, str(cluster_label))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "        \n",
    "        #############################################################\n",
    "        ###############Validation \n",
    "        val_ith_cluster_silhouette_values = val_sil[unanno_obs['label_name'] == cluster_label]\n",
    "        val_ith_cluster_silhouette_values.sort()\n",
    "        \n",
    "        val_size_cluster_i = val_ith_cluster_silhouette_values.shape[0]\n",
    "        val_y_upper = val_y_lower + val_size_cluster_i\n",
    "\n",
    "        #color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax2.fill_betweenx(np.arange(val_y_lower, val_y_upper),\n",
    "                          0, val_ith_cluster_silhouette_values)\n",
    "        # edgecolor=color, facecolor=color\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax2.text(0.8, val_y_lower + 0.5 * val_size_cluster_i, str(cluster_label))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        val_y_lower = val_y_upper + 10  # 10 for the 0 samples\n",
    "     \n",
    "    ax1.set_title(\"Training silhouette plot for known labels\")\n",
    "    ax1.set_xlabel(\"Silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "    \n",
    "    ax2.set_title(\"Validation silhouette plot for known labels\")\n",
    "    ax2.set_xlabel(\"Silhouette coefficient values\")\n",
    "    ax2.set_ylabel(\"Cluster label\")\n",
    "    \n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    train_avg=silhouette_score(train_latent, anno_obs['truth_labels'].values)\n",
    "    val_avg=silhouette_score(val_latent, unanno_obs['truth_labels'].values)\n",
    "    \n",
    "    ax1.axvline(x=train_avg, color=\"red\", linestyle=\"--\")\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    \n",
    "    ax2.axvline(x=val_avg, color=\"red\", linestyle=\"--\")\n",
    "    ax2.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax2.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    \n",
    "    plt.suptitle(\"Silhouette scores for MARS latent space\", fontsize=14, fontweight='bold')\n",
    "    plt.show()  \n",
    "    \n",
    "    if save ==True:\n",
    "        fig.savefig(\"MARS_silhouette.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_assign(ae_merge_adata, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_silhouette(ae_merge_adata, save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MARS_silhouette(adata):\n",
    "    \n",
    "    \n",
    "    anno_data = adata[adata.obs['experiment'] == \"Annotated\", :]\n",
    "    unanno_data = adata[adata.obs['experiment'] == \"Unannotated\", :]\n",
    "    \n",
    "    anno_obs = anno_data.obs\n",
    "    unanno_obs = unanno_data.obs\n",
    "    \n",
    "    ###Silhouette\n",
    "    train_latent = anno_data.obsm['MARS_embedding']\n",
    "    train_sil=silhouette_samples(train_latent, anno_obs['truth_labels'].values)\n",
    "    \n",
    "    val_latent = unanno_data.obsm['MARS_embedding']\n",
    "    val_sil=silhouette_samples(val_latent, unanno_obs['truth_labels'].values)\n",
    "    \n",
    "    n_clusters=len(Counter(adata.obs['truth_labels'].values).keys())\n",
    "    \n",
    "    ###Plot silhouette score for train and test \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 10)\n",
    "        \n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_ylim([0, len(train_latent) + (n_clusters + 1) * 10])\n",
    "    \n",
    "    ax2.set_xlim([-0.1, 1])\n",
    "    ax2.set_ylim([0, len(val_latent) + (n_clusters + 1) * 10])\n",
    "         \n",
    "    y_lower = 10\n",
    "    val_y_lower = 10\n",
    "    \n",
    "    \n",
    "    name2num={'Malignant':1, 'Endothelial':2, 'T cell':3, \n",
    "          'Macrophage':4, 'B cell':5,'CAF':6, \n",
    "          'Dendritic':7 ,'Plasma B':8, 'NK':9}\n",
    "\n",
    "    cluster_list = list(name2num.keys())\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        \n",
    "        \n",
    "        #cluster_label = list(Counter(adata.obs['truth_labels']).keys())[i]\n",
    "        cluster_label = cluster_list[i]\n",
    "        \n",
    "        #############################################################\n",
    "        ###############Training \n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        train_ith_cluster_silhouette_values = train_sil[anno_obs['label_name'] == cluster_label]\n",
    "        train_ith_cluster_silhouette_values.sort()\n",
    "        \n",
    "        size_cluster_i = train_ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        #color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, train_ith_cluster_silhouette_values) \n",
    "        #edgecolor=color, facecolor=color\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(cluster_label))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "        \n",
    "        #############################################################\n",
    "        ###############Validation \n",
    "        val_ith_cluster_silhouette_values = val_sil[unanno_obs['label_name'] == cluster_label]\n",
    "        val_ith_cluster_silhouette_values.sort()\n",
    "        \n",
    "        val_size_cluster_i = val_ith_cluster_silhouette_values.shape[0]\n",
    "        val_y_upper = val_y_lower + val_size_cluster_i\n",
    "\n",
    "        #color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax2.fill_betweenx(np.arange(val_y_lower, val_y_upper),\n",
    "                          0, val_ith_cluster_silhouette_values)\n",
    "        # edgecolor=color, facecolor=color\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax2.text(-0.05, val_y_lower + 0.5 * val_size_cluster_i, str(cluster_label))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        val_y_lower = val_y_upper + 10  # 10 for the 0 samples\n",
    "     \n",
    "    ax1.set_title(\"Training silhouette plot for known labels\")\n",
    "    ax1.set_xlabel(\"Silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "    \n",
    "    ax2.set_title(\"Validation silhouette plot for known labels\")\n",
    "    ax2.set_xlabel(\"Silhouette coefficient values\")\n",
    "    ax2.set_ylabel(\"Cluster label\")\n",
    "    \n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    train_avg=silhouette_score(train_latent, anno_obs['truth_labels'].values)\n",
    "    val_avg=silhouette_score(val_latent, unanno_obs['truth_labels'].values)\n",
    "    \n",
    "    ax1.axvline(x=train_avg, color=\"red\", linestyle=\"--\")\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    \n",
    "    ax2.axvline(x=val_avg, color=\"red\", linestyle=\"--\")\n",
    "    ax2.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax2.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    \n",
    "    plt.suptitle(\"Silhouette scores for MARS latent space\", fontsize=14, fontweight='bold')\n",
    "    plt.show()           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_silhouette(ae_merge_adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-embedded pre training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain = ExperimentDataset(common_matrix, \n",
    "                             [mit_data.obs.index, scatac.obs.index], \n",
    "                             xy,\n",
    "                             'Unannotated')\n",
    "\n",
    "n_clusters= len(np.unique(unannotated.y))\n",
    "\n",
    "params, unknown= get_parser().parse_known_args()\n",
    "if torch.cuda.is_available() and not params.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "device = 'cuda:0' if torch.cuda.is_available() and params.cuda else 'cpu'\n",
    "\n",
    "params.device = device\n",
    "params.epochs_pretrain = 30\n",
    "params.pretrain_batch = 128\n",
    "params.epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_merge_mars = AE_MARS(n_clusters, params, [annotated_scrna], unannotated, pretrain, \n",
    "                ##These are roll over parametesr form original model \n",
    "                hid_dim_1=1000, hid_dim_2=100, p_drop=0.0, tau=0.2)\n",
    "\n",
    "ae_merge_adata, ae_merge_landmarks, ae_merge_scores , ae_merge_training_history, ae_merge_latent_tracker = ae_merge_mars.train(evaluation_mode=True,\n",
    "                                                                                                                               save_all_embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MARS on scRNA data alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_percent = 0.3\n",
    "x_train, test_x, y_train, test_y  = train_test_split(mit_data.X, \n",
    "                                                     mit_data.obs,\n",
    "                                                     test_size=validation_percent,\n",
    "                                                     stratify = mit_data.obs['dbscan'])\n",
    "\n",
    "annotated = ExperimentDataset(x_train, \n",
    "                              y_train.index, \n",
    "                              mit_data.var_names, \n",
    "                              'Annotated', y_train['dbscan'])\n",
    "unannotated = ExperimentDataset(test_x, \n",
    "                                test_y.index, \n",
    "                                mit_data.var_names, \n",
    "                                'Unannotated', test_y['dbscan'])\n",
    "\n",
    "pretrain = ExperimentDataset(test_x, \n",
    "                             test_y.index,\n",
    "                             mit_data.var_names,\n",
    "                             'Unannotated')\n",
    "\n",
    "n_clusters= len(np.unique(unannotated.y))\n",
    "\n",
    "params, unknown= get_parser().parse_known_args()\n",
    "if torch.cuda.is_available() and not params.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "device = 'cuda:0' if torch.cuda.is_available() and params.cuda else 'cpu'\n",
    "\n",
    "params.device = device\n",
    "params.epochs_pretrain = 30\n",
    "params.pretrain_batch = 128\n",
    "params.epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_mars = AE_MARS(n_clusters, params, [annotated] , unannotated, pretrain, \n",
    "                ##These are roll over parametesr form original model \n",
    "                hid_dim_1=1000, hid_dim_2=100, p_drop=0.0, tau=0.2)\n",
    "ae_adata, ae_landmarks, ae_scores , ae_training_history, ae_latent_tracker = ae_mars.train(evaluation_mode=True,\n",
    "                                                                                           save_all_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(self, latent_dim , n_feature , network_architecture , \n",
    "                 lambda_reconstruct, lambda_kl ,\n",
    "                 p_drop=0.2):\n",
    "        \n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.lambda_reconstruct = lambda_reconstruct\n",
    "        self.lambda_kl = lambda_kl\n",
    "        \n",
    "        self.hidden_encode1=network_architecture['n_hidden_recog_1']\n",
    "        self.hidden_encode2=network_architecture['n_hidden_recog_2']\n",
    "        self.hidden_encode3=network_architecture['n_hidden_recog_3']\n",
    "        \n",
    "        self.hidden_decode1=network_architecture['n_hidden_gener_1']\n",
    "        self.hidden_decode2=network_architecture['n_hidden_gener_2']\n",
    "        self.hidden_decode3=network_architecture['n_hidden_gener_3']\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_feature = n_feature\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(self.n_feature, self.hidden_encode1, bias=True),\n",
    "            nn.ELU(alpha=0.2),\n",
    "            nn.Dropout(p=p_drop),\n",
    "            nn.Linear(self.hidden_encode1, self.hidden_encode2, bias=True),\n",
    "            nn.ELU(alpha=0.2),\n",
    "            nn.Linear(self.hidden_encode2, self.hidden_encode3, bias=True),\n",
    "            nn.ELU(alpha=0.2)\n",
    "        )\n",
    "        \n",
    "        self.mu = nn.Linear(self.hidden_encode3, self.latent_dim, bias=True)\n",
    "        self.logvar = nn.Linear(self.hidden_encode3, self.latent_dim, bias=True)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(self.latent_dim, self.hidden_decode1, bias=True),\n",
    "            nn.ELU(alpha=0.2),\n",
    "            nn.Dropout(p=p_drop),\n",
    "            nn.Linear(self.hidden_decode1, self.hidden_decode2, bias=True),\n",
    "            nn.ELU(alpha=0.2),\n",
    "            nn.Linear(self.hidden_decode2, self.hidden_decode3, bias=True),\n",
    "            nn.ELU(alpha=0.2),\n",
    "            nn.Linear(self.hidden_decode3, self.n_feature, bias=True)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def encode(self, x):\n",
    "        '''\n",
    "        Return latent parameters\n",
    "        '''\n",
    "        encoded = self.encoder(x)\n",
    "        \n",
    "        mu = self.mu(encoded)\n",
    "        logvar = self.logvar(encoded)\n",
    "        \n",
    "        return [mu, logvar]\n",
    "        \n",
    "    \n",
    "    def decode(self, z):\n",
    "        '''\n",
    "        Reconstruct\n",
    "        '''\n",
    "        decoded = self.decoder(z)\n",
    "        return decoded\n",
    "    \n",
    "    \n",
    "    def reparameterize(self, mu , logvar):\n",
    "        ''' \n",
    "        Reparametraization sample N(mu, var) from N(0,1) noise\n",
    "        '''\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std) #std represents the size of the tensor\n",
    "        \n",
    "        return eps*std + mu                \n",
    "      \n",
    "    def forward(self, x):\n",
    "        \n",
    "        mu, logvar = self.encode(x)\n",
    "        \n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        decoded = self.decode(z)\n",
    "        \n",
    "        return decoded, mu, logvar\n",
    "    \n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#from model.utils import euclidean_dist\n",
    "\n",
    "##training set encoded distance to landmarks \n",
    "def loss_task(encoded, prototypes, target, criterion='dist'):\n",
    "    \"\"\"Calculate loss.\n",
    "    criterion: NNLoss - assign to closest prototype and calculate NNLoss\n",
    "    dist - loss is distance to prototype that example needs to be assigned to\n",
    "                and -distance to prototypes from other class\n",
    "    \"\"\"\n",
    "    \n",
    "    uniq = torch.unique(target, sorted=True)\n",
    "    \n",
    "    ###index of samples for each class of labels\n",
    "    class_idxs = list(map(lambda c: target.eq(c).nonzero(), uniq))\n",
    "    \n",
    "    # prepare targets so they start from 0,1\n",
    "    for idx,v in enumerate(uniq):\n",
    "        target[target==v]=idx\n",
    "    \n",
    "    dists = euclidean_dist(encoded, prototypes)\n",
    "    \n",
    "    if criterion=='NNLoss':\n",
    "       \n",
    "        loss = torch.nn.NLLLoss()\n",
    "        log_p_y = F.log_softmax(-dists, dim=1)\n",
    "        \n",
    "        loss_val = loss(log_p_y, target)\n",
    "        _, y_hat = log_p_y.max(1)\n",
    "        \n",
    "    \n",
    "    elif criterion=='dist':\n",
    "        \n",
    "        loss_val = torch.stack([dists[idx_example, idx_proto].mean(0) for idx_proto,idx_example in enumerate(class_idxs)]).mean()\n",
    "        #loss_val1 = loss_val1/len(embeddings) \n",
    "        y_hat = torch.max(-dists,1)[1]\n",
    "        \n",
    "    acc_val = y_hat.eq(target.squeeze()).float().mean()    \n",
    "        \n",
    "    return loss_val, acc_val\n",
    "\n",
    "def loss_test_nn(encoded, prototypes):\n",
    "    dists = euclidean_dist(encoded, prototypes)\n",
    "    min_dist = torch.min(dists, 1)\n",
    "    \n",
    "    y_hat = min_dist[1]\n",
    "    args_uniq = torch.unique(y_hat, sorted=True)\n",
    "    args_count = torch.stack([(y_hat==x_u).sum() for x_u in args_uniq])\n",
    "    print(args_count)\n",
    "    \n",
    "    loss = torch.nn.NLLLoss()\n",
    "    log_p_y = F.log_softmax(-dists, dim=1)\n",
    "    print(log_p_y.shape)\n",
    "        \n",
    "    loss_val = loss(log_p_y, y_hat)\n",
    "    _, y_hat = log_p_y.max(1)\n",
    "    \n",
    "    return loss_val, args_count\n",
    "\n",
    "###Intra cluster distance\n",
    "def loss_test_basic(encoded, prototypes):\n",
    "    \n",
    "    dists = euclidean_dist(encoded, prototypes)\n",
    "    min_dist = torch.min(dists, 1)\n",
    "    \n",
    "    y_hat = min_dist[1]\n",
    "    args_uniq = torch.unique(y_hat, sorted=True)\n",
    "    args_count = torch.stack([(y_hat==x_u).sum() for x_u in args_uniq])\n",
    "    #print(args_count)\n",
    "    \n",
    "    min_dist = min_dist[0] # get_distances\n",
    "    \n",
    "    #thr = torch.stack([torch.sort(min_dist[y_hat==idx_class])[0][int(len(min_dist[y_hat==idx_class])*0.9)] for idx_class in args_uniq])\n",
    "    #loss_val = torch.stack([min_dist[y_hat==idx_class][min_dist[y_hat==idx_class]>=thr[idx_class]].mean(0) for idx_class in args_uniq]).mean()\n",
    "    \n",
    "    loss_val = torch.stack([min_dist[y_hat==idx_class].mean(0) for idx_class in args_uniq]).mean()\n",
    "    \n",
    "    #loss_val,_ = loss_task(encoded, prototypes, y_hat, criterion='dist') # same\n",
    "    \n",
    "    return loss_val, args_count\n",
    "\n",
    "##For unannotated set \n",
    "def loss_test(encoded, prototypes, tau):\n",
    "    \n",
    "    #prototypes = torch.stack(prototypes).squeeze() \n",
    "    loss_val_test, args_count = loss_test_basic(encoded, prototypes)\n",
    "    \n",
    "    ###inter cluster distance \n",
    "    if tau>0:\n",
    "        dists = euclidean_dist(prototypes, prototypes)\n",
    "        nproto = prototypes.shape[0]\n",
    "        loss_val2 = - torch.sum(dists)/(nproto*nproto-nproto)\n",
    "        loss_val_test += tau*loss_val2\n",
    "        \n",
    "    return loss_val_test, args_count\n",
    "\n",
    "def reconstruction_loss(decoded, x):\n",
    "    \n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    loss_rcn = loss_func(decoded, x)\n",
    "    #print('Reconstruction {}'.format(loss_rcn))\n",
    "    \n",
    "    return loss_rcn\n",
    "\n",
    "\n",
    "def vae_loss(model, decoded, x, mu, logvar):\n",
    "    \n",
    "#     loss_func = torch.nn.MSELoss()\n",
    "#     loss_rcn = loss_func(decoded, x)\n",
    "    #KL =  torch.mean(-0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()))\n",
    "    \n",
    "    loss_rcn = torch.mean(torch.sum((x - decoded).pow(2),1))\n",
    "    \n",
    "    KL_loss = (-0.5)*(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    KL = torch.mean(torch.sum(KL_loss, axis=1))\n",
    "    \n",
    "    #print(model.lambda_reconstruct, model.lambda_kl)\n",
    "    total_loss = model.lambda_reconstruct*loss_rcn + model.lambda_kl * KL \n",
    "    \n",
    "    \n",
    "    #print(f\"Reconstruction Loss: {loss_rcn} KL Loss: {KL}\")\n",
    "    \n",
    "    return total_loss \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "#from sklearn.cluster import k_means_\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "## Training set (annotated landmarks)\n",
    "def compute_landmarks_tr(embeddings, target, prev_landmarks=None, tau=0.2):\n",
    "    \n",
    "    \"\"\"Computing landmarks of each class in the labeled meta-dataset. \n",
    "    \n",
    "    Landmark is a closed form solution of \n",
    "    minimizing distance to the mean and maximizing distance to other landmarks. \n",
    "    \n",
    "    If tau=0, landmarks are just mean of data points.\n",
    "    \n",
    "    embeddings: embeddings of the labeled dataset\n",
    "    target: labels in the labeled dataset\n",
    "    prev_landmarks: landmarks from previous iteration\n",
    "    tau: regularizer for inter- and intra-cluster distance\n",
    "    \"\"\"\n",
    "    \n",
    "    uniq = torch.unique(target, sorted=True)\n",
    "    class_idxs = list(map(lambda c: target.eq(c).nonzero(), uniq))\n",
    "    \n",
    "    landmarks_mean = torch.stack([embeddings[idx_class].mean(0) for idx_class in class_idxs]).squeeze()\n",
    "    \n",
    "    if prev_landmarks is None or tau==0:\n",
    "        return landmarks_mean\n",
    "    \n",
    "    suma = prev_landmarks.sum(0)\n",
    "    nlndmk = prev_landmarks.shape[0]\n",
    "    lndmk_dist_part = (tau/(nlndmk-1))*torch.stack([suma-p for p in prev_landmarks])\n",
    "    landmarks = 1/(1-tau)*(landmarks_mean-lndmk_dist_part)\n",
    "    \n",
    "    return landmarks\n",
    "\n",
    "\n",
    "\n",
    "def init_landmarks(n_clusters, tr_load, test_load, model, device, mode='kmeans', pretrain=True):\n",
    "    \"\"\"Initialization of landmarks of the labeled and unlabeled meta-dataset.\n",
    "    nclusters: number of expected clusters in the unlabeled meta-dataset\n",
    "    tr_load: data loader for labeled meta-dataset\n",
    "    test_load: data loader for unlabeled meta-dataset\n",
    "    \"\"\"\n",
    "    lndmk_tr = [torch.zeros(size=(len(np.unique(dl.dataset.y)), \n",
    "                                  model.latent_dim), \n",
    "                            requires_grad=True, device=device) for dl in tr_load]\n",
    "    \n",
    "    lndmk_test = [torch.zeros(size=(1, model.latent_dim), \n",
    "                              requires_grad=True, device=device) \n",
    "                       for _ in range(n_clusters)]\n",
    "    \n",
    "    kmeans_init_tr = [init_step(dl.dataset, model, device, pretrained=pretrain, mode=mode) \n",
    "                      for dl in tr_load]\n",
    "    \n",
    "    kmeans_init_test = init_step(test_load.dataset, model, device, \n",
    "                                 pretrained=pretrain, mode=mode, \n",
    "                                 n_clusters=n_clusters)\n",
    "    \n",
    "    ##No gradient calculation\n",
    "    with torch.no_grad():\n",
    "        [lndmk.copy_(kmeans_init_tr[idx])  for idx,lndmk in enumerate(lndmk_tr)]\n",
    "        [lndmk_test[i].copy_(kmeans_init_test[i,:]) for i in range(kmeans_init_test.shape[0])]\n",
    "        \n",
    "    return lndmk_tr, lndmk_test\n",
    "\n",
    "\n",
    "def init_step(dataset, model, device, pretrained, mode='kmeans',n_clusters=None):\n",
    "    \"\"\"Initialization of landmarks with k-means or k-means++ given dataset.\"\"\"\n",
    "    \n",
    "    if n_clusters==None:\n",
    "        n_clusters = len(np.unique(dataset.y))\n",
    "    \n",
    "    nexamples = len(dataset.x)\n",
    "        \n",
    "    X =  torch.stack([dataset.x[i] for i in range(nexamples)])\n",
    "    \n",
    "    if mode=='kmeans++':\n",
    "        if not pretrained: # find centroids in original space\n",
    "            landmarks = k_means_._init_centroids(X.cpu().numpy(), n_clusters, 'k-means++')\n",
    "            landmarks = torch.tensor(landmarks, device=device)\n",
    "            landmarks = landmarks.to(device)\n",
    "            lndmk_encoded,_ = model(landmarks)\n",
    "            \n",
    "        else:\n",
    "            X = X.to(device)\n",
    "            encoded,_ = model(X)\n",
    "            landmarks = k_means_._init_centroids(encoded.data.cpu().numpy(), n_clusters, 'k-means++')\n",
    "            lndmk_encoded = torch.tensor(landmarks, device=device)\n",
    "    \n",
    "    elif mode=='kmeans': # run kmeans clustering\n",
    "        if not pretrained: \n",
    "            kmeans = KMeans(n_clusters, random_state=0).fit(X.cpu().numpy())\n",
    "            landmarks = torch.tensor(kmeans.cluster_centers_, device=device)\n",
    "            landmarks = landmarks.to(device)\n",
    "            ##Feed forward net on landmarks  (k means cluster)\n",
    "            ##landmarks are k means cluster centers = coordinates of cluster center\n",
    "            #lndmk_encoded,_ = model(landmarks)\n",
    "            _ , lndmk_encoded, _ = model(landmarks)\n",
    "        \n",
    "        ##cluster on lower embedding space\n",
    "        else:\n",
    "            X = X.to(device)\n",
    "            #encoded,_ = model(X)\n",
    "            decode , encoded, logvar = model(X)\n",
    "            kmeans = KMeans(n_clusters, random_state=0).fit(encoded.data.cpu().numpy())\n",
    "            lndmk_encoded = torch.tensor(kmeans.cluster_centers_, device=device)\n",
    "    \n",
    "    return lndmk_encoded\n",
    "\n",
    "\n",
    "class VAE_MARS:\n",
    "    def __init__(self, n_clusters, params, \n",
    "                 labeled_data, unlabeled_data, \n",
    "                 pretrain_data=None, \n",
    "                 \n",
    "                 val_split=1.0, hid_dim_1=1000, hid_dim_2=100, p_drop=0.0, tau=0.2,\n",
    "                 \n",
    "                 latent_dim = 50, \n",
    "                 n_feature = 10, \n",
    "                 \n",
    "                 network_architecture={\"n_hidden_recog_1\": 250, \"n_hidden_recog_2\": 100,\n",
    "                                       \"n_hidden_recog_3\": 100, \"n_hidden_gener_1\": 100,\n",
    "                                       \"n_hidden_gener_2\": 100, \"n_hidden_gener_3\": 100},\n",
    "                 \n",
    "                 lambda_reconstruct = 1e-4, lambda_kl = 1e-3\n",
    "                ):\n",
    "        \n",
    "        \"\"\"Initialization of MARS.\n",
    "        n_clusters: number of clusters in the unlabeled meta-dataset\n",
    "        params: parameters of the MARS model\n",
    "        labeled_data: list of labeled datasets. Each dataset needs to be instance of CellDataset.\n",
    "        unlabeled_data: unlabeled dataset. Instance of CellDataset.\n",
    "        pretrain_data: dataset for pretraining MARS. Instance of CellDataset. If not specified, unlabeled_data\n",
    "                        will be used.\n",
    "        val_split: percentage of data to use for train/val split (default: 1, meaning no validation set)\n",
    "        hid_dim_1: dimension in the first layer of the network (default: 1000)\n",
    "        hid_dim_2: dimension in the second layer of the network (default: 100)\n",
    "        p_drop: dropout probability (default: 0)\n",
    "        tau: regularizer for inter-cluster distance\n",
    "        \"\"\"\n",
    "        train_load, test_load, pretrain_load, val_load = init_data_loaders(labeled_data, unlabeled_data, \n",
    "                                                                           pretrain_data, params.pretrain_batch, \n",
    "                                                                           val_split)\n",
    "        self.train_loader = train_load\n",
    "        self.test_loader = test_load\n",
    "        self.pretrain_loader = pretrain_load\n",
    "        self.val_loader = val_load\n",
    "        \n",
    "        ##data file type (string name)\n",
    "        self.labeled_metadata = [data.metadata for data in labeled_data]\n",
    "        self.unlabeled_metadata = unlabeled_data.metadata\n",
    "        \n",
    "        self.genes = unlabeled_data.yIDs\n",
    "        \n",
    "        ##number of genes \n",
    "        x_dim = self.test_loader.dataset.get_dim()\n",
    "        \n",
    "        ##KNN clusters\n",
    "        self.n_clusters = n_clusters\n",
    "        \n",
    "        ##General Parameters\n",
    "        self.device = params.device\n",
    "        self.epochs = params.epochs\n",
    "        self.epochs_pretrain = params.epochs_pretrain\n",
    "        self.pretrain_flag = params.pretrain\n",
    "        self.model_file = params.model_file\n",
    "        self.lr = params.learning_rate\n",
    "        self.lr_gamma = params.lr_scheduler_gamma\n",
    "        self.step_size = params.lr_scheduler_step\n",
    "        \n",
    "        self.tau = tau\n",
    "        \n",
    "        ##VAE parameter\n",
    "        self.lambda_reconstruct = lambda_reconstruct\n",
    "        self.lambda_kl = lambda_kl\n",
    "        \n",
    "#         self.hidden_encode1=network_architecture['n_hidden_recog_1']\n",
    "#         self.hidden_encode2=network_architecture['n_hidden_recog_2']\n",
    "#         self.hidden_encode3=network_architecture['n_hidden_recog_3']\n",
    "        \n",
    "#         self.hidden_decode1=network_architecture['n_hidden_gener_1']\n",
    "#         self.hidden_decode2=network_architecture['n_hidden_gener_2']\n",
    "#         self.hidden_decode3=network_architecture['n_hidden_gener_3']\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_feature = n_feature\n",
    "        \n",
    "        self.init_model(self.latent_dim, self.n_feature, network_architecture, \n",
    "                        self.lambda_reconstruct, self.lambda_kl, params.device)\n",
    "        \n",
    "        \n",
    "    ###################################################################\n",
    "    ###################################################################    \n",
    "    ###### With the fine tuned hyper parameter     \n",
    "    ###### Change the implementation of AE to VAE     \n",
    "    def init_model(self, \n",
    "                   latent_dim , \n",
    "                   n_feature, \n",
    "                   network_architecture, \n",
    "                   lambda_reconstruct, lambda_kl,\n",
    "                   device\n",
    "                  ):\n",
    "        \"\"\"\n",
    "        Initialize the model.\n",
    "        \"\"\"\n",
    "        #self.model = FullNet(x_dim, hid_dim, z_dim, p_drop).to(device)\n",
    "        self.model = VAE(latent_dim = latent_dim, \n",
    "                         n_feature = n_feature, \n",
    "                         network_architecture= network_architecture,\n",
    "                         lambda_reconstruct = lambda_reconstruct, \n",
    "                         lambda_kl = lambda_kl).to(device)\n",
    "        \n",
    "    ###################################################################\n",
    "    ###################################################################\n",
    "    \n",
    "    def init_optim(self, param1, param2, learning_rate):\n",
    "        \"\"\"Initializing optimizers.\"\"\"\n",
    "        \n",
    "        optim = torch.optim.Adam(params=param1, lr=learning_rate)\n",
    "        optim_landmk_test = torch.optim.Adam(params=param2, lr=learning_rate)\n",
    "        \n",
    "        return optim, optim_landmk_test\n",
    "    \n",
    "    ###################################################################\n",
    "    ###################################################################\n",
    "    ########VAE (KL + reconstruction loss + regularized)\n",
    "    def pretrain(self, optim):\n",
    "        \"\"\"\n",
    "        Pretraining model with variational autoencoder.\n",
    "        only on unannotated dataset \n",
    "        optim: optimizer\n",
    "        \"\"\"\n",
    "        print('Pretraining..')\n",
    "        \n",
    "        pretrain_loss =[]\n",
    "        \n",
    "        for e in range(self.epochs_pretrain):\n",
    "            \n",
    "            start = time.time()\n",
    "            \n",
    "            \n",
    "            for _, batch in enumerate(self.pretrain_loader):\n",
    "                \n",
    "                x,_,_ = batch\n",
    "                x = x.to(self.device)\n",
    "                \n",
    "                #_, decoded = self.model(x)\n",
    "                decoded, mu, logvar = self.model(x)\n",
    "                \n",
    "                #loss = reconstruction_loss(decoded, x) \n",
    "                loss = vae_loss(self.model, decoded, x, mu, logvar)\n",
    "                \n",
    "                optim.zero_grad()              \n",
    "                loss.backward()                    \n",
    "                optim.step() \n",
    "                \n",
    "                \n",
    "            pretrain_loss.append(loss) \n",
    "            \n",
    "            print(f\"Pretraining Epoch {e}, Loss: {loss}\")\n",
    "            print(\"Time: \", time.time()-start)\n",
    "        \n",
    "        print(\"Pretraining done\")\n",
    "        return mu, pretrain_loss\n",
    "    ###################################################################\n",
    "    ###################################################################\n",
    "    \n",
    "    \n",
    "    def train(self, evaluation_mode=True, save_all_embeddings=True):\n",
    "        \"\"\"Train model.\n",
    "        evaluation_mode: if True, validates model on the unlabeled dataset. \n",
    "        In the evaluation mode, ground truth labels of the unlabeled dataset must be \n",
    "        provided to validate model\n",
    "        \n",
    "        save_all_embeddings: if True, MARS embeddings for annotated and unannotated \n",
    "        experiments will be saved in an anndata object,\n",
    "        otherwise only unnanotated will be saved. \n",
    "        If naming is called after, all embeddings need to be saved\n",
    "        \n",
    "        return: adata: anndata object containing labeled and unlabeled meta-dataset \n",
    "        with MARS embeddings and estimated labels on the unlabeled dataset\n",
    "                landmk_all: landmarks of the labeled and unlabeled meta-dataset in the \n",
    "                order given for training. Landmarks on the unlabeled\n",
    "                            dataset are provided last\n",
    "                metrics: clustering metrics if evaluation_mode is True\n",
    "                \n",
    "        \"\"\"\n",
    "        tr_iter = [iter(dl) for dl in self.train_loader]\n",
    "        \n",
    "        if self.val_loader is not None:\n",
    "            val_iter = [iter(dl) for dl in self.val_loader]\n",
    "        \n",
    "        ##############################\n",
    "        ####Pre train step \n",
    "        ##############################\n",
    "        optim_pretrain = torch.optim.Adam(params=list(self.model.parameters()), lr=self.lr)\n",
    "        \n",
    "        if self.pretrain_flag:\n",
    "            pretrain_latent , pretrain_loss = self.pretrain(optim_pretrain)\n",
    "        else:\n",
    "            self.model.load_state_dict(torch.load(self.MODEL_FILE))    \n",
    "        ##############################\n",
    "        \n",
    "        \n",
    "        test_iter = iter(self.test_loader)\n",
    "        \n",
    "        \n",
    "        ##initialize training (annotated landmark) and testing (unannotated landmark)\n",
    "        landmk_tr, landmk_test = init_landmarks(self.n_clusters, \n",
    "                                                self.train_loader, \n",
    "                                                self.test_loader, \n",
    "                                                self.model, self.device)\n",
    "        \n",
    "        optim, optim_landmk_test = self.init_optim(list(self.model.encoder.parameters()), \n",
    "                                                   landmk_test, self.lr)\n",
    "        \n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optim,\n",
    "                                                       gamma=self.lr_gamma,\n",
    "                                                       step_size=self.step_size)\n",
    "        \n",
    "        latent_tracker = []\n",
    "        training_history = {'Loss':[],'Accuracy':[],'Loss_tracker':[]}\n",
    "        best_acc = 0\n",
    "        \n",
    "        for epoch in range(1, self.epochs+1):\n",
    "            \n",
    "            start = time.time()\n",
    "            \n",
    "            #########################\n",
    "            #####   Model training \n",
    "            #########################\n",
    "            self.model.train()\n",
    "            \n",
    "            ##Do_epoch train over each minibatch return\n",
    "            ##training loss, accuracy, updated landmarks (annoated and unannotated)\n",
    "            ##latent_history returns latents space for 1) training 2) testing 3) training landmark 4) testing landmark\n",
    "            loss_tr, acc_tr, landmk_tr, landmk_test, latent_history, loss_tracker = self.do_epoch(tr_iter, test_iter,\n",
    "                                                                                    optim, \n",
    "                                                                                    optim_landmk_test,\n",
    "                                                                                    landmk_tr, \n",
    "                                                                                    landmk_test)\n",
    "            \n",
    "            ##Loss training includes 1 - VAE loss 2- embedding and landmark distance \n",
    "            print(f'Epoch {epoch} Loss training: {loss_tr}, Accuracy training: {acc_tr}')\n",
    "            \n",
    "            print(\"Time: \", time.time()-start)\n",
    "        \n",
    "            ###Track model training \n",
    "            training_history['Loss'].append(loss_tr)\n",
    "            training_history['Accuracy'].append(acc_tr)\n",
    "            training_history['Loss_tracker'].append(loss_tracker)\n",
    "            latent_tracker.append(latent_history)\n",
    "            \n",
    "            ##only print out the last epoch result indicating end of training\n",
    "            if epoch==self.epochs: \n",
    "                print('\\n=== Epoch: {} ==='.format(epoch))\n",
    "                print('Train acc: {}'.format(acc_tr))\n",
    "            \n",
    "            if self.val_loader is None:\n",
    "                continue\n",
    "            \n",
    "            #########################\n",
    "            #####   Model evaluation \n",
    "            #########################\n",
    "            self.model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                loss_val, acc_val = self.do_val_epoch(val_iter, landmk_tr)\n",
    "                \n",
    "                if acc_val > best_acc:\n",
    "                    print('Saving model...')\n",
    "                    best_acc = acc_val\n",
    "                    best_state = self.model.state_dict()\n",
    "                    #torch.save(model.state_dict(), self.model_file)\n",
    "                postfix = ' (Best)' if acc_val >= best_acc else ' (Best: {})'.format(best_acc)\n",
    "                print('Val loss: {}, acc: {}{}'.format(loss_val, acc_val, postfix))\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "        if self.val_loader is None:\n",
    "            best_state = self.model.state_dict() # best is last\n",
    "        \n",
    "        landmk_all = landmk_tr + [torch.stack(landmk_test).squeeze()]\n",
    "        \n",
    "        ##Test time (assign labels to unlabaled data)\n",
    "        adata_test, eval_results = self.assign_labels(landmk_all[-1], evaluation_mode)\n",
    "        \n",
    "        adata = self.save_result(tr_iter, adata_test, save_all_embeddings)\n",
    "        \n",
    "        if evaluation_mode:\n",
    "            return adata, landmk_all, eval_results, training_history, latent_tracker, pretrain_latent , pretrain_loss\n",
    "        \n",
    "        return adata, landmk_all, training_history, latent_tracker, pretrain_latent , pretrain_loss\n",
    "    \n",
    "    def save_result(self, tr_iter, adata_test, save_all_embeddings):\n",
    "        \"\"\"Saving embeddings from labeled and unlabeled dataset, ground truth labels and \n",
    "        predictions to joint anndata object.\"\"\"\n",
    "        \n",
    "        adata_all = []\n",
    "\n",
    "        if save_all_embeddings:\n",
    "            for task in range(len(tr_iter)): # saving embeddings from labeled dataset\n",
    "                \n",
    "                task = int(task)\n",
    "                x, y, cells = next(tr_iter[task])\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                \n",
    "                #encoded,_ = self.model(x)\n",
    "                decoded, mu, logvar = self.model(x)\n",
    "                #adata_all.append(self.pack_anndata(x, cells, encoded, gtruth=y))\n",
    "                adata_all.append(self.pack_anndata(x, cells, mu, gtruth=y))\n",
    "            \n",
    "        adata_all.append(adata_test)    \n",
    "        \n",
    "        if save_all_embeddings:\n",
    "            adata = adata_all[0].concatenate(adata_all[1:], \n",
    "                                             batch_key='experiment',\n",
    "                                             batch_categories=self.labeled_metadata+[self.unlabeled_metadata])\n",
    "        else:\n",
    "            adata = adata_all[0]\n",
    "\n",
    "        adata.obsm['MARS_embedding'] = np.concatenate([a.uns['MARS_embedding'] for a in adata_all])\n",
    "        #adata.write('result_adata.h5ad')\n",
    "        \n",
    "        return adata\n",
    "    \n",
    "    def assign_labels(self, landmk_test, evaluation_mode):\n",
    "        \"\"\"Assigning cluster labels to the unlabeled meta-dataset.\n",
    "        test_iter: iterator over unlabeled dataset\n",
    "        landmk_test: landmarks in the unlabeled dataset\n",
    "        evaluation mode: computes clustering metrics if True\n",
    "        \"\"\"\n",
    "          \n",
    "        torch.no_grad()\n",
    "        self.model.eval() # eval mode\n",
    "        \n",
    "        test_iter = iter(self.test_loader)\n",
    "        \n",
    "        x_test, y_true, cells = next(test_iter) # cells are needed because dataset is in random order\n",
    "        x_test = x_test.to(self.device)\n",
    "        \n",
    "        #encoded_test,_ = self.model(x_test)\n",
    "        decoded, encoded_test, logvar = self.model(x_test)\n",
    "        \n",
    "        ###Embedding space eucledian distance \n",
    "        dists = euclidean_dist(encoded_test, landmk_test)\n",
    "        \n",
    "        ###Prediction based on the minimal distance to learned landmark\n",
    "        y_pred = torch.min(dists, 1)[1]\n",
    "        \n",
    "        adata = self.pack_anndata(x_test, cells, encoded_test, y_true, y_pred)\n",
    "        \n",
    "        eval_results = None\n",
    "        if evaluation_mode:\n",
    "            eval_results = compute_scores(y_true, y_pred)\n",
    "            \n",
    "        return adata, eval_results\n",
    "    \n",
    "    \n",
    "    def pack_anndata(self, x_input, cells, embedding, gtruth=[], estimated=[]):\n",
    "        \"\"\"Pack results in anndata object.\n",
    "        x_input: gene expressions in the input space\n",
    "        cells: cell identifiers\n",
    "        embedding: resulting embedding of x_test using MARS\n",
    "        landmk: MARS estimated landmarks\n",
    "        gtruth: ground truth labels if available (default: empty list)\n",
    "        estimated: MARS estimated clusters if available (default: empty list)\n",
    "        \"\"\"\n",
    "        adata = anndata.AnnData(x_input.data.cpu().numpy())\n",
    "        adata.obs_names = cells\n",
    "        adata.var_names = self.genes\n",
    "        if len(estimated)!=0:\n",
    "            adata.obs['MARS_labels'] = pd.Categorical(values=estimated.cpu().numpy())\n",
    "        if len(gtruth)!=0:\n",
    "            adata.obs['truth_labels'] = pd.Categorical(values=gtruth.cpu().numpy())\n",
    "        adata.uns['MARS_embedding'] = embedding.data.cpu().numpy()\n",
    "        \n",
    "        return adata\n",
    "    \n",
    "    \n",
    "    ####Originally, MARS does not care about reconstruction loss, it only minimizes embedding to landmark distances\n",
    "    ####We can alter this by adding reconstruction and KL term to the total loss being regularized \n",
    "    \n",
    "    def do_epoch(self, tr_iter, test_iter, optim, optim_landmk_test, landmk_tr, landmk_test):\n",
    "        \"\"\"\n",
    "        One training epoch.\n",
    "        tr_iter: iterator over labeled meta-data\n",
    "        test_iter: iterator over unlabeled meta-data\n",
    "        \n",
    "        optim: optimizer for embedding\n",
    "        optim_landmk_test: optimizer for test landmarks\n",
    "        \n",
    "        landmk_tr: landmarks of labeled meta-data from previous epoch\n",
    "        landmk_test: landmarks of unlabeled meta-data from previous epoch\n",
    "        \"\"\"\n",
    "        \n",
    "        latent_history = {'Train_latent':[], 'Train_label':[],\n",
    "                          'Test_latent':[], 'Test_label':[], \n",
    "                          'Train_landmark':[], 'Test_landmark':[]\n",
    "                         }\n",
    "        loss_tracker = {'Test_anno':0,\n",
    "                        'Train_latent_loss':0, 'Train_reconstr_loss': 0, \n",
    "                        'Test_latent_loss':0, 'Test_reconstr_loss': 0}\n",
    "        \n",
    "        self.set_requires_grad(False)\n",
    "        \n",
    "        ##Partially freeze landmark_test\n",
    "        for landmk in landmk_test:\n",
    "            landmk.requires_grad=False\n",
    "            \n",
    "        ##Initialize gradient of landmakr_test\n",
    "        optim_landmk_test.zero_grad()\n",
    "        \n",
    "        ##########################################################################\n",
    "        ##########################################################################\n",
    "        # Update centroids    \n",
    "        ##########################################################################\n",
    "        ##########################################################################\n",
    "        \n",
    "        task_idx = torch.randperm(len(tr_iter)) ##shuffle per epoch\n",
    "        ###Annotated landmark (No gradient descent needed)\n",
    "        for task in task_idx:\n",
    "            \n",
    "            ##Training set through the VAE to generate latent space\n",
    "            task = int(task)\n",
    "            x, y, _ = next(tr_iter[task])\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            #encoded,_ = self.model(x)\n",
    "            decoded, encoded, logvar = self.model(x)\n",
    "            \n",
    "            curr_landmk_tr = compute_landmarks_tr(encoded, y, landmk_tr[task], tau=self.tau)\n",
    "            landmk_tr[task] = curr_landmk_tr.data # save landmarks\n",
    "            \n",
    "            latent_history['Train_landmark'].append(encoded)\n",
    "            latent_history['Train_label'].append(y)\n",
    "        \n",
    "        ###Unannotated landmark (Use gradient descent to minimize)\n",
    "        \n",
    "        ##Unfreeze landmark_test --> autograd update centroids\n",
    "        for landmk in landmk_test:\n",
    "            landmk.requires_grad=True\n",
    "            \n",
    "        x, y_test,_ = next(test_iter)\n",
    "        x = x.to(self.device)\n",
    "        #encoded,_ = self.model(x)\n",
    "        decoded, encoded, logvar = self.model(x)\n",
    "        \n",
    "        ###minimize intra cluster difference and maximize inter cluster difference \n",
    "        loss, args_count = loss_test(encoded, \n",
    "                                     torch.stack(landmk_test).squeeze(), \n",
    "                                     self.tau)\n",
    "        loss.backward()\n",
    "        optim_landmk_test.step()\n",
    "        \n",
    "        latent_history['Test_landmark'].append(encoded)\n",
    "        latent_history['Test_label'].append(y_test)\n",
    "        loss_tracker['Test_anno']=loss\n",
    "        \n",
    "        ##########################################################################\n",
    "        ##########################################################################\n",
    "        # Update embedding\n",
    "        ##########################################################################\n",
    "        ##########################################################################\n",
    "        \n",
    "        self.set_requires_grad(True)\n",
    "        for landmk in landmk_test:\n",
    "            landmk.requires_grad=False\n",
    "            \n",
    "        optim.zero_grad()\n",
    "        total_accuracy = 0\n",
    "        total_loss = 0\n",
    "        ntasks = 0\n",
    "        mean_accuracy = 0\n",
    "        \n",
    "        ###Annotated set\n",
    "        task_idx = torch.randperm(len(tr_iter))\n",
    "        \n",
    "        for task in task_idx:\n",
    "            task = int(task)\n",
    "            x, y, _ = next(tr_iter[task])\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            #encoded,_ = self.model(x)\n",
    "            decoded, encoded, logvar = self.model(x)\n",
    "            \n",
    "            ###Eucledian distance between embedding and landmark\n",
    "            loss, acc = loss_task(encoded, landmk_tr[task], y, criterion='dist')\n",
    "            \n",
    "            ##Add VAE reconstruction loss to the model \n",
    "            annotated_vae_loss = vae_loss(self.model, decoded, x, encoded, logvar)\n",
    "            \n",
    "            ##Record latent space\n",
    "            latent_history['Train_latent'].append(encoded)\n",
    "            \n",
    "            loss_tracker['Train_latent_loss']=loss\n",
    "            loss_tracker['Train_reconstr_loss']=annotated_vae_loss\n",
    "            \n",
    "            total_loss += loss\n",
    "            #total_loss += annotated_vae_loss\n",
    "            \n",
    "            total_accuracy += acc.item()\n",
    "            \n",
    "            ntasks += 1\n",
    "        \n",
    "        if ntasks>0:\n",
    "            mean_accuracy = total_accuracy / ntasks\n",
    "        \n",
    "        ##Un-Annotated\n",
    "        # test part\n",
    "        x,y,_ = next(test_iter)\n",
    "        x = x.to(self.device)\n",
    "        #encoded,_ = self.model(x)\n",
    "        decoded, encoded, logvar = self.model(x)\n",
    "        \n",
    "        loss,_ = loss_test(encoded, torch.stack(landmk_test).squeeze(), self.tau)\n",
    "        \n",
    "        ##Add VAE reconstruction loss to the model \n",
    "        unannotated_vae_loss = vae_loss(self.model, decoded, x, encoded, logvar)\n",
    "        latent_history['Test_latent'].append(encoded)\n",
    "        \n",
    "        loss_tracker['Test_latent_loss']=(loss)\n",
    "        loss_tracker['Test_reconstr_loss']=(unannotated_vae_loss)\n",
    "            \n",
    "        total_loss += loss\n",
    "        #total_loss += unannotated_vae_loss\n",
    "        ntasks += 1\n",
    "    \n",
    "        mean_loss = total_loss / ntasks\n",
    "        \n",
    "        mean_loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        return mean_loss, mean_accuracy, landmk_tr, landmk_test, latent_history, loss_tracker\n",
    "    \n",
    "    def do_val_epoch(self, val_iter, prev_landmk):\n",
    "        \"\"\"One epoch of validation.\n",
    "        val_iter: iterator over validation set\n",
    "        prev_landmk: landmarks from previous epoch\n",
    "        \"\"\"\n",
    "        ntasks = len(val_iter)\n",
    "        task_idx = torch.randperm(ntasks)\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        \n",
    "        for task in task_idx:\n",
    "            x, y, _ = next(val_iter[task])\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            #encoded = self.model(x)\n",
    "            decoded, encoded, logvar = self.model(x)\n",
    "            \n",
    "            ###Eucledian distance between embedding and landmark\n",
    "            loss, acc = loss_task(encoded, prev_landmk[task], y, criterion='dist')\n",
    "            val_vae_loss = vae_loss(self.model, decoded, x, encoded, logvar)\n",
    "            \n",
    "            #total_loss += val_vae_loss\n",
    "            total_loss += loss\n",
    "            total_accuracy += acc.item()\n",
    "        \n",
    "        mean_accuracy = total_accuracy / ntasks\n",
    "        mean_loss = total_loss / ntasks\n",
    "        \n",
    "        return mean_loss, mean_accuracy\n",
    "    \n",
    "    \n",
    "    def set_requires_grad(self, requires_grad):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "    \n",
    "    def name_cell_types(self, adata, landmk_all, cell_name_mappings, \n",
    "                        top_match=5, umap_reduce_dim=True, ndim=10):\n",
    "        \"\"\"For each test cluster, estimate sigma and mean. \n",
    "        Fit Gaussian distribution with that mean and sigma\n",
    "        and calculate the probability of each of the train landmarks \n",
    "        to be the neighbor to the mean data point.\n",
    "        Normalization is performed with regards to all other landmarks in train.\"\"\"\n",
    "        \n",
    "        experiments = list(OrderedDict.fromkeys(list(adata.obs['experiment'])))\n",
    "        \n",
    "        ###only get labels from labeled data\n",
    "        encoded_tr = []\n",
    "        landmk_tr = []\n",
    "        landmk_tr_labels = []\n",
    "        for idx, exp in enumerate(experiments[:-1]):\n",
    "            tiss = adata[adata.obs['experiment'] == exp,:]\n",
    "            \n",
    "            if exp==self.unlabeled_metadata: \n",
    "                raise ValueError(\"Error: Unlabeled dataset needs to be last one in the input anndata object.\")\n",
    "                \n",
    "            encoded_tr.append(tiss.obsm['MARS_embedding'])\n",
    "            landmk_tr.append(landmk_all[idx])\n",
    "            landmk_tr_labels.append(np.unique(tiss.obs['truth_labels']))\n",
    "            \n",
    "        tiss = adata[adata.obs['experiment'] == self.unlabeled_metadata,:]\n",
    "        ypred_test = tiss.obs['MARS_labels']\n",
    "        uniq_ytest = np.unique(ypred_test)\n",
    "        encoded_test = tiss.obsm['MARS_embedding']\n",
    "        \n",
    "        landmk_tr_labels = np.concatenate(landmk_tr_labels)\n",
    "        encoded_tr = np.concatenate(encoded_tr)\n",
    "        landmk_tr = np.concatenate([p.cpu() for p in landmk_tr])\n",
    "        \n",
    "        if  umap_reduce_dim:\n",
    "            encoded_extend = np.concatenate((encoded_tr, encoded_test, landmk_tr))\n",
    "            adata = anndata.AnnData(encoded_extend)\n",
    "            sc.pp.neighbors(adata, n_neighbors=15, use_rep='X')\n",
    "            sc.tl.umap(adata, n_components=ndim)\n",
    "            encoded_extend = adata.obsm['X_umap']\n",
    "            n1 = len(encoded_tr)\n",
    "            n2 = n1 + len(encoded_test)\n",
    "            \n",
    "            ##UMAP embedding space\n",
    "            encoded_tr = encoded_extend[:n1,:]\n",
    "            encoded_test = encoded_extend[n1:n2,:]\n",
    "            landmk_tr = encoded_extend[n2:,:]\n",
    "        \n",
    "        interp_names = defaultdict(list)\n",
    "        for ytest in uniq_ytest:\n",
    "            print('\\nCluster label: {}'.format(str(ytest)))\n",
    "            idx = np.where(ypred_test==ytest)\n",
    "            subset_encoded = encoded_test[idx[0],:]\n",
    "            \n",
    "            mean = np.expand_dims(np.mean(subset_encoded, axis=0),0)\n",
    "            \n",
    "            sigma  = self.estimate_sigma(subset_encoded)\n",
    "            \n",
    "            prob = np.exp(-np.power(distance.cdist(mean, landmk_tr, metric='euclidean'),2)/(2*sigma*sigma))\n",
    "            prob = np.squeeze(prob, 0)\n",
    "            normalizat = np.sum(prob)\n",
    "            \n",
    "            if normalizat==0:\n",
    "                print('Unassigned')\n",
    "                interp_names[ytest].append(\"unassigned\")\n",
    "                continue\n",
    "            \n",
    "            prob = np.divide(prob, normalizat)\n",
    "            \n",
    "            uniq_tr = np.unique(landmk_tr_labels)\n",
    "            prob_unique = []\n",
    "            \n",
    "            for cell_type in uniq_tr: # sum probabilities of same landmarks\n",
    "                prob_unique.append(np.sum(prob[np.where(landmk_tr_labels==cell_type)]))\n",
    "            \n",
    "            sorted = np.argsort(prob_unique, axis=0)\n",
    "            best = uniq_tr[sorted[-top_match:]]\n",
    "            sortedv = np.sort(prob_unique, axis=0)\n",
    "            sortedv = sortedv[-top_match:]\n",
    "            for idx, b in enumerate(best):\n",
    "                interp_names[ytest].append((cell_name_mappings[b], sortedv[idx]))\n",
    "                print('{}: {}'.format(cell_name_mappings[b], sortedv[idx]))\n",
    "                \n",
    "        return interp_names\n",
    "    \n",
    "    \n",
    "    def estimate_sigma(self, dataset):\n",
    "        nex = dataset.shape[0]\n",
    "        dst = []\n",
    "        for i in range(nex):\n",
    "            for j in range(i+1, nex):\n",
    "                dst.append(distance.euclidean(dataset[i,:],dataset[j,:]))\n",
    "        return np.std(dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining..\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [128 x 261], m2: [377 x 250] at /tmp/pip-req-build-9oilk29k/aten/src/TH/generic/THTensorMath.cpp:197",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-6e8751fa4985>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m vae_merge_adata, vae_merge_landmarks, vae_merge_scores , vae_merge_training_history, vae_merge_latent_tracker, pretrainlatent, pretrainloss = vae_merge_mars.train(evaluation_mode=True,\n\u001b[0;32m---> 16\u001b[0;31m                                                                                                                                save_all_embeddings=True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-629d07ca64c2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, evaluation_mode, save_all_embeddings)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrain_flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0mpretrain_latent\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mpretrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim_pretrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-629d07ca64c2>\u001b[0m in \u001b[0;36mpretrain\u001b[0;34m(self, optim)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m                 \u001b[0;31m#_, decoded = self.model(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m                 \u001b[0mdecoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m                 \u001b[0;31m#loss = reconstruction_loss(decoded, x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/mars/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-629d07ca64c2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-629d07ca64c2>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mlatent\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         '''\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/mars/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/mars/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/mars/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/mars/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/mars/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [128 x 261], m2: [377 x 250] at /tmp/pip-req-build-9oilk29k/aten/src/TH/generic/THTensorMath.cpp:197"
     ]
    }
   ],
   "source": [
    "vae_merge_mars = VAE_MARS(n_clusters, params, [annotated_colon, annotated_scrna] , unannotated, pretrain, \n",
    "                \n",
    "                ##These are roll over parametesr form original model \n",
    "                hid_dim_1=1000, hid_dim_2=100, p_drop=0.0, tau=1e-3,\n",
    "                \n",
    "                ##VAE MARS specific parameters\n",
    "                latent_dim = 50 , n_feature = len(xy), \n",
    "                \n",
    "                network_architecture={\"n_hidden_recog_1\": 250, \"n_hidden_recog_2\": 100,\n",
    "                           \"n_hidden_recog_3\": 100, \"n_hidden_gener_1\": 100,\n",
    "                           \"n_hidden_gener_2\": 100, \"n_hidden_gener_3\": 100},\n",
    "                \n",
    "                lambda_reconstruct = 1, lambda_kl = 4)\n",
    "\n",
    "vae_merge_adata, vae_merge_landmarks, vae_merge_scores , vae_merge_training_history, vae_merge_latent_tracker, pretrainlatent, pretrainloss = vae_merge_mars.train(evaluation_mode=True,\n",
    "                                                                                                                               save_all_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_loss_tracker (vae_merge_training_history, save =True)\n",
    "MARS_history(vae_merge_training_history, save=True)\n",
    "MARS_latent_pca(vae_merge_latent_tracker , 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_loss_tracker (vae_merge_training_history, save =True)\n",
    "MARS_history(vae_merge_training_history, save=True)\n",
    "MARS_latent_pca(vae_merge_latent_tracker , 29)\n",
    "MARS_latent_umap( vae_merge_adata, plot_gene_list = ['MARS_labels','experiment'], save=True)\n",
    "cell_type_assign(vae_merge_adata, save=True)\n",
    "MARS_silhouette(vae_merge_adata, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_merge_mars11 = VAE_MARS(11 , params, [annotated] , unannotated, pretrain, \n",
    "                \n",
    "                ##These are roll over parametesr form original model \n",
    "                hid_dim_1=1000, hid_dim_2=100, p_drop=0.0, tau=1e-3,\n",
    "                \n",
    "                ##VAE MARS specific parameters\n",
    "                latent_dim = 50 , n_feature = len(xy), \n",
    "                \n",
    "                network_architecture={\"n_hidden_recog_1\": 250, \"n_hidden_recog_2\": 100,\n",
    "                           \"n_hidden_recog_3\": 100, \"n_hidden_gener_1\": 100,\n",
    "                           \"n_hidden_gener_2\": 100, \"n_hidden_gener_3\": 100},\n",
    "                \n",
    "                lambda_reconstruct = 1, lambda_kl = 4)\n",
    "\n",
    "vae_merge_adata11, vae_merge_landmarks11, vae_merge_scores11 , vae_merge_training_history11, vae_merge_latent_tracker11, pretrainlatent11, pretrainloss11 = vae_merge_mars11.train(evaluation_mode=True,                                                                                                                              save_all_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_loss_tracker (vae_merge_training_history11, save =True)\n",
    "MARS_history(vae_merge_training_history11, save=True)\n",
    "MARS_latent_pca(vae_merge_latent_tracker11 , 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_latent_umap(vae_merge_adata11, plot_gene_list = ['MARS_labels','experiment'], save=True)\n",
    "cell_type_assign(vae_merge_adata11, save=True)\n",
    "#MARS_silhouette(vae_merge_adata11, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_assign(vae_merge_adata, save=True)\n",
    "cell_type_assign(ae_merge_adata, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CVAE MARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('-pretrain_batch', '--pretrain_batch',\n",
    "                        type=int,\n",
    "                        help='Batch size for pretraining. Default: no batch',\n",
    "                        default=None)\n",
    "    \n",
    "    parser.add_argument('-pretrain','--pretrain',\n",
    "                        type = bool,\n",
    "                        default = True,\n",
    "                        help='Pretrain model with autoencoder; otherwise load existing')\n",
    "    \n",
    "    parser.add_argument('-nepoch', '--epochs',\n",
    "                        type=int,\n",
    "                        help='number of epochs to train for',\n",
    "                        default=30)\n",
    "\n",
    "    parser.add_argument('-nepoch_pretrain', '--epochs_pretrain',\n",
    "                        type=int,\n",
    "                        help='number of epochs to pretrain for',\n",
    "                        default=25)\n",
    "\n",
    "    parser.add_argument('-source_file','--model_file',\n",
    "                        type = str,\n",
    "                        default = 'trained_models/source.pt',\n",
    "                        help='location for storing source model and data')\n",
    "\n",
    "    parser.add_argument('-lr', '--learning_rate',\n",
    "                        type=float,\n",
    "                        help='learning rate for the model, default=0.001',\n",
    "                        default=0.001)\n",
    "\n",
    "    parser.add_argument('-lrS', '--lr_scheduler_step',\n",
    "                        type=int,\n",
    "                        help='StepLR learning rate scheduler step, default=20',\n",
    "                        default=20) \n",
    "\n",
    "    parser.add_argument('-lrG', '--lr_scheduler_gamma',\n",
    "                        type=float,\n",
    "                        help='StepLR learning rate scheduler gamma, default=0.5',\n",
    "                        default=0.5)\n",
    "  \n",
    "    parser.add_argument('-seed', '--manual_seed',\n",
    "                        type=int,\n",
    "                        help='input for the manual seeds initializations',\n",
    "                        default=3)\n",
    "    \n",
    "    parser.add_argument('--cuda',\n",
    "                        action='store_true',\n",
    "                        help='enables cuda')\n",
    "    \n",
    "    return parser\n",
    "\n",
    "\n",
    "'''\n",
    "Class representing dataset for an single-cell experiment.\n",
    "'''\n",
    "\n",
    "IMG_CACHE = {}\n",
    "\n",
    "\n",
    "class ExperimentDataset(data.Dataset):\n",
    "    \n",
    "    \n",
    "    def __init__(self, x, cells, genes, metadata, y=[]):\n",
    "        '''\n",
    "        x: numpy array of gene expressions of cells (rows are cells)\n",
    "        cells: cell IDs in the order of appearance\n",
    "        genes: gene IDs in the order of appearance\n",
    "        metadata: experiment identifier\n",
    "        y: numeric labels of cells (empty list if unknown)\n",
    "        '''\n",
    "        super(ExperimentDataset, self).__init__()\n",
    "        \n",
    "        self.nitems = x.shape[0]\n",
    "        if len(y)>0:\n",
    "            print(\"== Dataset: Found %d items \" % x.shape[0])\n",
    "            print(\"== Dataset: Found %d classes\" % len(np.unique(y)))\n",
    "                \n",
    "        if type(x)==torch.Tensor:\n",
    "            self.x = x\n",
    "        else:\n",
    "            shape = x.shape[1]\n",
    "            self.x = [torch.from_numpy(inst).view(shape).float() for inst in x]\n",
    "        if len(y)==0:\n",
    "            y = np.zeros(len(self.x), dtype=np.int64)\n",
    "        self.y = tuple(y.tolist())\n",
    "        self.xIDs = cells\n",
    "        self.yIDs = genes\n",
    "        self.metadata = metadata\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx].squeeze(), self.y[idx], self.xIDs[idx], self.metadata[idx].squeeze()\n",
    "    #, self.yIDs[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nitems\n",
    "    \n",
    "    def get_dim(self):\n",
    "        return self.x[0].shape[0]\n",
    "\n",
    "class EpochSampler(object):\n",
    "    '''\n",
    "    EpochSampler: yield permuted indexes at each epoch.\n",
    "   \n",
    "    __len__ returns the number of episodes per epoch (same as 'self.iterations').\n",
    "    '''\n",
    "\n",
    "    def __init__(self, indices):\n",
    "        '''\n",
    "        Initialize the EpochSampler object\n",
    "        Args:\n",
    "        - labels: an iterable containing all the labels for the current dataset\n",
    "        samples indexes will be infered from this iterable.\n",
    "        - iterations: number of epochs\n",
    "        '''\n",
    "        super(EpochSampler, self).__init__()\n",
    "        \n",
    "        self.indices = indices\n",
    "        \n",
    "\n",
    "    def __iter__(self):\n",
    "        '''\n",
    "        yield a batch of indexes\n",
    "        '''\n",
    "        \n",
    "    \n",
    "        while(True):\n",
    "            shuffled_idx = self.indices[torch.randperm(len(self.indices))]\n",
    "            \n",
    "            yield shuffled_idx\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        returns the number of iterations (episodes) per epoch\n",
    "        '''\n",
    "        return self.iterations\n",
    "    \n",
    "import torch\n",
    "from torch.utils.data import DataLoader ##prefetch by batch\n",
    "#from model.epoch_sampler import EpochSampler\n",
    "\n",
    "\n",
    "def init_labeled_loader(data, val_split = 0.8):\n",
    "    \"\"\"Initialize loaders for train and validation sets. \n",
    "    Class labels are used only\n",
    "    for stratified sampling between train and validation set.\n",
    "    \n",
    "    Validation_split = % keeps in training set \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    target = torch.tensor(list(data.y))\n",
    "    uniq = torch.unique(target, sorted=True)\n",
    "    \n",
    "    class_idxs = list(map(lambda c: target.eq(c).nonzero(), uniq))\n",
    "    class_idxs = [idx[torch.randperm(len(idx))] for idx in class_idxs]\n",
    "    \n",
    "    train_idx = torch.cat([idx[:int(val_split*len(idx))] for idx in class_idxs])\n",
    "    val_idx = torch.cat([idx[int(val_split*len(idx)):] for idx in class_idxs])\n",
    "    \n",
    "    train_loader = DataLoader(data, \n",
    "                              batch_sampler=EpochSampler(train_idx),\n",
    "                              pin_memory=True)\n",
    "    \n",
    "    val_loader = DataLoader(data, \n",
    "                            batch_sampler=EpochSampler(val_idx),\n",
    "                            pin_memory=True)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def init_loader(datasets, val_split = 0.8):\n",
    "    \n",
    "    train_loader_all = []\n",
    "    val_loader_all = []\n",
    "    \n",
    "    for data in datasets:\n",
    "        \n",
    "        curr_load_tr, curr_load_val = init_labeled_loader(data, val_split)\n",
    "        train_loader_all.append(curr_load_tr)\n",
    "        val_loader_all.append(curr_load_val)\n",
    "    \n",
    "    if val_split==1:\n",
    "        val_loader_all = None\n",
    "        \n",
    "    return train_loader_all, val_loader_all\n",
    "\n",
    "\n",
    "def init_data_loaders(labeled_data, unlabeled_data, \n",
    "                      pretrain_data, pretrain_batch, val_split):\n",
    "    \n",
    "    \"\"\"Initialize loaders for pretraing, \n",
    "    training (labeled and unlabeled datasets) and validation. \"\"\"\n",
    "    \n",
    "    train_loader, val_loader = init_loader(labeled_data, val_split)\n",
    "    \n",
    "    if not pretrain_data:\n",
    "        pretrain_data = unlabeled_data\n",
    "    \n",
    "    pretrain_loader = torch.utils.data.DataLoader(dataset=pretrain_data,\n",
    "                                                  shuffle=True,\n",
    "                                                  batch_size=pretrain_batch if pretrain_batch!=None else len(unlabeled_data.x))        \n",
    "    test_loader = DataLoader(unlabeled_data, \n",
    "                            batch_sampler=EpochSampler(torch.randperm(len(unlabeled_data.x))),\n",
    "                            pin_memory=True) \n",
    "    \n",
    "    #test_loader,_ = init_loader([unlabeled_data], 1.0) # to reproduce results in the paper\n",
    "    #test_loader = test_loader[0]\n",
    "    return train_loader, test_loader, pretrain_loader, val_loader\n",
    "           \n",
    "           \n",
    "def euclidean_dist(x, y):\n",
    "    '''\n",
    "    Compute euclidean distance between two tensors\n",
    "    '''\n",
    "    # x: N x D\n",
    "    # y: M x D\n",
    "    n = x.size(0)\n",
    "    m = y.size(0)\n",
    "    d = x.size(1)\n",
    "    if d != y.size(1):\n",
    "        raise Exception\n",
    "\n",
    "    x = x.unsqueeze(1).expand(n, m, d)\n",
    "    y = y.unsqueeze(0).expand(n, m, d)\n",
    "\n",
    "    return torch.pow(x - y, 2).sum(2)\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "from scipy.optimize import linear_sum_assignment \n",
    "\n",
    "def compute_scores(y_true, y_pred, scoring={'accuracy','precision','recall','nmi',\n",
    "                                                'adj_rand','f1_score','adj_mi'}):\n",
    "    y_true = y_true.cpu().numpy()\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    \n",
    "    scores = {}\n",
    "    y_true, y_pred = hungarian_match(y_true, y_pred)\n",
    "    set_scores(scores, y_true, y_pred, scoring)\n",
    "        \n",
    "    return scores\n",
    "\n",
    "\n",
    "def set_scores(scores, y_true, y_pred, scoring):\n",
    "    labels=list(set(y_true))\n",
    "    \n",
    "    for metric in scoring:\n",
    "        if metric=='accuracy':\n",
    "            scores[metric] = metrics.accuracy_score(y_true, y_pred)\n",
    "        elif metric=='precision':\n",
    "            scores[metric] = metrics.precision_score(y_true, y_pred, labels, average='macro')\n",
    "        elif metric=='recall':\n",
    "            scores[metric] = metrics.recall_score(y_true, y_pred, labels, average='macro')\n",
    "        elif metric=='f1_score':\n",
    "            scores[metric] = metrics.f1_score(y_true, y_pred, labels, average='macro')\n",
    "        elif metric=='nmi':\n",
    "            scores[metric] = metrics.normalized_mutual_info_score(y_true, y_pred)\n",
    "        elif metric=='adj_mi':\n",
    "            scores[metric] = metrics.adjusted_mutual_info_score(y_true, y_pred)\n",
    "        elif metric=='adj_rand':\n",
    "            scores[metric] = metrics.adjusted_rand_score(y_true, y_pred)\n",
    "                \n",
    "                \n",
    "def hungarian_match(y_true, y_pred):\n",
    "    \"\"\"Matches predicted labels to original using hungarian algorithm.\"\"\"\n",
    "    \n",
    "    y_true = adjust_range(y_true)\n",
    "    y_pred = adjust_range(y_pred)\n",
    "    \n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    # Confusion matrix.\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    ind = linear_sum_assignment(-w)\n",
    "    ind = np.asarray(ind)\n",
    "    ind = np.transpose(ind)\n",
    "    d = {i:j for i, j in ind}\n",
    "    y_pred = np.array([d[v] for v in y_pred])\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "def adjust_range(y):\n",
    "    \"\"\"Assures that the range of indices if from 0 to n-1.\"\"\"\n",
    "    y = np.array(y, dtype=np.int64)\n",
    "    val_set = set(y)\n",
    "    mapping = {val:i for  i,val in enumerate(val_set)}\n",
    "    y = np.array([mapping[val] for val in y], dtype=np.int64)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "label = torch.as_tensor (np.concatenate( (np.zeros(raw_rna.shape[0]),np.ones(raw_atac.shape[0]) ), axis=0))\n",
    "one_hot_label = torch.nn.functional.one_hot(label.to(torch.int64), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_annotated = ExperimentDataset(raw_rna, \n",
    "                              mit_data.obs.index, \n",
    "                              xy, \n",
    "                              one_hot_label[:raw_rna.shape[0]], \n",
    "                              rna_y['dbscan'])\n",
    "\n",
    "cvae_unannotated = ExperimentDataset(raw_atac, \n",
    "                                scatac.obs.index, \n",
    "                                xy, \n",
    "                                one_hot_label[raw_rna.shape[0]:], \n",
    "                                atac_y['dbscan'])\n",
    "\n",
    "cvae_pretrain = ExperimentDataset(raw_atac, \n",
    "                             scatac.obs.index, \n",
    "                             xy,\n",
    "                             one_hot_label[raw_rna.shape[0]:])\n",
    "\n",
    "n_clusters= len(np.unique(unannotated.y))\n",
    "\n",
    "params, unknown= get_parser().parse_known_args()\n",
    "if torch.cuda.is_available() and not params.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "device = 'cuda:0' if torch.cuda.is_available() and params.cuda else 'cpu'\n",
    "\n",
    "params.device = device\n",
    "params.epochs_pretrain = 30\n",
    "params.pretrain_batch = 128\n",
    "params.epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(self, latent_dim , n_feature , network_architecture , \n",
    "                 lambda_reconstruct, lambda_kl , batch_shape=2,\n",
    "                 p_drop=0.2):\n",
    "        \n",
    "        super(CVAE, self).__init__()\n",
    "        \n",
    "        self.lambda_reconstruct = lambda_reconstruct\n",
    "        self.lambda_kl = lambda_kl\n",
    "        \n",
    "        self.hidden_encode1=network_architecture['n_hidden_recog_1']\n",
    "        self.hidden_encode2=network_architecture['n_hidden_recog_2']\n",
    "        self.hidden_encode3=network_architecture['n_hidden_recog_3']\n",
    "        \n",
    "        self.hidden_decode1=network_architecture['n_hidden_gener_1']\n",
    "        self.hidden_decode2=network_architecture['n_hidden_gener_2']\n",
    "        self.hidden_decode3=network_architecture['n_hidden_gener_3']\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_feature = n_feature\n",
    "        \n",
    "        self.batch_shape = batch_shape\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(self.n_feature + self.batch_shape , self.hidden_encode1, bias=True),\n",
    "            \n",
    "            nn.ELU(alpha=0.2),\n",
    "            nn.Dropout(p=p_drop),\n",
    "            nn.Linear(self.hidden_encode1, self.hidden_encode2, bias=True),\n",
    "            nn.ELU(alpha=0.2),\n",
    "            nn.Linear(self.hidden_encode2, self.hidden_encode3, bias=True),\n",
    "            nn.ELU(alpha=0.2)\n",
    "        )\n",
    "        \n",
    "        self.mu = nn.Linear(self.hidden_encode3, self.latent_dim, bias=True)\n",
    "        self.logvar = nn.Linear(self.hidden_encode3, self.latent_dim, bias=True)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(self.latent_dim + self.batch_shape, self.hidden_decode1, bias=True),\n",
    "            \n",
    "            nn.ELU(alpha=0.2),\n",
    "            nn.Dropout(p=p_drop),\n",
    "            nn.Linear(self.hidden_decode1, self.hidden_decode2, bias=True),\n",
    "            nn.ELU(alpha=0.2),\n",
    "            nn.Linear(self.hidden_decode2, self.hidden_decode3, bias=True),\n",
    "            nn.ELU(alpha=0.2),\n",
    "            nn.Linear(self.hidden_decode3, self.n_feature, bias=True)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def encode(self, x):\n",
    "        '''\n",
    "        Return latent parameters\n",
    "        '''\n",
    "        encoded = self.encoder(x)\n",
    "        \n",
    "        mu = self.mu(encoded)\n",
    "        logvar = self.logvar(encoded)\n",
    "        \n",
    "        return [mu, logvar]\n",
    "        \n",
    "    \n",
    "    def decode(self, z):\n",
    "        '''\n",
    "        Reconstruct\n",
    "        '''\n",
    "        decoded = self.decoder(z)\n",
    "        return decoded\n",
    "    \n",
    "    \n",
    "    def reparameterize(self, mu , logvar):\n",
    "        ''' \n",
    "        Reparametraization sample N(mu, var) from N(0,1) noise\n",
    "        '''\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std) #std represents the size of the tensor\n",
    "        \n",
    "        return eps*std + mu                \n",
    "      \n",
    "    def forward(self, x, batch_label):\n",
    "        \n",
    "        x = torch.cat((x, batch_label), dim=1)\n",
    "        \n",
    "        mu, logvar = self.encode(x)\n",
    "        \n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        z = torch.cat((z, batch_label), dim=1)\n",
    "        \n",
    "        decoded = self.decode(z)\n",
    "        \n",
    "        return decoded, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_task(encoded, prototypes, target, criterion='dist'):\n",
    "    \"\"\"Calculate loss.\n",
    "    criterion: NNLoss - assign to closest prototype and calculate NNLoss\n",
    "    dist - loss is distance to prototype that example needs to be assigned to\n",
    "                and -distance to prototypes from other class\n",
    "    \"\"\"\n",
    "    \n",
    "    uniq = torch.unique(target, sorted=True)\n",
    "    \n",
    "    ###index of samples for each class of labels\n",
    "    class_idxs = list(map(lambda c: target.eq(c).nonzero(), uniq))\n",
    "    \n",
    "    # prepare targets so they start from 0,1\n",
    "    for idx,v in enumerate(uniq):\n",
    "        target[target==v]=idx\n",
    "    \n",
    "    dists = euclidean_dist(encoded, prototypes)\n",
    "    \n",
    "    if criterion=='NNLoss':\n",
    "       \n",
    "        loss = torch.nn.NLLLoss()\n",
    "        log_p_y = F.log_softmax(-dists, dim=1)\n",
    "        \n",
    "        loss_val = loss(log_p_y, target)\n",
    "        _, y_hat = log_p_y.max(1)\n",
    "        \n",
    "    \n",
    "    elif criterion=='dist':\n",
    "        \n",
    "        loss_val = torch.stack([dists[idx_example, idx_proto].mean(0) for idx_proto,idx_example in enumerate(class_idxs)]).mean()\n",
    "        #loss_val1 = loss_val1/len(embeddings) \n",
    "        y_hat = torch.max(-dists,1)[1]\n",
    "        \n",
    "    acc_val = y_hat.eq(target.squeeze()).float().mean()    \n",
    "        \n",
    "    return loss_val, acc_val\n",
    "\n",
    "def loss_test_nn(encoded, prototypes):\n",
    "    \n",
    "    dists = euclidean_dist(encoded, prototypes)\n",
    "    min_dist = torch.min(dists, 1)\n",
    "    \n",
    "    y_hat = min_dist[1]\n",
    "    args_uniq = torch.unique(y_hat, sorted=True)\n",
    "    args_count = torch.stack([(y_hat==x_u).sum() for x_u in args_uniq])\n",
    "    print(args_count)\n",
    "    \n",
    "    loss = torch.nn.NLLLoss()\n",
    "    log_p_y = F.log_softmax(-dists, dim=1)\n",
    "    print(log_p_y.shape)\n",
    "        \n",
    "    loss_val = loss(log_p_y, y_hat)\n",
    "    _, y_hat = log_p_y.max(1)\n",
    "    \n",
    "    return loss_val, args_count\n",
    "\n",
    "###Intra cluster distance\n",
    "def loss_test_basic(encoded, prototypes):\n",
    "    \n",
    "    dists = euclidean_dist(encoded, prototypes)\n",
    "    min_dist = torch.min(dists, 1)\n",
    "    \n",
    "    y_hat = min_dist[1]\n",
    "    args_uniq = torch.unique(y_hat, sorted=True)\n",
    "    args_count = torch.stack([(y_hat==x_u).sum() for x_u in args_uniq])\n",
    "    #print(args_count)\n",
    "    \n",
    "    min_dist = min_dist[0] # get_distances\n",
    "    \n",
    "    #thr = torch.stack([torch.sort(min_dist[y_hat==idx_class])[0][int(len(min_dist[y_hat==idx_class])*0.9)] for idx_class in args_uniq])\n",
    "    #loss_val = torch.stack([min_dist[y_hat==idx_class][min_dist[y_hat==idx_class]>=thr[idx_class]].mean(0) for idx_class in args_uniq]).mean()\n",
    "    \n",
    "    loss_val = torch.stack([min_dist[y_hat==idx_class].mean(0) for idx_class in args_uniq]).mean()\n",
    "    \n",
    "    #loss_val,_ = loss_task(encoded, prototypes, y_hat, criterion='dist') # same\n",
    "    \n",
    "    return loss_val, args_count\n",
    "\n",
    "##For unannotated set \n",
    "def loss_test(encoded, prototypes, tau):\n",
    "    \n",
    "    #prototypes = torch.stack(prototypes).squeeze() \n",
    "    loss_val_test, args_count = loss_test_basic(encoded, prototypes)\n",
    "    \n",
    "    ###inter cluster distance \n",
    "    if tau>0:\n",
    "        dists = euclidean_dist(prototypes, prototypes)\n",
    "        nproto = prototypes.shape[0]\n",
    "        loss_val2 = - torch.sum(dists)/(nproto*nproto-nproto)\n",
    "        loss_val_test += tau*loss_val2\n",
    "        \n",
    "    return loss_val_test, args_count\n",
    "\n",
    "def reconstruction_loss(decoded, x):\n",
    "    \n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    loss_rcn = loss_func(decoded, x)\n",
    "    #print('Reconstruction {}'.format(loss_rcn))\n",
    "    \n",
    "    return loss_rcn\n",
    "\n",
    "\n",
    "def vae_loss(model, decoded, x, mu, logvar):\n",
    "    \n",
    "#     loss_func = torch.nn.MSELoss()\n",
    "#     loss_rcn = loss_func(decoded, x)\n",
    "    #KL =  torch.mean(-0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()))\n",
    "    \n",
    "    loss_rcn = torch.mean(torch.sum((x - decoded).pow(2),1))\n",
    "    \n",
    "    KL_loss = (-0.5)*(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    KL = torch.mean(torch.sum(KL_loss, axis=1))\n",
    "    \n",
    "    #print(model.lambda_reconstruct, model.lambda_kl)\n",
    "    total_loss = model.lambda_reconstruct*loss_rcn + model.lambda_kl * KL \n",
    "    \n",
    "    \n",
    "    #print(f\"Reconstruction Loss: {loss_rcn} KL Loss: {KL}\")\n",
    "    \n",
    "    return total_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "#from sklearn.cluster import k_means_\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "## Training set (annotated landmarks)\n",
    "def compute_landmarks_tr(embeddings, target, prev_landmarks=None, tau=0.2):\n",
    "    \n",
    "    \"\"\"Computing landmarks of each class in the labeled meta-dataset. \n",
    "    \n",
    "    Landmark is a closed form solution of \n",
    "    minimizing distance to the mean and maximizing distance to other landmarks. \n",
    "    \n",
    "    If tau=0, landmarks are just mean of data points.\n",
    "    \n",
    "    embeddings: embeddings of the labeled dataset\n",
    "    target: labels in the labeled dataset\n",
    "    prev_landmarks: landmarks from previous iteration\n",
    "    tau: regularizer for inter- and intra-cluster distance\n",
    "    \"\"\"\n",
    "    \n",
    "    uniq = torch.unique(target, sorted=True)\n",
    "    class_idxs = list(map(lambda c: target.eq(c).nonzero(), uniq))\n",
    "    \n",
    "    landmarks_mean = torch.stack([embeddings[idx_class].mean(0) for idx_class in class_idxs]).squeeze()\n",
    "    \n",
    "    if prev_landmarks is None or tau==0:\n",
    "        return landmarks_mean\n",
    "    \n",
    "    suma = prev_landmarks.sum(0)\n",
    "    nlndmk = prev_landmarks.shape[0]\n",
    "    lndmk_dist_part = (tau/(nlndmk-1))*torch.stack([suma-p for p in prev_landmarks])\n",
    "    landmarks = 1/(1-tau)*(landmarks_mean-lndmk_dist_part)\n",
    "    \n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_landmarks(n_clusters, tr_load, test_load,\n",
    "                   model, device, mode='kmeans', pretrain=True\n",
    "                   ):\n",
    "    \"\"\"Initialization of landmarks of the labeled and unlabeled meta-dataset.\n",
    "    nclusters: number of expected clusters in the unlabeled meta-dataset\n",
    "    tr_load: data loader for labeled meta-dataset\n",
    "    test_load: data loader for unlabeled meta-dataset\n",
    "    \"\"\"\n",
    "    lndmk_tr = [torch.zeros(size=(len(np.unique(dl.dataset.y)), \n",
    "                                  model.latent_dim), \n",
    "                            requires_grad=True, device=device) for dl in tr_load]\n",
    "    \n",
    "    lndmk_test = [torch.zeros(size=(1, model.latent_dim), \n",
    "                              requires_grad=True, device=device) \n",
    "                       for _ in range(n_clusters)]\n",
    "    \n",
    "    kmeans_init_tr = [init_step(dl.dataset, model, device, pretrained=pretrain, mode=mode) \n",
    "                      for dl in tr_load]\n",
    "    \n",
    "    kmeans_init_test = init_step(test_load.dataset, model, device, \n",
    "                                 pretrained=pretrain, mode=mode, \n",
    "                                 n_clusters=n_clusters)\n",
    "    \n",
    "    ##No gradient calculation\n",
    "    with torch.no_grad():\n",
    "        [lndmk.copy_(kmeans_init_tr[idx])  for idx,lndmk in enumerate(lndmk_tr)]\n",
    "        [lndmk_test[i].copy_(kmeans_init_test[i,:]) for i in range(kmeans_init_test.shape[0])]\n",
    "        \n",
    "    return lndmk_tr, lndmk_test\n",
    "\n",
    "\n",
    "def init_step(dataset, model, device, pretrained, mode='kmeans',n_clusters=None):\n",
    "    \"\"\"Initialization of landmarks with k-means or k-means++ given dataset.\"\"\"\n",
    "    \n",
    "    if n_clusters==None:\n",
    "        n_clusters = len(np.unique(dataset.y))\n",
    "    \n",
    "    nexamples = len(dataset.x)\n",
    "        \n",
    "    X =  torch.stack([dataset.x[i] for i in range(nexamples)])\n",
    "    batch_label = torch.stack([dataset.metadata[i] for i in range(nexamples)])\n",
    "    \n",
    "    \n",
    "    if mode=='kmeans++':\n",
    "        if not pretrained: # find centroids in original space\n",
    "            landmarks = k_means_._init_centroids(X.cpu().numpy(), n_clusters, 'k-means++')\n",
    "            landmarks = torch.tensor(landmarks, device=device)\n",
    "            landmarks = landmarks.to(device)\n",
    "            lndmk_encoded,_ = model(landmarks)\n",
    "            \n",
    "        else:\n",
    "            X = X.to(device)\n",
    "            encoded,_ = model(X)\n",
    "            landmarks = k_means_._init_centroids(encoded.data.cpu().numpy(), n_clusters, 'k-means++')\n",
    "            lndmk_encoded = torch.tensor(landmarks, device=device)\n",
    "    \n",
    "    elif mode=='kmeans': # run kmeans clustering\n",
    "        if not pretrained: \n",
    "            kmeans = KMeans(n_clusters, random_state=0).fit(X.cpu().numpy())\n",
    "            landmarks = torch.tensor(kmeans.cluster_centers_, device=device)\n",
    "            landmarks = landmarks.to(device)\n",
    "            ##Feed forward net on landmarks  (k means cluster)\n",
    "            ##landmarks are k means cluster centers = coordinates of cluster center\n",
    "            #lndmk_encoded,_ = model(landmarks)\n",
    "            _ , lndmk_encoded, _ = model(landmarks)\n",
    "        \n",
    "        ##cluster on lower embedding space\n",
    "        else:\n",
    "            X = X.to(device)\n",
    "            #encoded,_ = model(X)\n",
    "            decode , encoded, logvar = model(X, batch_label)\n",
    "            kmeans = KMeans(n_clusters, random_state=0).fit(encoded.data.cpu().numpy())\n",
    "            lndmk_encoded = torch.tensor(kmeans.cluster_centers_, device=device)\n",
    "    \n",
    "    return lndmk_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE_MARS:\n",
    "    def __init__(self, n_clusters, params, \n",
    "                 labeled_data, unlabeled_data, \n",
    "                 pretrain_data=None, \n",
    "                 \n",
    "                 val_split=1.0, hid_dim_1=1000, hid_dim_2=100, p_drop=0.0, tau=0.2,\n",
    "                 \n",
    "                 latent_dim = 50, \n",
    "                 n_feature = 10, \n",
    "                 \n",
    "                 network_architecture={\"n_hidden_recog_1\": 250, \"n_hidden_recog_2\": 100,\n",
    "                                       \"n_hidden_recog_3\": 100, \"n_hidden_gener_1\": 100,\n",
    "                                       \"n_hidden_gener_2\": 100, \"n_hidden_gener_3\": 100},\n",
    "                 \n",
    "                 lambda_reconstruct = 1e-4, lambda_kl = 1e-3, batch_shape = 2\n",
    "                ):\n",
    "        \n",
    "        \"\"\"Initialization of MARS.\n",
    "        n_clusters: number of clusters in the unlabeled meta-dataset\n",
    "        params: parameters of the MARS model\n",
    "        labeled_data: list of labeled datasets. Each dataset needs to be instance of CellDataset.\n",
    "        unlabeled_data: unlabeled dataset. Instance of CellDataset.\n",
    "        pretrain_data: dataset for pretraining MARS. Instance of CellDataset. If not specified, unlabeled_data\n",
    "                        will be used.\n",
    "        val_split: percentage of data to use for train/val split (default: 1, meaning no validation set)\n",
    "        hid_dim_1: dimension in the first layer of the network (default: 1000)\n",
    "        hid_dim_2: dimension in the second layer of the network (default: 100)\n",
    "        p_drop: dropout probability (default: 0)\n",
    "        tau: regularizer for inter-cluster distance\n",
    "        \"\"\"\n",
    "        train_load, test_load, pretrain_load, val_load = init_data_loaders(labeled_data, unlabeled_data, \n",
    "                                                                           pretrain_data, params.pretrain_batch, \n",
    "                                                                           val_split)\n",
    "        self.train_loader = train_load\n",
    "        self.test_loader = test_load\n",
    "        self.pretrain_loader = pretrain_load\n",
    "        self.val_loader = val_load\n",
    "        \n",
    "        ##data file type (string name)\n",
    "#         self.labeled_metadata = [data.metadata for data in labeled_data]\n",
    "#         self.unlabeled_metadata = unlabeled_data.metadata\n",
    "        \n",
    "        self.labeled_metadata = [\"Annotated\"]\n",
    "        self.unlabeled_metadata = \"Unannotated\"\n",
    "        \n",
    "        self.genes = unlabeled_data.yIDs\n",
    "        \n",
    "        ##number of genes \n",
    "        x_dim = self.test_loader.dataset.get_dim()\n",
    "        \n",
    "        ##KNN clusters\n",
    "        self.n_clusters = n_clusters\n",
    "        \n",
    "        ##General Parameters\n",
    "        self.device = params.device\n",
    "        self.epochs = params.epochs\n",
    "        self.epochs_pretrain = params.epochs_pretrain\n",
    "        self.pretrain_flag = params.pretrain\n",
    "        self.model_file = params.model_file\n",
    "        self.lr = params.learning_rate\n",
    "        self.lr_gamma = params.lr_scheduler_gamma\n",
    "        self.step_size = params.lr_scheduler_step\n",
    "        \n",
    "        self.tau = tau\n",
    "        \n",
    "        ##VAE parameter\n",
    "        self.lambda_reconstruct = lambda_reconstruct\n",
    "        self.lambda_kl = lambda_kl\n",
    "        \n",
    "#         self.hidden_encode1=network_architecture['n_hidden_recog_1']\n",
    "#         self.hidden_encode2=network_architecture['n_hidden_recog_2']\n",
    "#         self.hidden_encode3=network_architecture['n_hidden_recog_3']\n",
    "        \n",
    "#         self.hidden_decode1=network_architecture['n_hidden_gener_1']\n",
    "#         self.hidden_decode2=network_architecture['n_hidden_gener_2']\n",
    "#         self.hidden_decode3=network_architecture['n_hidden_gener_3']\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_feature = n_feature\n",
    "        \n",
    "        self.batch_shape = batch_shape\n",
    "        \n",
    "        self.init_model(self.latent_dim, self.n_feature, network_architecture, \n",
    "                        self.lambda_reconstruct, self.lambda_kl, params.device, self.batch_shape)\n",
    "        \n",
    "        #print(self.lambda_reconstruct)\n",
    "        \n",
    "    ###################################################################\n",
    "    ###################################################################    \n",
    "    ###### With the fine tuned hyper parameter     \n",
    "    ###### Change the implementation of AE to VAE     \n",
    "    def init_model(self, \n",
    "                   latent_dim , \n",
    "                   n_feature, \n",
    "                   network_architecture, \n",
    "                   lambda_reconstruct, lambda_kl,\n",
    "                   device, batch_shape\n",
    "                  ):\n",
    "        \"\"\"\n",
    "        Initialize the model.\n",
    "        \"\"\"\n",
    "        #self.model = FullNet(x_dim, hid_dim, z_dim, p_drop).to(device)\n",
    "        self.model = CVAE(latent_dim = latent_dim, \n",
    "                         n_feature = n_feature, \n",
    "                         network_architecture= network_architecture,\n",
    "                         lambda_reconstruct = lambda_reconstruct, \n",
    "                         lambda_kl = lambda_kl,\n",
    "                         batch_shape = batch_shape\n",
    "                        ).to(device)\n",
    "        \n",
    "        \n",
    "    ###################################################################\n",
    "    ###################################################################\n",
    "    \n",
    "    def init_optim(self, param1, param2, learning_rate):\n",
    "        \"\"\"Initializing optimizers.\"\"\"\n",
    "        \n",
    "        optim = torch.optim.Adam(params=param1, lr=learning_rate)\n",
    "        optim_landmk_test = torch.optim.Adam(params=param2, lr=learning_rate)\n",
    "        \n",
    "        return optim, optim_landmk_test\n",
    "    \n",
    "    ###################################################################\n",
    "    ###################################################################\n",
    "    ########VAE (KL + reconstruction loss + regularized)\n",
    "    def pretrain(self, optim):\n",
    "        \"\"\"\n",
    "        Pretraining model with variational autoencoder.\n",
    "        only on unannotated dataset \n",
    "        optim: optimizer\n",
    "        \"\"\"\n",
    "        print('Pretraining..')\n",
    "        \n",
    "        pretrain_loss =[]\n",
    "        \n",
    "        for e in range(self.epochs_pretrain):\n",
    "            \n",
    "            start = time.time()\n",
    "            \n",
    "            \n",
    "            for _, batch in enumerate(self.pretrain_loader):\n",
    "                \n",
    "                x, y, cell_name, batch_label = batch\n",
    "                \n",
    "                x = x.to(self.device)\n",
    "                batch_label = batch_label.to(self.device)\n",
    "                \n",
    "#                 print(batch_label.shape)\n",
    "#                 print(x.shape)\n",
    "                \n",
    "                #_, decoded = self.model(x)\n",
    "                decoded, mu, logvar = self.model(x , batch_label )\n",
    "                \n",
    "                #loss = reconstruction_loss(decoded, x) \n",
    "                loss = vae_loss(self.model, decoded, x, mu, logvar)\n",
    "                \n",
    "                optim.zero_grad()              \n",
    "                loss.backward()                    \n",
    "                optim.step() \n",
    "                \n",
    "                \n",
    "            pretrain_loss.append(loss) \n",
    "            \n",
    "            print(f\"Pretraining Epoch {e}, Loss: {loss}\")\n",
    "            print(\"Time: \", time.time()-start)\n",
    "        \n",
    "        print(\"Pretraining done\")\n",
    "        return mu, pretrain_loss\n",
    "    ###################################################################\n",
    "    ###################################################################\n",
    "    \n",
    "    \n",
    "    def train(self, evaluation_mode=True, save_all_embeddings=True):\n",
    "        \"\"\"Train model.\n",
    "        evaluation_mode: if True, validates model on the unlabeled dataset. \n",
    "        In the evaluation mode, ground truth labels of the unlabeled dataset must be \n",
    "        provided to validate model\n",
    "        \n",
    "        save_all_embeddings: if True, MARS embeddings for annotated and unannotated \n",
    "        experiments will be saved in an anndata object,\n",
    "        otherwise only unnanotated will be saved. \n",
    "        If naming is called after, all embeddings need to be saved\n",
    "        \n",
    "        return: adata: anndata object containing labeled and unlabeled meta-dataset \n",
    "        with MARS embeddings and estimated labels on the unlabeled dataset\n",
    "                landmk_all: landmarks of the labeled and unlabeled meta-dataset in the \n",
    "                order given for training. Landmarks on the unlabeled\n",
    "                            dataset are provided last\n",
    "                metrics: clustering metrics if evaluation_mode is True\n",
    "                \n",
    "        \"\"\"\n",
    "        tr_iter = [iter(dl) for dl in self.train_loader]\n",
    "        \n",
    "        if self.val_loader is not None:\n",
    "            val_iter = [iter(dl) for dl in self.val_loader]\n",
    "        \n",
    "        ##############################\n",
    "        ####Pre train step \n",
    "        ##############################\n",
    "        optim_pretrain = torch.optim.Adam(params=list(self.model.parameters()), lr=self.lr)\n",
    "        \n",
    "        if self.pretrain_flag:\n",
    "            pretrain_latent , pretrain_loss = self.pretrain(optim_pretrain)\n",
    "        else:\n",
    "            self.model.load_state_dict(torch.load(self.MODEL_FILE))    \n",
    "        ##############################\n",
    "        \n",
    "        \n",
    "        test_iter = iter(self.test_loader)\n",
    "        \n",
    "        \n",
    "        ##initialize training (annotated landmark) and testing (unannotated landmark)\n",
    "        landmk_tr, landmk_test = init_landmarks(self.n_clusters, \n",
    "                                                self.train_loader, \n",
    "                                                self.test_loader, \n",
    "                                                self.model, self.device)\n",
    "        \n",
    "        optim, optim_landmk_test = self.init_optim(list(self.model.encoder.parameters()), \n",
    "                                                   landmk_test, self.lr)\n",
    "        \n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optim,\n",
    "                                                       gamma=self.lr_gamma,\n",
    "                                                       step_size=self.step_size)\n",
    "        \n",
    "        latent_tracker = []\n",
    "        training_history = {'Loss':[],'Accuracy':[],'Loss_tracker':[]}\n",
    "        best_acc = 0\n",
    "        \n",
    "        for epoch in range(1, self.epochs+1):\n",
    "            \n",
    "            start = time.time()\n",
    "            \n",
    "            #########################\n",
    "            #####   Model training \n",
    "            #########################\n",
    "            self.model.train()\n",
    "            \n",
    "            ##Do_epoch train over each minibatch return\n",
    "            ##training loss, accuracy, updated landmarks (annoated and unannotated)\n",
    "            ##latent_history returns latents space for 1) training 2) testing 3) training landmark 4) testing landmark\n",
    "            loss_tr, acc_tr, landmk_tr, landmk_test, latent_history, loss_tracker = self.do_epoch(tr_iter, test_iter,\n",
    "                                                                                    optim, \n",
    "                                                                                    optim_landmk_test,\n",
    "                                                                                    landmk_tr, \n",
    "                                                                                    landmk_test)\n",
    "            \n",
    "            ##Loss training includes 1 - VAE loss 2- embedding and landmark distance \n",
    "            print(f'Epoch {epoch} Loss training: {loss_tr}, Accuracy training: {acc_tr}')\n",
    "            \n",
    "            print(\"Time: \", time.time()-start)\n",
    "        \n",
    "            ###Track model training \n",
    "            training_history['Loss'].append(loss_tr)\n",
    "            training_history['Accuracy'].append(acc_tr)\n",
    "            training_history['Loss_tracker'].append(loss_tracker)\n",
    "            latent_tracker.append(latent_history)\n",
    "            \n",
    "            ##only print out the last epoch result indicating end of training\n",
    "            if epoch==self.epochs: \n",
    "                print('\\n=== Epoch: {} ==='.format(epoch))\n",
    "                print('Train acc: {}'.format(acc_tr))\n",
    "            \n",
    "            if self.val_loader is None:\n",
    "                continue\n",
    "            \n",
    "            #########################\n",
    "            #####   Model evaluation \n",
    "            #########################\n",
    "            self.model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                loss_val, acc_val = self.do_val_epoch(val_iter, landmk_tr)\n",
    "                \n",
    "                if acc_val > best_acc:\n",
    "                    print('Saving model...')\n",
    "                    best_acc = acc_val\n",
    "                    best_state = self.model.state_dict()\n",
    "                    #torch.save(model.state_dict(), self.model_file)\n",
    "                postfix = ' (Best)' if acc_val >= best_acc else ' (Best: {})'.format(best_acc)\n",
    "                print('Val loss: {}, acc: {}{}'.format(loss_val, acc_val, postfix))\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "        if self.val_loader is None:\n",
    "            best_state = self.model.state_dict() # best is last\n",
    "        \n",
    "        landmk_all = landmk_tr + [torch.stack(landmk_test).squeeze()]\n",
    "        \n",
    "        ##Test time (assign labels to unlabaled data)\n",
    "        adata_test, eval_results = self.assign_labels(landmk_all[-1], evaluation_mode)\n",
    "        \n",
    "        adata = self.save_result(tr_iter, adata_test, save_all_embeddings)\n",
    "        \n",
    "        if evaluation_mode:\n",
    "            return adata, landmk_all, eval_results, training_history, latent_tracker, pretrain_latent , pretrain_loss\n",
    "        \n",
    "        return adata, landmk_all, training_history, latent_tracker, pretrain_latent , pretrain_loss\n",
    "    \n",
    "    def save_result(self, tr_iter, adata_test, save_all_embeddings):\n",
    "        \"\"\"Saving embeddings from labeled and unlabeled dataset, ground truth labels and \n",
    "        predictions to joint anndata object.\"\"\"\n",
    "        \n",
    "        adata_all = []\n",
    "\n",
    "        if save_all_embeddings:\n",
    "            for task in range(len(tr_iter)): # saving embeddings from labeled dataset\n",
    "                \n",
    "                task = int(task)\n",
    "                \n",
    "                x, y, cells, batch_label = next(tr_iter[task])\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                \n",
    "                #encoded,_ = self.model(x)\n",
    "                decoded, mu, logvar = self.model(x , batch_label)\n",
    "                #adata_all.append(self.pack_anndata(x, cells, encoded, gtruth=y))\n",
    "                adata_all.append(self.pack_anndata(x, cells, mu, gtruth=y))\n",
    "            \n",
    "        adata_all.append(adata_test)    \n",
    "        \n",
    "        if save_all_embeddings:\n",
    "            adata = adata_all[0].concatenate(adata_all[1:], \n",
    "                                             batch_key='experiment',\n",
    "                                             batch_categories=self.labeled_metadata+[self.unlabeled_metadata])\n",
    "        else:\n",
    "            adata = adata_all[0]\n",
    "\n",
    "        adata.obsm['MARS_embedding'] = np.concatenate([a.uns['MARS_embedding'] for a in adata_all])\n",
    "        #adata.write('result_adata.h5ad')\n",
    "        \n",
    "        return adata\n",
    "    \n",
    "    def assign_labels(self, landmk_test, evaluation_mode):\n",
    "        \"\"\"Assigning cluster labels to the unlabeled meta-dataset.\n",
    "        test_iter: iterator over unlabeled dataset\n",
    "        landmk_test: landmarks in the unlabeled dataset\n",
    "        evaluation mode: computes clustering metrics if True\n",
    "        \"\"\"\n",
    "          \n",
    "        torch.no_grad()\n",
    "        self.model.eval() # eval mode\n",
    "        \n",
    "        test_iter = iter(self.test_loader)\n",
    "        \n",
    "        x_test, y_true, cells, batch_label = next(test_iter) # cells are needed because dataset is in random order\n",
    "        x_test = x_test.to(self.device)\n",
    "        \n",
    "        #encoded_test,_ = self.model(x_test)\n",
    "        decoded, encoded_test, logvar = self.model(x_test, batch_label)\n",
    "        \n",
    "        ###Embedding space eucledian distance \n",
    "        dists = euclidean_dist(encoded_test, landmk_test)\n",
    "        \n",
    "        ###Prediction based on the minimal distance to learned landmark\n",
    "        y_pred = torch.min(dists, 1)[1]\n",
    "        \n",
    "        adata = self.pack_anndata(x_test, cells, encoded_test, y_true, y_pred)\n",
    "        \n",
    "        eval_results = None\n",
    "        if evaluation_mode:\n",
    "            eval_results = compute_scores(y_true, y_pred)\n",
    "            \n",
    "        return adata, eval_results\n",
    "    \n",
    "    \n",
    "    def pack_anndata(self, x_input, cells, embedding, gtruth=[], estimated=[]):\n",
    "        \"\"\"Pack results in anndata object.\n",
    "        x_input: gene expressions in the input space\n",
    "        cells: cell identifiers\n",
    "        embedding: resulting embedding of x_test using MARS\n",
    "        landmk: MARS estimated landmarks\n",
    "        gtruth: ground truth labels if available (default: empty list)\n",
    "        estimated: MARS estimated clusters if available (default: empty list)\n",
    "        \"\"\"\n",
    "        adata = anndata.AnnData(x_input.data.cpu().numpy())\n",
    "        adata.obs_names = cells\n",
    "        adata.var_names = self.genes\n",
    "        if len(estimated)!=0:\n",
    "            adata.obs['MARS_labels'] = pd.Categorical(values=estimated.cpu().numpy())\n",
    "        if len(gtruth)!=0:\n",
    "            adata.obs['truth_labels'] = pd.Categorical(values=gtruth.cpu().numpy())\n",
    "        adata.uns['MARS_embedding'] = embedding.data.cpu().numpy()\n",
    "        \n",
    "        return adata\n",
    "    \n",
    "    \n",
    "    ####Originally, MARS does not care about reconstruction loss, it only minimizes embedding to landmark distances\n",
    "    ####We can alter this by adding reconstruction and KL term to the total loss being regularized \n",
    "    \n",
    "    def do_epoch(self, tr_iter, test_iter, optim, optim_landmk_test, landmk_tr, landmk_test):\n",
    "        \"\"\"\n",
    "        One training epoch.\n",
    "        tr_iter: iterator over labeled meta-data\n",
    "        test_iter: iterator over unlabeled meta-data\n",
    "        \n",
    "        optim: optimizer for embedding\n",
    "        optim_landmk_test: optimizer for test landmarks\n",
    "        \n",
    "        landmk_tr: landmarks of labeled meta-data from previous epoch\n",
    "        landmk_test: landmarks of unlabeled meta-data from previous epoch\n",
    "        \"\"\"\n",
    "        \n",
    "        latent_history = {'Train_latent':[], 'Train_label':[],\n",
    "                          'Test_latent':[], 'Test_label':[], \n",
    "                          'Train_landmark':[], 'Test_landmark':[]\n",
    "                         }\n",
    "        loss_tracker = {'Test_anno':0,\n",
    "                        'Train_latent_loss':0, 'Train_reconstr_loss': 0, \n",
    "                        'Test_latent_loss':0, 'Test_reconstr_loss': 0}\n",
    "        \n",
    "        self.set_requires_grad(False)\n",
    "        \n",
    "        ##Partially freeze landmark_test\n",
    "        for landmk in landmk_test:\n",
    "            landmk.requires_grad=False\n",
    "            \n",
    "        ##Initialize gradient of landmakr_test\n",
    "        optim_landmk_test.zero_grad()\n",
    "        \n",
    "        ##########################################################################\n",
    "        ##########################################################################\n",
    "        # Update centroids    \n",
    "        ##########################################################################\n",
    "        ##########################################################################\n",
    "        \n",
    "        task_idx = torch.randperm(len(tr_iter)) ##shuffle per epoch\n",
    "        ###Annotated landmark (No gradient descent needed)\n",
    "        for task in task_idx:\n",
    "            \n",
    "            ##Training set through the VAE to generate latent space\n",
    "            task = int(task)\n",
    "            x, y, cell, batch_label = next(tr_iter[task])\n",
    "            x, y, batch_label = x.to(self.device), y.to(self.device), batch_label.to(self.device)\n",
    "            #encoded,_ = self.model(x)\n",
    "            decoded, encoded, logvar = self.model(x, batch_label)\n",
    "            \n",
    "            curr_landmk_tr = compute_landmarks_tr(encoded, y, landmk_tr[task], tau=self.tau)\n",
    "            landmk_tr[task] = curr_landmk_tr.data # save landmarks\n",
    "            \n",
    "            latent_history['Train_landmark'].append(encoded)\n",
    "            latent_history['Train_label'].append(y)\n",
    "        \n",
    "        ###Unannotated landmark (Use gradient descent to minimize)\n",
    "        \n",
    "        ##Unfreeze landmark_test --> autograd update centroids\n",
    "        for landmk in landmk_test:\n",
    "            landmk.requires_grad=True\n",
    "            \n",
    "        x, y_test, cell , batch_label = next(test_iter)\n",
    "        x = x.to(self.device)\n",
    "        #encoded,_ = self.model(x)\n",
    "        decoded, encoded, logvar = self.model(x, batch_label)\n",
    "        \n",
    "        ###minimize intra cluster difference and maximize inter cluster difference \n",
    "        loss, args_count = loss_test(encoded, \n",
    "                                     torch.stack(landmk_test).squeeze(), \n",
    "                                     self.tau)\n",
    "        loss.backward()\n",
    "        optim_landmk_test.step()\n",
    "        \n",
    "        latent_history['Test_landmark'].append(encoded)\n",
    "        latent_history['Test_label'].append(y_test)\n",
    "        loss_tracker['Test_anno']=loss\n",
    "        \n",
    "        ##########################################################################\n",
    "        ##########################################################################\n",
    "        # Update embedding\n",
    "        ##########################################################################\n",
    "        ##########################################################################\n",
    "        \n",
    "        self.set_requires_grad(True)\n",
    "        for landmk in landmk_test:\n",
    "            landmk.requires_grad=False\n",
    "            \n",
    "        optim.zero_grad()\n",
    "        total_accuracy = 0\n",
    "        total_loss = 0\n",
    "        ntasks = 0\n",
    "        mean_accuracy = 0\n",
    "        \n",
    "        ###Annotated set\n",
    "        task_idx = torch.randperm(len(tr_iter))\n",
    "        \n",
    "        for task in task_idx:\n",
    "            task = int(task)\n",
    "            x, y, cell, batch_label = next(tr_iter[task])\n",
    "            x, y, batch_label = x.to(self.device), y.to(self.device), batch_label.to(self.device)\n",
    "            #encoded,_ = self.model(x)\n",
    "            decoded, encoded, logvar = self.model(x, batch_label)\n",
    "            \n",
    "            ###Eucledian distance between embedding and landmark\n",
    "            loss, acc = loss_task(encoded, landmk_tr[task], y, criterion='dist')\n",
    "            \n",
    "            ##Add VAE reconstruction loss to the model \n",
    "            annotated_vae_loss = vae_loss(self.model, decoded, x, encoded, logvar)\n",
    "            \n",
    "            ##Record latent space\n",
    "            latent_history['Train_latent'].append(encoded)\n",
    "            \n",
    "            loss_tracker['Train_latent_loss']=loss\n",
    "            loss_tracker['Train_reconstr_loss']=annotated_vae_loss\n",
    "            \n",
    "            total_loss += loss\n",
    "            #total_loss += annotated_vae_loss\n",
    "            \n",
    "            total_accuracy += acc.item()\n",
    "            \n",
    "            ntasks += 1\n",
    "        \n",
    "        if ntasks>0:\n",
    "            mean_accuracy = total_accuracy / ntasks\n",
    "        \n",
    "        ##Un-Annotated\n",
    "        # test part\n",
    "        x,y, cell, batch_label = next(test_iter)\n",
    "        x = x.to(self.device)\n",
    "        #encoded,_ = self.model(x)\n",
    "        decoded, encoded, logvar = self.model(x, batch_label)\n",
    "        \n",
    "        loss,_ = loss_test(encoded, torch.stack(landmk_test).squeeze(), self.tau)\n",
    "        \n",
    "        ##Add VAE reconstruction loss to the model \n",
    "        unannotated_vae_loss = vae_loss(self.model, decoded, x, encoded, logvar)\n",
    "        latent_history['Test_latent'].append(encoded)\n",
    "        \n",
    "        loss_tracker['Test_latent_loss']=(loss)\n",
    "        loss_tracker['Test_reconstr_loss']=(unannotated_vae_loss)\n",
    "            \n",
    "        total_loss += loss\n",
    "        #total_loss += unannotated_vae_loss\n",
    "        ntasks += 1\n",
    "    \n",
    "        mean_loss = total_loss / ntasks\n",
    "        \n",
    "        mean_loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        return mean_loss, mean_accuracy, landmk_tr, landmk_test, latent_history, loss_tracker\n",
    "    \n",
    "    def do_val_epoch(self, val_iter, prev_landmk):\n",
    "        \"\"\"One epoch of validation.\n",
    "        val_iter: iterator over validation set\n",
    "        prev_landmk: landmarks from previous epoch\n",
    "        \"\"\"\n",
    "        ntasks = len(val_iter)\n",
    "        task_idx = torch.randperm(ntasks)\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        \n",
    "        for task in task_idx:\n",
    "            x, y, cell, batch_label = next(val_iter[task])\n",
    "            x, y, batch_label = x.to(self.device), y.to(self.device), batch_label.to(self.device)\n",
    "            #encoded = self.model(x)\n",
    "            decoded, encoded, logvar = self.model(x, batch_label)\n",
    "            \n",
    "            ###Eucledian distance between embedding and landmark\n",
    "            loss, acc = loss_task(encoded, prev_landmk[task], y, criterion='dist')\n",
    "            val_vae_loss = vae_loss(self.model, decoded, x, encoded, logvar)\n",
    "            \n",
    "            #total_loss += val_vae_loss\n",
    "            total_loss += loss\n",
    "            total_accuracy += acc.item()\n",
    "        \n",
    "        mean_accuracy = total_accuracy / ntasks\n",
    "        mean_loss = total_loss / ntasks\n",
    "        \n",
    "        return mean_loss, mean_accuracy\n",
    "    \n",
    "    \n",
    "    def set_requires_grad(self, requires_grad):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "    \n",
    "    def name_cell_types(self, adata, landmk_all, cell_name_mappings, \n",
    "                        top_match=5, umap_reduce_dim=True, ndim=10):\n",
    "        \"\"\"For each test cluster, estimate sigma and mean. \n",
    "        Fit Gaussian distribution with that mean and sigma\n",
    "        and calculate the probability of each of the train landmarks \n",
    "        to be the neighbor to the mean data point.\n",
    "        Normalization is performed with regards to all other landmarks in train.\"\"\"\n",
    "        \n",
    "        experiments = list(OrderedDict.fromkeys(list(adata.obs['experiment'])))\n",
    "        \n",
    "        ###only get labels from labeled data\n",
    "        encoded_tr = []\n",
    "        landmk_tr = []\n",
    "        landmk_tr_labels = []\n",
    "        for idx, exp in enumerate(experiments[:-1]):\n",
    "            tiss = adata[adata.obs['experiment'] == exp,:]\n",
    "            \n",
    "            if exp==self.unlabeled_metadata: \n",
    "                raise ValueError(\"Error: Unlabeled dataset needs to be last one in the input anndata object.\")\n",
    "                \n",
    "            encoded_tr.append(tiss.obsm['MARS_embedding'])\n",
    "            landmk_tr.append(landmk_all[idx])\n",
    "            landmk_tr_labels.append(np.unique(tiss.obs['truth_labels']))\n",
    "            \n",
    "        tiss = adata[adata.obs['experiment'] == self.unlabeled_metadata,:]\n",
    "        ypred_test = tiss.obs['MARS_labels']\n",
    "        uniq_ytest = np.unique(ypred_test)\n",
    "        encoded_test = tiss.obsm['MARS_embedding']\n",
    "        \n",
    "        landmk_tr_labels = np.concatenate(landmk_tr_labels)\n",
    "        encoded_tr = np.concatenate(encoded_tr)\n",
    "        landmk_tr = np.concatenate([p.cpu() for p in landmk_tr])\n",
    "        \n",
    "        if  umap_reduce_dim:\n",
    "            encoded_extend = np.concatenate((encoded_tr, encoded_test, landmk_tr))\n",
    "            adata = anndata.AnnData(encoded_extend)\n",
    "            sc.pp.neighbors(adata, n_neighbors=15, use_rep='X')\n",
    "            sc.tl.umap(adata, n_components=ndim)\n",
    "            encoded_extend = adata.obsm['X_umap']\n",
    "            n1 = len(encoded_tr)\n",
    "            n2 = n1 + len(encoded_test)\n",
    "            \n",
    "            ##UMAP embedding space\n",
    "            encoded_tr = encoded_extend[:n1,:]\n",
    "            encoded_test = encoded_extend[n1:n2,:]\n",
    "            landmk_tr = encoded_extend[n2:,:]\n",
    "        \n",
    "        interp_names = defaultdict(list)\n",
    "        for ytest in uniq_ytest:\n",
    "            print('\\nCluster label: {}'.format(str(ytest)))\n",
    "            idx = np.where(ypred_test==ytest)\n",
    "            subset_encoded = encoded_test[idx[0],:]\n",
    "            \n",
    "            mean = np.expand_dims(np.mean(subset_encoded, axis=0),0)\n",
    "            \n",
    "            sigma  = self.estimate_sigma(subset_encoded)\n",
    "            \n",
    "            prob = np.exp(-np.power(distance.cdist(mean, landmk_tr, metric='euclidean'),2)/(2*sigma*sigma))\n",
    "            prob = np.squeeze(prob, 0)\n",
    "            normalizat = np.sum(prob)\n",
    "            \n",
    "            if normalizat==0:\n",
    "                print('Unassigned')\n",
    "                interp_names[ytest].append(\"unassigned\")\n",
    "                continue\n",
    "            \n",
    "            prob = np.divide(prob, normalizat)\n",
    "            \n",
    "            uniq_tr = np.unique(landmk_tr_labels)\n",
    "            prob_unique = []\n",
    "            \n",
    "            for cell_type in uniq_tr: # sum probabilities of same landmarks\n",
    "                prob_unique.append(np.sum(prob[np.where(landmk_tr_labels==cell_type)]))\n",
    "            \n",
    "            sorted = np.argsort(prob_unique, axis=0)\n",
    "            best = uniq_tr[sorted[-top_match:]]\n",
    "            sortedv = np.sort(prob_unique, axis=0)\n",
    "            sortedv = sortedv[-top_match:]\n",
    "            for idx, b in enumerate(best):\n",
    "                interp_names[ytest].append((cell_name_mappings[b], sortedv[idx]))\n",
    "                print('{}: {}'.format(cell_name_mappings[b], sortedv[idx]))\n",
    "                \n",
    "        return interp_names\n",
    "    \n",
    "    \n",
    "    def estimate_sigma(self, dataset):\n",
    "        nex = dataset.shape[0]\n",
    "        dst = []\n",
    "        for i in range(nex):\n",
    "            for j in range(i+1, nex):\n",
    "                dst.append(distance.euclidean(dataset[i,:],dataset[j,:]))\n",
    "        return np.std(dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_mars = CVAE_MARS( n_clusters, params, [cvae_annotated], cvae_unannotated, cvae_pretrain, \n",
    "                        val_split=1.0, hid_dim_1=1000, hid_dim_2=100, p_drop=0.0, tau=0.2,\n",
    "                        latent_dim = 50, n_feature = xy.shape[0], \n",
    "                        network_architecture={\"n_hidden_recog_1\": 250, \"n_hidden_recog_2\": 100,\n",
    "                                       \"n_hidden_recog_3\": 100, \"n_hidden_gener_1\": 100,\n",
    "                                       \"n_hidden_gener_2\": 100, \"n_hidden_gener_3\": 100},\n",
    "                        lambda_reconstruct = 1, lambda_kl = 4, batch_shape = 2)\n",
    "\n",
    "#adata, landmarks, scores , training_history, latent_tracker, \n",
    "cvae_adata, cvae_landmarks, cvae_scores, cvae_training_history, cvae_latent_tracker, cvae_pretrain_latent , cave_pretrain_loss = cvae_mars.train(evaluation_mode=True,save_all_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_loss_tracker (cvae_training_history, save =True)\n",
    "MARS_history(cvae_training_history, save=True)\n",
    "MARS_latent_pca(cvae_latent_tracker , 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_latent_umap( cvae_adata, plot_gene_list = ['MARS_labels','experiment'], save=True)\n",
    "cell_type_assign(cvae_adata, save=True)\n",
    "MARS_silhouette(cvae_adata, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(cvae_adata, \n",
    "           color=['experiment','label_name'], ncols=1, save=\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(vae_merge_adata, \n",
    "           color=['experiment','label_name'], ncols=1, save=\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(ae_merge_adata, \n",
    "           color=['experiment','label_name'], ncols=1, save=\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(cvae_adata[cvae_adata.obs.experiment==\"Unannotated\"], \n",
    "           color=['MARS_labels', 'label_name'], ncols=2, save=\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(vae_merge_adata[vae_merge_adata.obs.experiment==\"Unannotated\"], \n",
    "           color=['MARS_labels', 'label_name'], ncols=2, save='.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(ae_merge_adata[ae_merge_adata.obs.experiment==\"Unannotated\"], \n",
    "           color=['MARS_labels', 'label_name'], ncols=2, save='.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(vae_merge_adata11[vae_merge_adata11.obs.experiment==\"Unannotated\"], \n",
    "           color=['MARS_labels', 'label_name'], ncols=2, save='.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([ae_merge_scores,vae_merge_scores,cvae_scores, vae_merge_scores11], index=['AE','VAE','CVAE','VAE11'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MARS_silhouette_score(adata):\n",
    "    \n",
    "    \n",
    "    anno_data = adata[adata.obs['experiment'] == \"Annotated\", :]\n",
    "    unanno_data = adata[adata.obs['experiment'] == \"Unannotated\", :]\n",
    "    \n",
    "    anno_obs = anno_data.obs\n",
    "    unanno_obs = unanno_data.obs\n",
    "    \n",
    "    ###Silhouette\n",
    "    train_latent = anno_data.obsm['MARS_embedding']\n",
    "    train_sil=silhouette_samples(train_latent, anno_obs['truth_labels'].values)\n",
    "    \n",
    "    val_latent = unanno_data.obsm['MARS_embedding']\n",
    "    val_sil=silhouette_samples(val_latent, unanno_obs['truth_labels'].values)\n",
    "    \n",
    "    n_clusters=len(Counter(adata.obs['truth_labels'].values).keys())\n",
    "    \n",
    "    train_avg=silhouette_score(train_latent, anno_obs['truth_labels'].values)\n",
    "    val_avg=silhouette_score(val_latent, unanno_obs['truth_labels'].values)\n",
    "    \n",
    "    return train_sil , val_sil, train_avg , val_avg\n",
    "    \n",
    "    \n",
    "#     ###Plot silhouette score for train and test \n",
    "#     fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "#     fig.set_size_inches(18, 10)\n",
    "        \n",
    "#     ax1.set_xlim([-0.1, 1])\n",
    "#     ax1.set_ylim([0, len(train_latent) + (n_clusters + 1) * 10])\n",
    "    \n",
    "#     ax2.set_xlim([-0.1, 1])\n",
    "#     ax2.set_ylim([0, len(val_latent) + (n_clusters + 1) * 10])\n",
    "         \n",
    "#     y_lower = 10\n",
    "#     val_y_lower = 10\n",
    "    \n",
    "    \n",
    "#     name2num={'Malignant':1, 'Endothelial':2, 'T cell':3, \n",
    "#           'Macrophage':4, 'B cell':5,'CAF':6, \n",
    "#           'Dendritic':7 ,'Plasma B':8, 'NK':9}\n",
    "\n",
    "#     cluster_list = list(name2num.keys())\n",
    "    \n",
    "#     for i in range(n_clusters):\n",
    "        \n",
    "        \n",
    "#         #cluster_label = list(Counter(adata.obs['truth_labels']).keys())[i]\n",
    "#         cluster_label = cluster_list[i]\n",
    "        \n",
    "#         #############################################################\n",
    "#         ###############Training \n",
    "#         # Aggregate the silhouette scores for samples belonging to\n",
    "#         # cluster i, and sort them\n",
    "#         train_ith_cluster_silhouette_values = train_sil[anno_obs['label_name'] == cluster_label]\n",
    "#         train_ith_cluster_silhouette_values.sort()\n",
    "        \n",
    "#         size_cluster_i = train_ith_cluster_silhouette_values.shape[0]\n",
    "#         y_upper = y_lower + size_cluster_i\n",
    "\n",
    "#         #color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "#         ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "#                           0, train_ith_cluster_silhouette_values) \n",
    "#         #edgecolor=color, facecolor=color\n",
    "\n",
    "#         # Label the silhouette plots with their cluster numbers at the middle\n",
    "#         ax1.text(0.8, y_lower + 0.5 * size_cluster_i, str(cluster_label))\n",
    "\n",
    "#         # Compute the new y_lower for next plot\n",
    "#         y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "        \n",
    "#         #############################################################\n",
    "#         ###############Validation \n",
    "#         val_ith_cluster_silhouette_values = val_sil[unanno_obs['label_name'] == cluster_label]\n",
    "#         val_ith_cluster_silhouette_values.sort()\n",
    "        \n",
    "#         val_size_cluster_i = val_ith_cluster_silhouette_values.shape[0]\n",
    "#         val_y_upper = val_y_lower + val_size_cluster_i\n",
    "\n",
    "#         #color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "#         ax2.fill_betweenx(np.arange(val_y_lower, val_y_upper),\n",
    "#                           0, val_ith_cluster_silhouette_values)\n",
    "#         # edgecolor=color, facecolor=color\n",
    "\n",
    "#         # Label the silhouette plots with their cluster numbers at the middle\n",
    "#         ax2.text(0.8, val_y_lower + 0.5 * val_size_cluster_i, str(cluster_label))\n",
    "\n",
    "#         # Compute the new y_lower for next plot\n",
    "#         val_y_lower = val_y_upper + 10  # 10 for the 0 samples\n",
    "     \n",
    "#     ax1.set_title(\"Training silhouette plot for known labels\")\n",
    "#     ax1.set_xlabel(\"Silhouette coefficient values\")\n",
    "#     ax1.set_ylabel(\"Cluster label\")\n",
    "    \n",
    "#     ax2.set_title(\"Validation silhouette plot for known labels\")\n",
    "#     ax2.set_xlabel(\"Silhouette coefficient values\")\n",
    "#     ax2.set_ylabel(\"Cluster label\")\n",
    "    \n",
    "#     # The vertical line for average silhouette score of all the values\n",
    "#     train_avg=silhouette_score(train_latent, anno_obs['truth_labels'].values)\n",
    "#     val_avg=silhouette_score(val_latent, unanno_obs['truth_labels'].values)\n",
    "    \n",
    "#     ax1.axvline(x=train_avg, color=\"red\", linestyle=\"--\")\n",
    "#     ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "#     ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    \n",
    "#     ax2.axvline(x=val_avg, color=\"red\", linestyle=\"--\")\n",
    "#     ax2.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "#     ax2.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    \n",
    "#     plt.suptitle(\"Silhouette scores for MARS latent space\", fontsize=14, fontweight='bold')\n",
    "#     plt.show()  \n",
    "    \n",
    "#     if save ==True:\n",
    "#         fig.savefig(\"MARS_silhouette.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_train_sil , ae_val_sil, ae_train_avg , ae_val_avg = MARS_silhouette_score(ae_merge_adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_train_sil , vae_val_sil, vae_train_avg , vae_val_avg = MARS_silhouette_score(vae_merge_adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_train_sil , cvae_val_sil, cvae_train_avg , cvae_val_avg = MARS_silhouette_score(cvae_adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_train_avg, vae_train_avg , cvae_train_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_val_avg, vae_val_avg, cvae_val_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_train_sil=pd.concat([ae_merge_adata.obs[ae_merge_adata.obs['experiment']==\"Annotated\"], \n",
    "                        pd.DataFrame(ae_train_sil, columns=[\"Sil\"], \n",
    "                                     index =  ae_merge_adata.obs[ae_merge_adata.obs['experiment']==\"Annotated\"].index ) ],\n",
    "                       axis=1)\n",
    "ae_train_sil['type']=\"Annotated AE MARS\"\n",
    "ae_test_sil=pd.concat([ae_merge_adata.obs[ae_merge_adata.obs['experiment']==\"Unannotated\"], \n",
    "                        pd.DataFrame(ae_val_sil, columns=[\"Sil\"], \n",
    "                                     index =  ae_merge_adata.obs[ae_merge_adata.obs['experiment']==\"Unannotated\"].index ) ],\n",
    "                       axis=1)\n",
    "ae_test_sil['type']=\"Unannotated AE MARS\"\n",
    "\n",
    "ae_sil = pd.concat([ae_train_sil ,ae_test_sil],axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1,1, figsize=(15,6))\n",
    "plot = sns.boxplot(x=\"label_name\", y=\"Sil\",hue=\"type\",data=ae_sil, palette=\"Set3\")\n",
    "plot.set_xlabel(\"Ground Truth Label\", fontsize=15)\n",
    "plot.set_ylabel(\"Silhouette Coefficients\", fontsize=15)\n",
    "plot.axhline(linewidth=1, color='black', linestyle=\"dashdot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1,1, figsize=(15,6))\n",
    "plot = sns.violinplot(x=\"label_name\", y=\"Sil\",hue=\"type\",data=ae_sil, palette=\"muted\")\n",
    "plot.set_xlabel(\"Ground Truth Label\", fontsize=15)\n",
    "plot.set_ylabel(\"Silhouette Coefficients\", fontsize=15)\n",
    "plot.axhline(linewidth=1, color='black', linestyle=\"dashdot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_train_sil=pd.concat([vae_merge_adata.obs[vae_merge_adata.obs['experiment']==\"Annotated\"], \n",
    "                        pd.DataFrame(vae_train_sil, columns=[\"Sil\"], \n",
    "                                     index =  vae_merge_adata.obs[vae_merge_adata.obs['experiment']==\"Annotated\"].index ) ],\n",
    "                       axis=1)\n",
    "vae_train_sil['type']=\"Annotated VAE MARS\"\n",
    "vae_test_sil=pd.concat([vae_merge_adata.obs[vae_merge_adata.obs['experiment']==\"Unannotated\"], \n",
    "                        pd.DataFrame(vae_val_sil, columns=[\"Sil\"], \n",
    "                                     index =  vae_merge_adata.obs[vae_merge_adata.obs['experiment']==\"Unannotated\"].index ) ],\n",
    "                       axis=1)\n",
    "vae_test_sil['type']=\"Unannotated VAE MARS\"\n",
    "\n",
    "vae_sil = pd.concat([vae_train_sil ,vae_test_sil],axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1,1, figsize=(15,6))\n",
    "plot = sns.boxplot(x=\"label_name\", y=\"Sil\",hue=\"type\",data=vae_sil, palette=\"Set3\")\n",
    "plot.set_xlabel(\"Ground Truth Label\", fontsize=15)\n",
    "plot.set_ylabel(\"Silhouette Coefficients\", fontsize=15)\n",
    "plot.axhline(linewidth=1, color='black', linestyle=\"dashdot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_train_sil=pd.concat([cvae_adata.obs[cvae_adata.obs['experiment']==\"Annotated\"], \n",
    "                        pd.DataFrame(cvae_train_sil, columns=[\"Sil\"], \n",
    "                                     index =  cvae_adata.obs[cvae_adata.obs['experiment']==\"Annotated\"].index ) ],\n",
    "                       axis=1)\n",
    "cvae_train_sil['type']=\"Annotated CVAE MARS\"\n",
    "cvae_test_sil=pd.concat([cvae_adata.obs[cvae_adata.obs['experiment']==\"Unannotated\"], \n",
    "                        pd.DataFrame(cvae_val_sil, columns=[\"Sil\"], \n",
    "                                     index =  cvae_adata.obs[cvae_adata.obs['experiment']==\"Unannotated\"].index ) ],\n",
    "                       axis=1)\n",
    "cvae_test_sil['type']=\"Unannotated CVAE MARS\"\n",
    "\n",
    "cvae_sil = pd.concat([cvae_train_sil ,cvae_test_sil],axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1,1, figsize=(15,6))\n",
    "plot = sns.boxplot(x=\"label_name\", y=\"Sil\",hue=\"type\",data=cvae_sil, palette=\"Set3\")\n",
    "plot.set_xlabel(\"Ground Truth Label\", fontsize=15)\n",
    "plot.set_ylabel(\"Silhouette Coefficients\", fontsize=15)\n",
    "plot.axhline(linewidth=1, color='black', linestyle=\"dashdot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install statannot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_sil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(Counter(cvae_sil.label_name).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_pair = []\n",
    "for name in list(Counter(cvae_sil.label_name).keys()):\n",
    "    box_pair.append(((name,'Annotated CVAE MARS'),(name,'Unannotated CVAE MARS') ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statannot import add_stat_annotation\n",
    "plt.subplots(1,1, figsize=(15,6))\n",
    "plot = sns.boxplot(x=\"label_name\", y=\"Sil\",hue=\"type\",data=cvae_sil, palette=\"Set3\")\n",
    "\n",
    "plot.set_xlabel(\"Ground Truth Label\", fontsize=15)\n",
    "plot.set_ylabel(\"Silhouette Coefficients\", fontsize=15)\n",
    "plot.axhline(linewidth=1, color='black', linestyle=\"dashdot\")\n",
    "\n",
    "plot= add_stat_annotation(plot, data=cvae_sil, x=\"label_name\", y='Sil', hue='type',\n",
    "                    box_pairs= box_pair,\n",
    "                    test='Mann-Whitney', text_format='star', loc='outside', verbose=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_pair = []\n",
    "for name in list(Counter(cvae_sil.label_name).keys()):\n",
    "    box_pair.append(((name,'Unannotated AE MARS'),(name,'Unannotated VAE MARS')))\n",
    "    box_pair.append(((name,'Unannotated AE MARS'),(name,'Unannotated CVAE MARS')))\n",
    "    box_pair.append(((name,'Unannotated VAE MARS'),(name,'Unannotated CVAE MARS')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sil = pd.concat([ae_test_sil ,vae_test_sil ,cvae_test_sil],axis=0 )\n",
    "plt.subplots(1,1, figsize=(15,6))\n",
    "plot = sns.boxplot(x=\"label_name\", y=\"Sil\",hue=\"type\",data=test_sil, palette=\"Set3\")\n",
    "plot.set_xlabel(\"Ground Truth Label\", fontsize=15)\n",
    "plot.set_ylabel(\"Silhouette Coefficients\", fontsize=15)\n",
    "plot.axhline(linewidth=1, color='black', linestyle=\"dashdot\")\n",
    "\n",
    "plot, test_results = add_stat_annotation(plot, data=test_sil, x=\"label_name\", y='Sil', hue='type',\n",
    "                    box_pairs= box_pair,\n",
    "                    test='Kruskal', text_format='star', loc='outside', verbose=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.get_figure().savefig(\"test_sil.png\",dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_pair = []\n",
    "for name in list(Counter(cvae_sil.label_name).keys()):\n",
    "    box_pair.append(((name,'Annotated AE MARS'),(name,'Annotated VAE MARS')))\n",
    "    box_pair.append(((name,'Annotated AE MARS'),(name,'Annotated CVAE MARS')))\n",
    "    box_pair.append(((name,'Annotated VAE MARS'),(name,'Annotated CVAE MARS')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sil = pd.concat([ae_train_sil ,vae_train_sil ,cvae_train_sil],axis=0 )\n",
    "plt.subplots(1,1, figsize=(15,6))\n",
    "plot = sns.boxplot(x=\"label_name\", y=\"Sil\",hue=\"type\",data=train_sil, palette=\"Set3\")\n",
    "plot.set_xlabel(\"Ground Truth Label\", fontsize=15)\n",
    "plot.set_ylabel(\"Silhouette Coefficients\", fontsize=15)\n",
    "plot.axhline(linewidth=1, color='black', linestyle=\"dashdot\")\n",
    "\n",
    "plot , test_result= add_stat_annotation(plot, data=train_sil, x=\"label_name\", y='Sil', hue='type',\n",
    "                    box_pairs= box_pair,\n",
    "                    test='Kruskal', text_format='star', loc='outside', verbose=2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plot.get_figure().savefig(\"train_sil.png\",dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Past exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_neg3_4 = VAE_MARS(n_clusters, params, [annotated] , unannotated, pretrain, \n",
    "                \n",
    "                ##These are roll over parametesr form original model \n",
    "                hid_dim_1=1000, hid_dim_2=100, p_drop=0.0, tau=1e-3,\n",
    "                \n",
    "                ##VAE MARS specific parameters\n",
    "                latent_dim = 50 , n_feature = len(mit_data.var_names), \n",
    "                \n",
    "                network_architecture={\"n_hidden_recog_1\": 250, \"n_hidden_recog_2\": 100,\n",
    "                           \"n_hidden_recog_3\": 100, \"n_hidden_gener_1\": 100,\n",
    "                           \"n_hidden_gener_2\": 100, \"n_hidden_gener_3\": 100},\n",
    "                \n",
    "                lambda_reconstruct = 1, lambda_kl = 4)\n",
    "\n",
    "adata_neg3_4, landmarks_neg3_4, scores_neg3_4 , training_history_neg3_4, latent_tracker_neg3_4, pretrain_latent_neg3_4 , pretrain_loss_neg3_4= mars_neg3_4.train(evaluation_mode=True,\n",
    "save_all_embeddings=True)\n",
    "\n",
    "MARS_loss_tracker (training_history_neg3_4)\n",
    "MARS_history(training_history_neg3_4)\n",
    "MARS_latent_pca(latent_tracker_neg3_4 , 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_latent_umap(adata_neg3_4, plot_gene_list = ['MARS_labels','experiment',\n",
    "                                              'CST3','CCL5','FCGR3A','NKG7','MS4A1','CD79A','CD8A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_smalltau4 = VAE_MARS(n_clusters, params, [annotated] , unannotated, pretrain, \n",
    "                \n",
    "                ##These are roll over parametesr form original model \n",
    "                hid_dim_1=1000, hid_dim_2=100, p_drop=0.0, tau=0.01,\n",
    "                \n",
    "                ##VAE MARS specific parameters\n",
    "                latent_dim = 50 , n_feature = len(mit_data.var_names), \n",
    "                \n",
    "                network_architecture={\"n_hidden_recog_1\": 250, \"n_hidden_recog_2\": 100,\n",
    "                           \"n_hidden_recog_3\": 100, \"n_hidden_gener_1\": 100,\n",
    "                           \"n_hidden_gener_2\": 100, \"n_hidden_gener_3\": 100},\n",
    "                \n",
    "                lambda_reconstruct = 1, lambda_kl = 4)\n",
    "\n",
    "adata_smalltau4, landmarks_smalltau4, scores_smalltau4 , training_history_smalltau4, latent_tracker_smalltau4, pretrain_latent_smalltau4 , pretrain_loss_smalltau4 = mars_smalltau4.train(evaluation_mode=True,\n",
    "save_all_embeddings=True)\n",
    "\n",
    "MARS_loss_tracker (training_history_smalltau4)\n",
    "MARS_history(training_history_smalltau4)\n",
    "MARS_latent_pca(latent_tracker_smalltau4 , 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_latent_umap(adata_smalltau4, plot_gene_list = ['MARS_labels','experiment',\n",
    "                                              'CST3','CCL5','FCGR3A','NKG7','MS4A1','CD79A','CD8A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_smalltau = VAE_MARS(n_clusters, params, [annotated] , unannotated, pretrain, \n",
    "                \n",
    "                ##These are roll over parametesr form original model \n",
    "                hid_dim_1=1000, hid_dim_2=100, p_drop=0.0, tau=0.01,\n",
    "                \n",
    "                ##VAE MARS specific parameters\n",
    "                latent_dim = 50 , n_feature = len(mit_data.var_names), \n",
    "                \n",
    "                network_architecture={\"n_hidden_recog_1\": 250, \"n_hidden_recog_2\": 100,\n",
    "                           \"n_hidden_recog_3\": 100, \"n_hidden_gener_1\": 100,\n",
    "                           \"n_hidden_gener_2\": 100, \"n_hidden_gener_3\": 100},\n",
    "                \n",
    "                lambda_reconstruct = 1, lambda_kl = 1e-3)\n",
    "\n",
    "adata_smalltau, landmarks_smalltau, scores_smalltau , training_history_smalltau, latent_tracker_smalltau, pretrain_latent_smalltau , pretrain_loss_smalltau = mars_smalltau.train(evaluation_mode=True,\n",
    "save_all_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_loss_tracker (training_history_smalltau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_history(training_history_smalltau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_latent_pca(latent_tracker_smalltau , 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_latent_umap(adata_smalltau, plot_gene_list = ['MARS_labels','experiment',\n",
    "                                              'CST3','CCL5','FCGR3A','NKG7','MS4A1','CD79A','CD8A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_smalltau_largekl = VAE_MARS(n_clusters, params, [annotated] , unannotated, pretrain, \n",
    "                \n",
    "                ##These are roll over parametesr form original model \n",
    "                hid_dim_1=1000, hid_dim_2=100, p_drop=0.0, tau=0.01,\n",
    "                \n",
    "                ##VAE MARS specific parameters\n",
    "                latent_dim = 50 , n_feature = len(mit_data.var_names), \n",
    "                \n",
    "                network_architecture={\"n_hidden_recog_1\": 250, \"n_hidden_recog_2\": 100,\n",
    "                           \"n_hidden_recog_3\": 100, \"n_hidden_gener_1\": 100,\n",
    "                           \"n_hidden_gener_2\": 100, \"n_hidden_gener_3\": 100},\n",
    "                \n",
    "                lambda_reconstruct = 1, lambda_kl = 0.1)\n",
    "\n",
    "adata_smalltau_largekl, landmarks_smalltau_largekl, scores_smalltau_largekl , training_history_smalltau_largekl, latent_tracker_smalltau_largekl, pretrain_latent_smalltau_largekl , pretrain_loss_smalltau_largekl = mars_smalltau_largekl.train(evaluation_mode=True,\n",
    "save_all_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_loss_tracker (training_history_smalltau_largekl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_history(training_history_smalltau_largekl)\n",
    "MARS_latent_pca(latent_tracker_smalltau_largekl , 29)\n",
    "MARS_latent_umap(adata_smalltau_largekl, plot_gene_list = ['MARS_labels','experiment',\n",
    "                                              'CST3','CCL5','FCGR3A','NKG7','MS4A1','CD79A','CD8A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars = VAE_MARS(n_clusters, params, [annotated] , unannotated, pretrain, \n",
    "                \n",
    "                ##These are roll over parametesr form original model \n",
    "                hid_dim_1=1000, hid_dim_2=100, p_drop=0.0, tau=0.2,\n",
    "                \n",
    "                ##VAE MARS specific parameters\n",
    "                latent_dim = 50 , n_feature = len(mit_data.var_names), \n",
    "                \n",
    "                network_architecture={\"n_hidden_recog_1\": 250, \"n_hidden_recog_2\": 100,\n",
    "                           \"n_hidden_recog_3\": 100, \"n_hidden_gener_1\": 100,\n",
    "                           \"n_hidden_gener_2\": 100, \"n_hidden_gener_3\": 100},\n",
    "                \n",
    "                lambda_reconstruct = 1, lambda_kl = 4)\n",
    "\n",
    "adata, landmarks, scores , training_history, latent_tracker, pretrain_latent , pretrain_loss = mars.train(evaluation_mode=True, \n",
    "                                                                                                          save_all_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_loss_tracker (training_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(training_history['Loss_tracker'])\n",
    "plt.scatter(temp['Test_anno'], temp['Test_latent_loss'], label =\"Train latent loss\")\n",
    "plt.scatter(temp['Test_anno'], temp['Train_latent_loss'], label= \"Test latent loss\")\n",
    "plt.xlabel(\"Unannotated landmark loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_history(training_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_latent_pca(latent_tracker , 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_latent_umap(adata, plot_gene_list = ['MARS_labels','experiment',\n",
    "                                              'CST3','CCL5','FCGR3A','NKG7','MS4A1','CD79A','CD8A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars = VAE_MARS(n_clusters, params, [annotated] , unannotated, pretrain, \n",
    "                \n",
    "                ##These are roll over parametesr form original model \n",
    "                hid_dim_1=1000, hid_dim_2=100, p_drop=0.0, tau=0.2,\n",
    "                \n",
    "                ##VAE MARS specific parameters\n",
    "                latent_dim = 50 , n_feature = len(mit_data.var_names), \n",
    "                \n",
    "                network_architecture={\"n_hidden_recog_1\": 250, \"n_hidden_recog_2\": 100,\n",
    "                           \"n_hidden_recog_3\": 100, \"n_hidden_gener_1\": 100,\n",
    "                           \"n_hidden_gener_2\": 100, \"n_hidden_gener_3\": 100},\n",
    "                \n",
    "                lambda_reconstruct = 1, lambda_kl = 1e-3)\n",
    "\n",
    "adata, landmarks, scores , training_history, latent_tracker, pretrain_latent , pretrain_loss = mars.train(evaluation_mode=True, \n",
    "                                                                                                          save_all_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_loss_tracker (training_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_history(training_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_latent_pca(latent_tracker , 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_latent_umap(adata, plot_gene_list = ['MARS_labels','experiment',\n",
    "                                              'CST3','CCL5','FCGR3A','NKG7','MS4A1','CD79A','CD8A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_loss_tracker (ae_training_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_history(ae_training_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_latent_pca(ae_latent_tracker , 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARS_latent_umap(ae_adata, plot_gene_list = ['MARS_labels','experiment',\n",
    "                                              'CST3','CCL5','FCGR3A','NKG7','MS4A1','CD79A','CD8A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More granular annotations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
